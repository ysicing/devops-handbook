[{"content":"接上文Lima虚拟机初体验一在M1上玩转Lima\n快速开始 M1系统信息 # brew install m-cli ➜ ~ m info ProductName:\tmacOS ProductVersion:\t11.5.2 BuildVersion:\t20G95 m1 patch # 安装构建依赖 brew update brew install libffi gettext glib pkg-config autoconf automake pixman ninja # 下载源码 git clone https://github.com/qemu/qemu cd qemu git checkout 3c93dfa42c394fdd55684f2fbf24cf2f39b97d47 curl https://patchwork.kernel.org/series/485309/mbox/ | git am # build mkdir build \u0026amp;\u0026amp; cd build ../configure --target-list=aarch64-softmmu --enable-hvf --disable-gnutls make -j8 # 安装 sudo make install 注意一点:\n 可能通过brew安装的qume，会识别默认该路径下的/opt/homebrew/bin/qemu-system-aarch64, 而patch生成的则在/usr/local/bin/qemu-system-aarch64。\n rm /opt/homebrew/bin/qemu-system-aarch64 测试patch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  wget https://mirrors.tuna.tsinghua.edu.cn/debian-cd/current/arm64/iso-cd/debian-11.0.0-arm64-netinst.iso qemu-img create -f qcow2 virtual-disk.qcow2 8G cp $(dirname $(which qemu-img))/../share/qemu/edk2-aarch64-code.fd . dd if=/dev/zero conv=sync bs=1m count=64 of=ovmf_vars.fd /usr/local/bin/qemu-system-aarch64 \\  -machine virt,accel=hvf,highmem=off \\  -cpu cortex-a72 -smp 4 -m 4G \\  -device virtio-gpu-pci \\  -device virtio-keyboard-pci \\  -drive \u0026#34;format=raw,file=edk2-aarch64-code.fd,if=pflash,readonly=on\u0026#34; \\  -drive \u0026#34;format=raw,file=ovmf_vars.fd,if=pflash\u0026#34; \\  -drive \u0026#34;format=qcow2,file=virtual-disk.qcow2\u0026#34; \\  -cdrom debian-11.0.0-arm64-netinst.iso   成功启动虚拟机表示patch成功了，可以通过lima启动管理虚拟机了\n安装Lima并启动 安装lima\n1  brew install lima   启动lima\n1 2 3 4  # 下载专用debian配置 wget https://sh.ysicing.me/lima/debian.yml # 启动debian虚拟机 limactl start ./debian.yml   附录阅读  Github - Create Virtual Machines using QEMU on Silicon based Apple Macs Github - Running Linux and Windows on M1 with QEMU  ","description":"M1 Mac上使用Lima运行Debian虚拟机","id":4,"section":"posts","tags":["vm","debian","macOS"],"title":"Lima虚拟机初体验二","uri":"https://ysicing.me/posts/lima-vm-on-macos-m1/"},{"content":"什么是Lima  Linux virtual machines on macOS\n 即专注于在macOS上支持文件共享、端口转发和使用 containerd 的 Linux 虚拟机, 其诞生的初衷是推广在macOS上使用containerd和nerdctl\n功能  支持自动文件共享 支持端口转发 内置支持containerd 基于QEMU支持 Intel on Intel, ARM on Intel, ARM on ARM, Intel on ARM 支持Linux发行版常见都可以,如Debian  快速开始 安装 1 2  # Intel Mac brew install lima   Debian自定义配置 生成debian配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  cat \u0026gt; ./debian.yml\u0026lt;\u0026lt;EOFarch:\u0026#34;default\u0026#34;images:- location:\u0026#34;~/hack/iso/debian-11-generic-amd64-20210814-734.qcow2\u0026#34;arch:\u0026#34;x86_64\u0026#34;- location:\u0026#34;https://cloud.debian.org/images/cloud/bullseye/20210814-734/debian-11-generic-amd64-20210814-734.qcow2\u0026#34;arch:\u0026#34;x86_64\u0026#34;cpus:2memory:\u0026#34;4GiB\u0026#34;disk:\u0026#34;100GiB\u0026#34;mounts:- location:\u0026#34;~\u0026#34;writable:false- location:\u0026#34;/tmp/lima\u0026#34;writable:truessh:localPort:60024provision:- mode:systemscript:|#!/bin/bash echo \u0026#34;/usr/local/bin/nerdctl \\$@\u0026#34; \u0026gt; /usr/local/bin/docker chmod +x /usr/local/bin/docker- mode:systemscript:|#!/bin/bash nerdctl run --rm -v /usr/local/bin:/sysdir registry.cn-beijing.aliyuncs.com/k7scn/tools tar zxf /pkg.tgz -C /sysdirprobes:- script:|#!/bin/bash set -eux -o pipefail if ! timeout 120s bash -c \u0026#34;until test -f /usr/local/bin/dps; do sleep 3; done\u0026#34;; then echo \u0026gt;\u0026amp;2 \u0026#34;tools is not installed yet\u0026#34; exit 0 fiEOF  或者\n1  wget https://sh.ysicing.me/lima/debian.yml   启动lima虚拟机 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  limactl start ./debian.yml ? Creating an instance \u0026#34;debian\u0026#34; [Use arrows to move, type to filter] \u0026gt; Proceed with the default configuration Open an editor to override the configuration Exit ? Creating an instance \u0026#34;debian\u0026#34; Proceed with the default configuration INFO[0000] Downloading \u0026#34;https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz\u0026#34; (sha256:ce7a6e119b03c3fb8ded3d46d929962fd17417bea1d5bbc07e0fce49494d8a09) INFO[0000] Using cache \u0026#34;/Users/ysicing/Library/Caches/lima/download/by-url-sha256/3304d173f1e1824e5be6cf84bf2f78825cf0db416c4c975dbb2458776942629e/data\u0026#34; INFO[0001] Attempting to download the image from \u0026#34;~/hack/iso/debian-11-generic-amd64-20210814-734.qcow2\u0026#34; INFO[0002] Downloaded image from \u0026#34;~/hack/iso/debian-11-generic-amd64-20210814-734.qcow2\u0026#34; INFO[0002] [hostagent] Starting QEMU (hint: to watch the boot progress, see \u0026#34;/Users/ysicing/.lima/debian/serial.log\u0026#34;) INFO[0002] SSH Local Port: 60024 INFO[0002] [hostagent] Waiting for the essential requirement 1 of 4: \u0026#34;ssh\u0026#34; INFO[0020] [hostagent] The essential requirement 1 of 4 is satisfied INFO[0020] [hostagent] Waiting for the essential requirement 2 of 4: \u0026#34;sshfs binary to be installed\u0026#34; INFO[0029] [hostagent] The essential requirement 2 of 4 is satisfied INFO[0029] [hostagent] Waiting for the essential requirement 3 of 4: \u0026#34;/etc/fuse.conf to contain \\\u0026#34;user_allow_other\\\u0026#34;\u0026#34; INFO[0044] [hostagent] The essential requirement 3 of 4 is satisfied INFO[0044] [hostagent] Waiting for the essential requirement 4 of 4: \u0026#34;the guest agent to be running\u0026#34; INFO[0044] [hostagent] The essential requirement 4 of 4 is satisfied INFO[0044] [hostagent] Mounting \u0026#34;/Users/ysicing\u0026#34; INFO[0044] [hostagent] Mounting \u0026#34;/tmp/lima\u0026#34; INFO[0045] [hostagent] Waiting for the optional requirement 1 of 2: \u0026#34;systemd must be available\u0026#34; INFO[0045] [hostagent] Forwarding \u0026#34;/run/user/501/lima-guestagent.sock\u0026#34; (guest) to \u0026#34;/Users/ysicing/.lima/debian/ga.sock\u0026#34; (host) INFO[0045] [hostagent] The optional requirement 1 of 2 is satisfied INFO[0045] [hostagent] Waiting for the optional requirement 2 of 2: \u0026#34;containerd binaries to be installed\u0026#34; INFO[0045] [hostagent] Not forwarding TCP [::]:22 INFO[0045] [hostagent] Not forwarding TCP 0.0.0.0:22 INFO[0045] [hostagent] The optional requirement 2 of 2 is satisfied INFO[0045] READY. Run `limactl shell debian` to open the shell.   如果过程失败了, 请检查yaml中的端口配置 ssh.localPort, 较大概率是端口冲突了\n如果提示READY, 则表示虚拟机已经ok\n1 2 3  08:08 ➜ hack limactl shell debian debian ysicing@lima-debian:/Users/ysicing/hack$   1 2  limactl shell debian uname -a Linux lima-debian 5.10.0-8-amd64 #1 SMP Debian 5.10.46-4 (2021-08-03) x86_64 GNU/Linux   容器使用  定制后的配置，可以无缝使用docker命令\n 运行容器 1 2 3 4 5 6  # 类似docker run limactl shell debian nerdctl run -d --name nginx -p 127.0.0.1:8080:80 nginx:alpine # 类型docker ps limactl shell debian nerdctl ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8b877fa8e942 docker.io/library/nginx:alpine \u0026#34;/docker-entrypoint.…\u0026#34; 10 seconds ago Up 127.0.0.1:8080-\u0026gt;80/tcp nginx   或者\nlimactl shell debian docker run -d --name nginx -p 127.0.0.1:8080:80 nginx:alpine limactl shell debian docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8b877fa8e942 docker.io/library/nginx:alpine \u0026quot;/docker-entrypoint.…\u0026quot; 10 seconds ago Up 127.0.0.1:8080-\u0026gt;80/tcp nginx limactl shell debian dps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8b877fa8e942 docker.io/library/nginx:alpine \u0026quot;/docker-entrypoint.…\u0026quot; 10 seconds ago Up 127.0.0.1:8080-\u0026gt;80/tcp nginx 浏览器访问http://127.0.0.1:8080显示nginx默认静态页\n其他  使用root用户  sudo password root su root 附录 配置说明 lima默认配置, 默认在当前用户~/.lima/default/lima.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165  # /Users/ysicing/.lima/default/lima.yaml# ===================================================================== ## BASIC CONFIGURATION# ===================================================================== ## Arch: \u0026#34;default\u0026#34;, \u0026#34;x86_64\u0026#34;, \u0026#34;aarch64\u0026#34;.# \u0026#34;default\u0026#34; corresponds to the host architecture.arch:\u0026#34;default\u0026#34;# An image must support systemd and cloud-init.# Ubuntu and Fedora are known to work.# Default: none (must be specified)images:# Try to use a local image first.- location:\u0026#34;~/Downloads/hirsute-server-cloudimg-amd64.img\u0026#34;arch:\u0026#34;x86_64\u0026#34;- location:\u0026#34;~/Downloads/hirsute-server-cloudimg-arm64.img\u0026#34;arch:\u0026#34;aarch64\u0026#34;# Download the file from the internet when the local file is missing.# Hint: run `limactl prune` to invalidate the \u0026#34;current\u0026#34; cache- location:\u0026#34;https://cloud-images.ubuntu.com/hirsute/current/hirsute-server-cloudimg-amd64.img\u0026#34;arch:\u0026#34;x86_64\u0026#34;- location:\u0026#34;https://cloud-images.ubuntu.com/hirsute/current/hirsute-server-cloudimg-arm64.img\u0026#34;arch:\u0026#34;aarch64\u0026#34;# CPUs: if you see performance issues, try limiting cpus to 1.# Default: 4cpus:2# Memory size# Default: \u0026#34;4GiB\u0026#34;memory:\u0026#34;4GiB\u0026#34;# Disk size# Default: \u0026#34;100GiB\u0026#34;disk:\u0026#34;100GiB\u0026#34;# Expose host directories to the guest# Default: nonemounts:- location:\u0026#34;~\u0026#34;# CAUTION: `writable` SHOULD be false for the home directory.# Setting `writable` to true is possible, but untested and dangerous.writable:false- location:\u0026#34;/tmp/lima\u0026#34;writable:truessh:# A localhost port of the host. Forwarded to port 22 of the guest.# Currently, this port number has to be specified manually.# Default: nonelocalPort:60022# Load ~/.ssh/*.pub in addition to $LIMA_HOME/_config/user.pub .# This option is useful when you want to use other SSH-based# applications such as rsync with the Lima instance.# If you have an insecure key under ~/.ssh, do not use this option.# Default: trueloadDotSSHPubKeys:true# ===================================================================== ## ADVANCED CONFIGURATION# ===================================================================== #containerd:# Enable system-wide (aka rootful) containerd and its dependencies (BuildKit, Stargz Snapshotter)# Default: falsesystem:false# Enable user-scoped (aka rootless) containerd and its dependencies# Default: trueuser:true# Provisioning scripts need to be idempotent because they might be called# multiple times, e.g. when the host VM is being restarted.# provision:# # `system` is executed with the root privilege# - mode: system# script: |# #!/bin/bash# set -eux -o pipefail# export DEBIAN_FRONTEND=noninteractive# apt-get install -y vim# # `user` is executed without the root privilege# - mode: user# script: |# #!/bin/bash# set -eux -o pipefail# cat \u0026lt;\u0026lt;EOF \u0026gt; ~/.vimrc# set number# EOF# probes:# # Only `readiness` probes are supported right now.# - mode: readiness# description: vim to be installed# script: |# #!/bin/bash# set -eux -o pipefail# if ! timeout 30s bash -c \u0026#34;until command -v vim; do sleep 3; done\u0026#34;; then# echo \u0026gt;\u0026amp;2 \u0026#34;vim is not installed yet\u0026#34;# exit 1# fi# hint: |# vim was not installed in the guest. Make sure the package system is working correctly.# Also see \u0026#34;/var/log/cloud-init-output.log\u0026#34; in the guest.# ===================================================================== ## FURTHER ADVANCED CONFIGURATION# ===================================================================== #firmware:# Use legacy BIOS instead of UEFI.# Default: falselegacyBIOS:falsevideo:# QEMU display, e.g., \u0026#34;none\u0026#34;, \u0026#34;cocoa\u0026#34;, \u0026#34;sdl\u0026#34;.# As of QEMU v5.2, enabling this is known to have negative impact# on performance on macOS hosts: https://gitlab.com/qemu-project/qemu/-/issues/334# Default: \u0026#34;none\u0026#34;display:\u0026#34;none\u0026#34;network:# The instance can get routable IP addresses from the vmnet framework using# https://github.com/lima-vm/vde_vmnet. Both vde_switch and vde_vmnet# daemons must be running before the instance is started. The interface type# (host, shared, or bridged) is configured in vde_vmnet and not lima.vde:# vnl (virtual network locator) points to the vde_switch socket directory,# optionally with vde:// prefix# - vnl: \u0026#34;vde:///var/run/vde.ctl\u0026#34;# # VDE Switch port number (not TCP/UDP port number). Set to 65535 for PTP mode.# # Default: 0# switchPort: 0# # MAC address of the instance; lima will pick one based on the instance name,# # so DHCP assigned ip addresses should remain constant over instance restarts.# macAddress: \u0026#34;\u0026#34;# # Interface name, defaults to \u0026#34;vde0\u0026#34;, \u0026#34;vde1\u0026#34;, etc.# name: \u0026#34;\u0026#34;# Port forwarding rules. Forwarding between ports 22 and ssh.localPort cannot be overridden.# Rules are checked sequentially until the first one matches.# portForwards:# - guestPort: 443# hostIP: \u0026#34;0.0.0.0\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;; allows privileged port forwarding# # default: hostPort: 443 (same as guestPort)# # default: guestIP: \u0026#34;127.0.0.1\u0026#34; (also matches bind addresses \u0026#34;0.0.0.0\u0026#34;, \u0026#34;::\u0026#34;, and \u0026#34;::1\u0026#34;)# # default: proto: \u0026#34;tcp\u0026#34; (only valid value right now)# - guestPortRange: [4000, 4999]# hostIP: \u0026#34;0.0.0.0\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;# # default: hostPortRange: [4000, 4999] (must specify same number of ports as guestPortRange)# - guestPort: 80# hostPort: 8080 # overrides the default value 80# - guestIP: \u0026#34;127.0.0.2\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;# hostIP: \u0026#34;127.0.0.2\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;# # default: guestPortRange: [1024, 65535]# # default: hostPortRange: [1024, 65535]# - guestPort: 8888# ignore: true (don\u0026#39;t forward this port)# # Lima internally appends this fallback rule at the end:# - guestIP: \u0026#34;127.0.0.1\u0026#34;# guestPortRange: [1024, 65535]# hostIP: \u0026#34;127.0.0.1\u0026#34;# hostPortRange: [1024, 65535]# # Any port still not matched by a rule will not be forwarded (ignored)  ","description":"Intel Mac上使用Lima运行Debian虚拟机","id":5,"section":"posts","tags":["vm","debian","macOS"],"title":"Lima虚拟机初体验一","uri":"https://ysicing.me/posts/lima-vm-on-macos/"},{"content":"前几天升级Debian11后, 使用iTerm登录机器提示:\nperl: warning: Setting locale failed. perl: warning: Please check that your locale settings: LANGUAGE = \u0026quot;en_US.utf8\u0026quot;, LC_ALL = \u0026quot;en_US.UTF-8\u0026quot;, LC_TERMINAL = \u0026quot;iTerm2\u0026quot;, LANG = \u0026quot;en_US.utf8\u0026quot; are supported and installed on your system. perl: warning: Falling back to the standard locale (\u0026quot;C\u0026quot;). locale: Cannot set LC_CTYPE to default locale: No such file or directory locale: Cannot set LC_MESSAGES to default locale: No such file or directory locale: Cannot set LC_ALL to default locale: No such file or directory /usr/bin/locale: Cannot set LC_CTYPE to default locale: No such file or directory /usr/bin/locale: Cannot set LC_MESSAGES to default locale: No such file or directory /usr/bin/locale: Cannot set LC_ALL to default locale: No such file or directory 这不能忍，一梭子dpkg-reconfigure解决\ndpkg-reconfigure locales # 勾选上 en_US.UTF-8 zh_CN.UTF-8 # ok, 选择默认 en_US.UTF-8 # 最终结果 Generating locales (this might take a while)... en_HK.UTF-8... done en_US.UTF-8... done zh_CN.UTF-8... done Generation complete. 注意:\n 需要使用root用户  ","description":"升级Debian11后又遇到locales报错","id":6,"section":"posts","tags":["debian"],"title":"Debian11配置Locales","uri":"https://ysicing.me/posts/debian11-locales/"},{"content":"为什么要Geocn 大部分场景下,我们网站允许任何人访问, 某些特殊场景下我们需要限制大陆ip访问。\n但是已有插件caddy-maxmind-geolocation为啥不用。\n默认使用的 GeoIP2 数据库是来自于 MaxMind 的 GeoLite2 免费数据库。这个数据库目前存在一下几个问题：\n 获取不便：从 2019 年 12 月 30 日起，必须注册后才能下载 数据量大：数据库庞大，包含全球的 IP 地址段，约 4 MB 准确度低：对中国大陆的 IP 地址判定不准，如：香港阿里云的 IP 被判定为新加坡、中国大陆等  新的政策要求注册才能下载会增加时间成本，而且会让自动化下载的难度大大增加。庞大的数据量无可厚非，但是对于大多数中国大陆的用户来说，仅需要去判断 IP 的地理位置是否属于中国大陆境内，其他国家的 IP 一律代理。过多的数据量会增加载入时间，降低查询效率。\n基于上述考虑, 结合caddy-maxmind-geolocation和Hackl0us/GeoIP2-CN两者, 只有判断ip是否是中国大陆ip即可。\n核心设计  使用Hackl0us/GeoIP2-CN提供的准确度高、用户使用体验好的仅含有中国大陆 IP 信息的 GeoIP2 数据库 使用caddyhttp.RequestMatcher, 不得不说 Request Matchers  获取request.RemoteAddr地址, 优先级最高 从request.Header获取X-Forwarded-For地址, 如果RemoteAddr为内网地址,才考虑    使用caddy 编译caddy2 可以参考 ysicing/caddy2\n默认情况下, 直接使用提供的二进制即可。不过我们需要集成第三方插件，需要使用xcaddy自行编译\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # 本地安装xcaddy go install github.com/caddyserver/xcaddy/cmd/xcaddy@latest # github action安装 - name: Install Go uses: actions/setup-go@v1 with: go-version: 1.17.x - name: install xcaddy run: | echo \u0026#34;install xcaddy\u0026#34; go get -u github.com/caddyserver/xcaddy/cmd/xcaddy@latest - name: build bin run: | export TZ=\u0026#39;Asia/Shanghai\u0026#39; export PATH=$PATH:$(go env GOPATH)/bin xcaddy build --with github.com/ysicing/caddy2-geocn   构建caddy二进制\n1 2 3 4  # 使用remote code xcaddy build --with github.com/ysicing/caddy2-geocn # 使用local code xcaddy build --with github.com/ysicing/caddy2-geocn=../caddy2-geocn   插件本地化测试 cd go/src/github.com/ysicing/ git clone https://github.com/ysicing/caddy2-geocn.git cd caddy2-geocn # 二进制 make build make run # docker GOOS=linux xcaddy build --with github.com/ysicing/caddy2-geocn=../caddy2-geocn rm -rf Country.mmdb wget https://github.com/Hackl0us/GeoIP2-CN/raw/release/Country.mmdb docker build -t ghcr.io/ysicing/caddy2-geocn -f example/Dockerfile . docker-compose.yaml示例如下:\n1 2 3 4 5 6 7 8 9  version:\u0026#39;2\u0026#39;services:caddy2-geocn:image:ghcr-proxy.hk1.godu.dev/ysicing/caddy2-geocncontainer_name:caddy2-geocnports:- \u0026#39;80:80\u0026#39;volumes:- \u0026#39;./Caddyfile:/etc/caddy/Caddyfile\u0026#39;  Caddyfile示例如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  (LOG) { log { output file /var/log/caddy.log { roll_size 1mb roll_keep 5 roll_keep_for 1h } format console { time_format \u0026#34;iso8601\u0026#34; } } } (COMCFG) { encode zstd gzip } { debug } :80 { import COMCFG import LOG metrics /metrics @geofilter { not geocn { db_file \u0026#34;/etc/caddy/Country.mmdb\u0026#34; } } # file_server @geofilter { # root /etc/caddy/example/deny #}  redir @geofilter https://www.baidu.com{uri} permanent file_server { root /etc/caddy/example/allow } } EOF docker-compose up -d   云环境 1  docker run -itd 80:80 ghcr.io/ysicing/caddy2-geocn:latest  应用场景 我的博客 就是很好的示例, 分别使用代理和非代理方式访问\n致谢  Hackl0us/GeoIP2-CN Caddyserver  ","description":"Caddy2插件Geocn相关信息","id":7,"section":"posts","tags":["caddy","caddy-plugin"],"title":"Caddy2插件Geocn","uri":"https://ysicing.me/posts/caddy2-plugin-geocn/"},{"content":" 记录日常运维开发工作。\n ChangeLog 20210830 变更  ysicing.me 境外访问 非大陆访问, 部署在hk 阿里云k3s集群上 blog.ysicing.net 大陆访问, 部署在南京 腾讯云k3s集群上  ","description":"关于","id":8,"section":"","tags":null,"title":"关于","uri":"https://ysicing.me/about/"},{"content":" k3s 是Rancher推出的轻量级 k8s.\n 升级debian10(buster)内核版本 1 2 3 4 5 6 7 8 9  sed -i \u0026#39;s/buster\\/updates/bullseye-security/g;s/buster/bullseye/g\u0026#39; /etc/apt/sources.list apt update apt dist-upgrade -y # apt install -t bullseye-backports linux-image-amd64 -y # update-grub # reboot # 内核 Linux bj01 5.10.0-0.bpo.8-amd64 #1 SMP Debian 5.10.46-2~bpo10+1 (2021-07-22) x86_64 GNU/Linux   具体可以参考 Debian个人常用操作指南 升级内核部分。\n安装 wireguard 1 2  # 所有节点需安装 apt install wireguard -y   安装docker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - echo \u0026#34;deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/debian bullseye stable\u0026#34; | tee /etc/apt/sources.list.d/docker.list apt update apt install -y docker-ce # 腾讯云使用bip有问题 169.254.123.1/24, 请使用172.30.42.1/16 cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://dyucrs4l.mirror.aliyuncs.com\u0026#34;], \u0026#34;bip\u0026#34;: \u0026#34;169.254.123.1/24\u0026#34;, \u0026#34;max-concurrent-downloads\u0026#34;: 10, \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-level\u0026#34;: \u0026#34;warn\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;30m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } EOF systemctl enable docker systemctl daemon-reload systemctl restart docker docker info -f \u0026#34;{{json .ServerVersion }}\u0026#34; docker pull registry.cn-beijing.aliyuncs.com/k7scn/tools docker run --rm -v /usr/local/bin:/sysdir registry.cn-beijing.aliyuncs.com/k7scn/tools tar zxf /pkg.tgz -C /sysdir   部署控制平面master节点 上面安装docker/tools时, 默认内置了k3s,版本v1.18.19, 也可以自行下载其他版本的k3s替换\nk3s --version k3s version v1.18.19+k3s1 (a260c3c6) # 执行安装, 默认开机启动 k3s.master.install 默认k3s.master.install配置参数如下:\n1 2 3 4 5 6 7  ExecStart=/usr/local/bin/k3s \\  server \\  --docker \\  --flannel-backend wireguard \\  --no-deploy traefik,servicelb \\  --kube-proxy-arg \u0026#34;proxy-mode=ipvs\u0026#34; \u0026#34;masquerade-all=true\u0026#34; \\  --kube-proxy-arg \u0026#34;metrics-bind-address=0.0.0.0\u0026#34;   查看组件状态\nk3s kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok 部署计算worker节点 从master节点获取token\n K3S_TOKEN : 加入集群所需的token，可以在控制节点上查看/var/lib/rancher/k3s/server/node-token 文件 K3S_URL: master节点地址  export K3S_URL=https://10.147.20.42:6443 export K3S_TOKEN=K102f2c1f6c878f693700c24b741d309d2ff4038ade912f0a44248781c04376e878::server:bc39d44d89042011b985f267eebe2b2f k3s.worker.install 查看节点状态 1 2 3 4 5  ~# kubectl get nodes NAME STATUS ROLES AGE VERSION k3s02 Ready \u0026lt;none\u0026gt; 53m v1.18.19+k3s1 k3s03 Ready \u0026lt;none\u0026gt; 52m v1.18.19+k3s1 k3s01 Ready master 54m v1.18.19+k3s1   ","description":"Debian11 (Bullseye)安装k3s","id":9,"section":"posts","tags":["kubernetes","k3s","debian"],"title":"Debian 11安装k3s","uri":"https://ysicing.me/posts/k3s-install-bullseye/"},{"content":"概述 随着互联网发展，https越来越普及，是个网站都要上个https，否则就是非主流哈哈哈。之前也介绍过caddy，自动签发https证书，这是一个传统的方式。那么服务上到k8s上，如何实现永久免费证书了。cert-manager 是 Kubernetes 上比较牛逼的证书管理工具，可以帮助从各种来源颁发证书，而且确保证书有效且最新，并尝试在到期前的配置时间续订证书。\n工作原理 cert-manager部署到 Kubernetes 集群后，根据实际指定的CRD资源来签发证书并自动续期。\n部分名词解释\n  Issuer/ClusterIssuer: 定义证书签发的姿势。两者唯一区别就是Issuer只能用来签发单一ns下的证书，ClusterIssuer可以签发集群级别的证书。\n  Certificate: 具体签发域名以及通过哪种姿势(即指定Issuer/ClusterIssuer)。\n  证书签发原理 由于cert-manager是基于ACME协议与Let\u0026rsquo;s Encrypt来签发免费证书，这里就不详细说明这个了, 其主流校验方式是HTTP-01 和 DNS-01。由于我们需要在内网签发证书，HTTP-01校验方式就不太适用了。\n操作步骤  推荐使用helmv3安装\n 安装 cert-manager # 创建命名空间 kubectl create namespace cert-manager # 添加helm仓库或者更新 helm repo add jetstack https://charts.jetstack.io helm repo update helminit # 一键安装 helm upgrade -i cert-manager -n cert-manager -f https://gitee.com/godu/helminit/raw/master/cert-manager.1.0.3.yaml --version v1.0.3 jetstack/cert-manager # 查看安装情况 kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-6564dc56bf-qgxlr 1/1 Running 0 97s cert-manager-cainjector-7b98cf8646-t6cqd 1/1 Running 0 97s cert-manager-webhook-5f8bb4f46c-44hzt 1/1 Running 0 97s DNS-01校验 因为dns托管在阿里云，且cert-manager不支持alidns，只能通过webhook方式部署，来扩展DNS提供商。\n这里以阿里dns + alidns-webhook方式为例。\n部署alidns-webhook  安装alidns-webhook  1 2  # 默认镜像使用阿里云k7scn kubectl apply -f https://gitee.com/godu/helminit/raw/master/cert-manager/alidns-cm-webhook.yaml    创建alidns api secret  1 2 3 4 5 6 7 8  apiVersion:v1kind:Secretmetadata:name:alidns-secretnamespace:cert-managerdata:access-key:YOUR_ACCESS_KEY# base64secret-key:YOUR_SECRET_KEY  这里需要注意两点，一个是Secret值问题，需要避免\\n或者空格影响\n建议这样 echo -n YOUR_ACCESS_KEY | base64， 还有如果是要创建 ClusterIssuer，Secret 需要创建在 cert-manager 所在命名空间中，如果是 Issuer，那就创建在 Issuer 所在命名空间中。\n第2个点就是配置alidns的RAM，这里有个RAM坑。\nRAM权限设置如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;acs:alidns:*:*:domain/\u0026lt;域名\u0026gt;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;alidns:DescribeSiteMonitorIspInfos\u0026#34;, \u0026#34;alidns:DescribeSiteMonitorIspCityInfos\u0026#34;, \u0026#34;alidns:DescribeSupportLines\u0026#34;, \u0026#34;alidns:DescribeDomains\u0026#34;, \u0026#34;alidns:DescribeDomainNs\u0026#34;, \u0026#34;alidns:DescribeDomainGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;acs:alidns:*:*:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] }   正常情况下只需要配置第一条规则即可，但是由于alidns-webhook会校验这个域名存不存在,如果不配置会报403权限\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # https://github.com/pragkent/alidns-webhook/blob/master/alidns/client.go#L26 func (c *Client) getHostedZone(zone string) (string, error) { request := alidns.CreateDescribeDomainsRequest() request.KeyWord = util.UnFqdn(zone) request.SearchMode = \u0026#34;EXACT\u0026#34; response, err := c.dnsc.DescribeDomains(request) if err != nil { return \u0026#34;\u0026#34;, err } zones := response.Domains.Domain if len(zones) == 0 { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;zone %s does not exist\u0026#34;, zone) } return zones[0].DomainName, nil }   创建ClusterIssuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  apiVersion:cert-manager.io/v1alpha2kind:ClusterIssuermetadata:name:letsencrypt-prodspec:acme:# Change to your letsencrypt emailemail:your@domain# 你的邮箱server:https://acme-v02.api.letsencrypt.org/directoryprivateKeySecretRef:name:letsencrypt-prodsolvers:- dns01:webhook:groupName:acme.yourcompany.comsolverName:alidnsconfig:region:\u0026#34;\u0026#34;accessKeySecretRef:name:alidns-secretkey:access-keysecretKeySecretRef:name:alidns-secretkey:secret-key  创建 Certificate 1 2 3 4 5 6 7 8 9 10 11 12 13 14  apiVersion:cert-manager.io/v1alpha2kind:Certificatemetadata:name:tls-ops-domain-prodnamespace:gaea-opspec:secretName:tls-ops-domain-prod# 证书保存的 secret 名commonName:dnsNames:- \u0026#34;*.bj-internal.ops.domain\u0026#34;- \u0026#34;*.bj-external.ops.domain\u0026#34;issuerRef:name:letsencrypt-prodkind:ClusterIssuer  通过dns验证可以创建泛域名证书。\n获取和使用证书 创建好 Certificate 后，稍等2-3分钟 我们可以 查看是否签发成功:\nkubectl get certificate -n gaea-op NAME READY SECRET AGE tls-ops-domain-prod True tls-ops-domain-prod 2m 查看证书信息\nkubectl get secrets/tls-ops-domain-prod -n gaea-op -o json | jq '.data.\u0026quot;tls.crt\u0026quot;' | tr '\\\u0026quot;' ' ' | base64 -D | openssl x509 -in /dev/stdin -text -noout 使用证书 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  apiVersion:extensions/v1beta1kind:Ingressmetadata:name:lognamespace:gaea-opspec:rules:- host:log01.bj-internal.ops.domainhttp:paths:- backend:serviceName:logservicePort:http-2379path:/tls:- hosts:- log01.bj-internal.ops.domainsecretName:tls-ops-domain-prodstatus:loadBalancer:ingress:- ip:10.10.10.159  总结 到这里为止, cert-manager  基本操作已经完成。\n源码部分 godu/helminit\n欢迎关注我的公众号“缘生小助手”，原创实践文章第一时间推送。\n  ","description":"k8s 上利用 cert-manager 自动签发 TLS 证书","id":10,"section":"posts","tags":["kubernetes"],"title":"k8s 上利用 cert-manager 自动签发 TLS 证书","uri":"https://ysicing.me/posts/cert-manager-install/"},{"content":" 趁着节前，需要折(dao)腾(shi)一下去年部署的artifactory镜像源，并将升级到7.x版本。\n 场景 部署私有化软件源，对内提供服务。这里只作为软件源测试使用，其他场景自行琢磨。\n部署服务 部署很简单，这里使用docker-compose方式部署\n# docker-compose.yaml version: '2.1' services: ossv2: image: registry.cn-beijing.aliyuncs.com/k7scn/artifactory-pro:7.6.1 container_name: ossv2 volumes: - /data/ossv2:/var/opt/jfrog/artifactory network_mode: host restart: always nginx: image: registry.cn-beijing.aliyuncs.com/k7scn/nginx:1.17.3 container_name: nginx volumes: - /var/log/nginx:/var/log/nginx:rw - ./config:/etc/nginx/conf.d:rw - ./nginxconfig.io:/etc/nginx/nginxconfig.io:rw - ./ssl:/etc/nginx/ssl:rw - ./wwwroot:/var/www:rw network_mode: host restart: always 注意事项  artifactory 持久化数据目录可用空间最好稍微大些(\u0026gt;500GB)，具体看镜像源数目多少。 使用nginx进行反向代理。  配置源 7.x 和 6.x 版本差别还挺大的。\ndocker-compose up -d 启动服务后，访问 \u0026lt;ip\u0026gt;:8082 进行初始化服务配置，基本默认配置即可或者SKIP跳过。\n 在此过程中有一步比较坑，就是创建默认的Repositories，这个跳过跳过，否则后面在使用过程中就要踩坑了，如配置Debian或者源后Alpine后，如果文件路径里有**.**就会导致文件无法下载404\n artifactory 源 简单介绍一下，\n Local 本地 （通常放一些内部的二进制文件或者其他，当oss用） Remote 远程 (常用，默认就基本使用这个 通常镜像aliyun或者tuna) Virtual 虚拟 (local + remote)  这里我截取部分软件源示例，基本傻瓜式操作\n示例配置Debian源 正常Debian源如下:\n1 2 3 4  deb https://mirrors.ysicing.me/debian/ buster main contrib non-free deb https://mirrors.ysicing.me/debian/ buster-updates main contrib non-free deb https://mirrors.ysicing.me/debian/ buster-backports main contrib non-free deb https://mirrors.ysicing.me/debian-security buster/updates main contrib non-free   如果想达成如上，需要做两件事\n 镜像Debian源 配置域名  镜像Debian源  创建Remote Repositories debian 和 debian-security   配置debian Remote Repositories  同理 debian-security 类似\n 配置完成后访问 \u0026lt;ip\u0026gt;:8082/artifactory/debian/ 或者 \u0026lt;ip\u0026gt;:8081/artifactory/debian/  到这里，我们已经实现了\n1 2  deb http://172.16.72.42:8082/artifactory/debian/ buster main contrib non-free ....   ip+端口的方式终究不太好记，是时候拖出nginx大杀器了\n配置nginx 这里不具体说nginx配置了，直接上配置\n# mirrors.conf server { listen 80; listen [::]:80; listen 443 ssl http2; listen [::]:443 ssl http2; server_name mirrors.ysicing.me; if ($http_x_forwarded_proto = '') { set $http_x_forwarded_proto $scheme; } index index.html; root /var/www/mirrors.ysicing.me/public; # SSL ssl_certificate /etc/nginx/ssl/ysicing.me.crt; ssl_certificate_key /etc/nginx/ssl/ysicing.me.key; # security # include security.conf; location = / { root /var/www/mirrors.ysicing.me/public; } location ~ .*\\.(html|htm|reponew)$ { root /var/www/mirrors.ysicing.me/public; } location ~ (docker.sh|func.sh|data.json)$ { root /var/www/mirrors.ysicing.me/public; } # location ~ ^/$ { # root /var/www/mirrors.ysicing.me/public; # } location /pypi/ { proxy_pass http://127.0.0.1:8081/artifactory/api/pypi/pypi/; include nginxconfig.io/proxy.conf; } # chunked_transfer_encoding on; # client_max_body_size 0; # reverse proxy location / { proxy_buffering off; proxy_buffer_size 128k; proxy_buffers 100 128k; client_max_body_size 100m; proxy_pass http://127.0.0.1:8081/artifactory/; include nginxconfig.io/proxy.conf; location ~ ^/artifactory/ { proxy_pass http://127.0.0.1:8081; include nginxconfig.io/proxy.conf; } } # additional config include nginxconfig.io/general.conf; } 类似两个配置拿自nginxconfig.io\n# proxy.conf proxy_http_version 1.1; proxy_cache_bypass $http_upgrade; proxy_read_timeout 2400s; proxy_pass_header Server; proxy_next_upstream error timeout non_idempotent; proxy_next_upstream_tries 1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026quot;upgrade\u0026quot;; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; # general.conf # favicon.ico location = /favicon.ico { log_not_found off; access_log off; } # robots.txt location = /robots.txt { log_not_found off; access_log off; } # assets, media location ~* \\.(?:css(\\.map)?|js(\\.map)?|jpe?g|png|gif|ico|cur|heic|webp|tiff?|mp3|m4a|aac|ogg|midi?|wav|mp4|mov|webm|mpe?g|avi|ogv|flv|wmv)$ { expires 7d; access_log on; } # svg, fonts location ~* \\.(?:svgz?|ttf|ttc|otf|eot|woff2?)$ { add_header Access-Control-Allow-Origin \u0026quot;*\u0026quot;; expires 7d; access_log off; } # gzip gzip on; gzip_vary on; gzip_proxied any; gzip_comp_level 6; gzip_types text/plain text/css text/xml application/json application/javascript application/rss+xml application/atom+xml image/svg+xml; 到这里软件源配置就基本完成了。\n效果图如下:\nUI借鉴了网易开源镜像站\n踩的pypi坑，小记备忘 pypi源不同于其他，需要额外配置nginx规则\n location /pypi/ { proxy_pass http://127.0.0.1:8081/artifactory/api/pypi/pypi/; include nginxconfig.io/proxy.conf; } 使用\n~/.pip/pip.conf [global] index-url = https://mirrors.ysicing.me/pypi/simple # 测试 git clone https://gitee.com/ysbot/CTFd.git --depth 1 cd CTFd pip3 install -r requirements.txt Go代理 新版本对Go代理做了优化，使用很流程，创建远程Go，源使用https://goproxy.cn\ngo env -w GO111MODULE=on go env -w GOPROXY=https://mirrors.ysicing.me/go/,direct 欢迎关注我的公众号“缘生小助手”，原创实践文章第一时间推送。\n  ","description":"使用Artifactory部署私有化软件源","id":11,"section":"posts","tags":["Artifactory"],"title":"使用Artifactory部署私有化软件源","uri":"https://ysicing.me/posts/artifactory-setup-mirrors/"},{"content":" 趁着搬家，需要折(dao)腾(shi)一下吃灰的群晖，将Clash升级到2.0\n 准备clash配置文件 由于日常使用clashx，这里就不介绍如何获取Clash配置文件了。自行解决，可以参考大佬整理的LAZY_RULES\n注意几点 1 2 3 4 5 6  #HTTP(S) and SOCKS5 共用端口mixed-port:7890# 允许局域网的连接（可用来共享代理）allow-lan:true# clash 的 RESTful APIexternal-controller:0.0.0.0:9090  效果 创建clash-prod服务  获取镜像或者上传镜像，这里默认上传镜像  docker pull registry.cn-beijing.aliyuncs.com/k7scn/clash:1.1.0 docker save registry.cn-beijing.aliyuncs.com/k7scn/clash:1.1.0 \u0026gt; clash.tar  创建容器  点击镜像创建容器后，高级设置。\n  高级设置，勾选启用自动重新启动 卷，持久化配置文件 网络，勾选使用与Docker Host相同的网络(共享网络)   配置完成应用，即可。\n创建clash-ui服务 同理类似，这里简单说下\ndocker pull registry.cn-beijing.aliyuncs.com/k7scn/clash:yacd docker save registry.cn-beijing.aliyuncs.com/k7scn/clash:yacd \u0026gt; yacd.tar  不需要额外配置，当服务起来后，获取clash-ui默认分配的端口，浏览器访问，配置API地址为http://127.0.0.1:9090  到这里就达到了上述效果。\n群晖配置代理 欢迎关注我的公众号“缘生小助手”，原创实践文章第一时间推送。\n  ","description":"","id":12,"section":"posts","tags":["clash"],"title":"群晖配置Clash","uri":"https://ysicing.me/posts/clash-dsm/"},{"content":"Caddy2使用小记  将Caddy升级到 Caddy2， 记录一下。 2.0版本改变挺大的。\n 环境 腾讯云香港轻量, Debian 10, 秒不可言。\n安装caddy2 使用镜像或者源安装(推荐源安装) 1 2 3 4 5  docker pull registry.cn-beijing.aliyuncs.com/k7scn/caddy # 或者 echo \u0026#34;deb [trusted=yes] https://apt.fury.io/caddy/ /\u0026#34; \u0026gt; /etc/apt/sources.list.d/caddy-fury.list apt update apt install caddy   使用二进制安装 1 2 3  wget https://github.com/caddyserver/caddy/releases/download/v2.1.1/caddy_2.1.1_linux_amd64.tar.gz tar xf caddy_2.1.1_linux_amd64.tar.gz mv caddy /usr/local/bin/caddy2   配置systemd\n# /etc/systemd/system/caddy.service [Unit] Description=Caddy Documentation=https://caddyserver.com/docs/ After=network.target [Service] User=caddy Group=caddy EnvironmentFile=-/etc/caddy/caddy.env ExecStart=/usr/local/bin/caddy2 run --environ --config /etc/caddy/Caddyfile ExecReload=/usr/local/bin/caddy2 reload --config /etc/caddy/Caddyfile TimeoutStopSec=5s LimitNOFILE=1048576 LimitNPROC=512 PrivateTmp=true ProtectSystem=full AmbientCapabilities=CAP_NET_BIND_SERVICE [Install] WantedBy=multi-user.target Caddyfile使用  目前 Caddy2 官方推荐配置文件使用 JSON 格式，或者，不使用配置文件，直接调用 admin API。但我推荐使用 Caddyfile, 日常维护起来要方便一些\n 1  caddy2 adapt --config /etc/caddy/Caddyfile --validate #校验是否合法   官方虽然说从v1升级到v2很简单，我信你的鬼\n示例  { # 开启实验性 HTTP/3 experimental_http3 # 测试通过的生产环境中去除该项 # acme_ca https://acme-staging-v02.api.letsencrypt.org/directory } (common_headers) { encode gzip } (secure_headers) { header { Strict-Transport-Security \u0026quot;max-age=31536000; includeSubDomains; preload\u0026quot; X-Frame-Options SAMEORIGIN X-Content-Type-Options nosniff } } ci.hk2.godu.dev { tls ops.ysicing@gmail.com import common_headers import secure_headers reverse_proxy 169.254.0.2:80 } cr.hk2.godu.dev { tls ops.ysicing@gmail.com import common_headers import secure_headers reverse_proxy 127.0.0.1:404 } :80 { respond \u0026quot;2020 hello\u0026quot; } 配置说明 caddy2 引入了全局选项global options的概念，并且如果全局选项存在，必须位于 Caddyfile 的开头\n# 全局配置 { # 开启实验性 HTTP/3 experimental_http3 # 测试通过的生产环境中去除该项 # acme_ca https://acme-staging-v02.api.letsencrypt.org/directory }  支持HTTP/3， 工作在 UDP 协议上，需要放行443 UDP Let’s Encrypt 测试环境，默认caddy2是处于生产环境，测试时可改为测试环境  官方文档\n欢迎关注我的公众号“缘生小助手”，原创实践文章第一时间推送。\n ","description":"Caddy2初体验","id":13,"section":"posts","tags":["caddy"],"title":"Caddy2初体验","uri":"https://ysicing.me/posts/caddy2/"},{"content":"源起 有时候跑一些服务，需要在Linux上运行，需要快速拉起分布式开发测试环境配置，不可能通过创建虚拟机安装服务等一系列操作，否则一行命令，一上午。这时候就需要一款科学的工具来管理虚拟机。有人会说，不是有docker么，为啥还用vagrant\n区别 这里简单说下区别\nvagrant是vm的编排工具,是管理编排环境，对标应该是docker-compose\ndocker是将应用轻量化构建和部署工具\n个人认为两者使用纬度不一样， 而且应用场景哪个方便用哪个。\n用法 上面说到vagrant是vm虚拟机的编排工具，目前支持多个虚拟机软件，常见用virtualbox，也推荐用这个。\n官方文档 Getting Started\n安装vagrant 1 2  # MacOS, 默认已经virtualbox brew cask install vagrant   或者通过官网安装 Download Vagrant\n初始化一个项目 Vagrantfile是用来定义vagrant project的，使用ruby语法，不过你不需要知道ruby就可以写一个Vagrantfile。\n示例来源于 ysicing/debian-vagrant\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box_check_update = false config.vm.provider \u0026#39;virtualbox\u0026#39; do |vb| vb.customize [ \u0026#34;guestproperty\u0026#34;, \u0026#34;set\u0026#34;, :id, \u0026#34;/VirtualBox/GuestAdd/VBoxService/--timesync-set-threshold\u0026#34;, 1000 ] end config.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, type: \u0026#34;nfs\u0026#34;, nfs_udp: false, disabled: true $num_instances = 1 (1..$num_instances).each do |i| config.vm.define \u0026#34;node#{i}\u0026#34; do |node| node.vm.box = \u0026#34;ysicing/debian\u0026#34; node.vm.hostname = \u0026#34;debian105-#{i}\u0026#34; #node.ssh.username = \u0026#34;root\u0026#34; #node.ssh.password = \u0026#34;vagrant\u0026#34; ip = \u0026#34;11.11.11.#{i+10}\u0026#34; node.vm.network \u0026#34;private_network\u0026#34;, ip: ip node.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.gui = false vb.memory = \u0026#34;1024\u0026#34; vb.cpus = 1 vb.name = \u0026#34;node#{i}\u0026#34; vb.customize [\u0026#34;modifyvm\u0026#34;, :id, \u0026#34;--natdnshostresolver1\u0026#34;, \u0026#34;on\u0026#34;] vb.customize [\u0026#34;modifyvm\u0026#34;, :id, \u0026#34;--ioapic\u0026#34;, \u0026#34;on\u0026#34;] end node.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026#34;uname -a\u0026#34; end end end   简单说下核心重要相关参数:\n vm.provider 虚拟机服务 vm.box 虚拟机镜像, 类似docker镜像镜像 cpus,memory,network,instances 虚拟机资源副本信息等  常用基础命令 1 2 3 4 5 6 7 8  # 启动 vagrant up # ssh登录 vagrant ssh # 销毁 vagrant destroy # 关机 vagrant halt   示例部署k8s集群 mkdir vm cat \u0026gt; vm/Vagrantfile \u0026lt;\u0026lt;EOF # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box_check_update = false config.vm.provider 'virtualbox' do |vb| vb.customize [ \u0026quot;guestproperty\u0026quot;, \u0026quot;set\u0026quot;, :id, \u0026quot;/VirtualBox/GuestAdd/VBoxService/--timesync-set-threshold\u0026quot;, 1000 ] end $num_instances = 3 (1..$num_instances).each do |i| config.vm.define \u0026quot;0660b6a92e962a930692651db4562957#{i}\u0026quot; do |node| node.vm.box = \u0026quot;ysicing/debian\u0026quot; node.vm.hostname = \u0026quot;0660b6a92e962a930692651db4562957#{i}\u0026quot; node.vm.network \u0026quot;public_network\u0026quot;, use_dhcp_assigned_default_route: true, bridge: 'en0: Wi-Fi (Wireless)' # node.vm.provision \u0026quot;shell\u0026quot;, run: \u0026quot;always\u0026quot;, inline: \u0026quot;ntpdate ntp.api.bz\u0026quot; node.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;11.11.11.11#{i}\u0026quot; node.vm.provision \u0026quot;shell\u0026quot;, run: \u0026quot;always\u0026quot;, inline: \u0026quot;echo hello from 0660b6a92e962a930692651db4562957#{i}\u0026quot; node.vm.provider \u0026quot;virtualbox\u0026quot; do |vb| vb.gui = false vb.memory = 4096 vb.cpus = 2 vb.name = \u0026quot;0660b6a92e962a930692651db4562957#{i}\u0026quot; vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--natdnshostresolver1\u0026quot;, \u0026quot;on\u0026quot;] vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--ioapic\u0026quot;, \u0026quot;on\u0026quot;] # cpu 使用率50% vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--cpuexecutioncap\u0026quot;, \u0026quot;50\u0026quot;] end end end end EOF # 启动虚拟机 vagrant up # 11.11.11.110，11.11.11.111，11.11.11.112 root/vagrant # pre k8s pkg docker run --rm -v /tmp:/data registry.cn-beijing.aliyuncs.com/k7scn/k7s:1.16.15 cp -a /kube.tgz /data/kube.tgz sealos init --master 11.11.11.110 --node 11.11.11.111 --node 11.11.11.112 --user root --passwd vagrant --version v1.16.15 --repo registry.cn-beijing.aliyuncs.com/k7scn --pkg-url /tmp/kube.tgz 其他场景 本地起多个虚拟机环境测试，其他没必要，感觉没docker-compose方便。\nVagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.provision \u0026quot;shell\u0026quot;, inline: \u0026quot;echo Hello\u0026quot; config.vm.define \u0026quot;web\u0026quot; do |web| web.vm.box = \u0026quot;apache\u0026quot; end config.vm.define \u0026quot;db\u0026quot; do |db| db.vm.box = \u0026quot;mysql\u0026quot; end end debian box简介 默认box使用ysicing/debian, 个人自用debian镜像，源码如下ysicing/debian-vagrant\n定制  默认允许root(密码vagrant)用户登录 默认dns为1.2.4.8, 114.114.114.114 镜像源为 mirrors.aliyun.com 默认硬盘80G 安装了常用运维工具 \u0026hellip;  欢迎关注我的公众号“缘生小助手”，原创实践文章第一时间推送。\n  ","description":"vagrant简单介绍使用","id":14,"section":"posts","tags":["vagrant"],"title":"Vagrant从入门到放弃","uri":"https://ysicing.me/posts/vagrant-study/"},{"content":"安装记录 1 2 3 4  git clone https://github.com/ysicing/istio-install.git cd istio-install istioctl install --set profile=demo --set hub=registry.cn-beijing.aliyuncs.com/k7scn kubectl apply -f .   部署demo kubectl create ns istio-demo kubectl label namespace istio-demo istio-injection=enabled kubectl apply -f example/bookinfo.yaml -n istio-demo ","description":"k8s部署istio 1.7","id":15,"section":"posts","tags":["istio"],"title":"Istio 1.7安装","uri":"https://ysicing.me/posts/istio-install/"},{"content":" 通过helm快速安装helm\n # 默认已存在默认sc kubectl get sc nfs-data (default) nfs-data Delete Immediate false 206d 安装consul\nhelminit kubectl create ns gaea-op helm upgrade -i consul --set global.imageRegistry=registry.cn-beijing.aliyuncs.com \\ --set image.repository=k7scn/b-consul \\ --set volumePermissions.enabled=true \\ --set volumePermissions.image.repository=k7scn/b-minideb \\ --set persistence.enabled=true \\ --set metrics.enabled=true \\ --set metrics.image.repository=k7scn/b-consul-exporter \\ --set ingress.enabled=true \\ --version 7.1.3 \\ bitnami/consul -n gaea-op ","description":"helm安装helm","id":16,"section":"posts","tags":["consul"],"title":"Consul安装","uri":"https://ysicing.me/posts/consul-install/"},{"content":" 基于sealos, 一键高可用，简单上手, 安装k8s 1.18.4版本\n 准备虚拟机 环境: 3台机器(debian/buster 10.4, 11.11.11.111~11.11.11.113, 2核4G80G存储)\n  ergo vm create --vmname k8s --vmnum 3  ➜ ~ ergo vm create --vmname k8s --vmnum 3 I0523 15:20:15.819808 17953 root.go:51] Using config file: 2020-05-23 15:20:15 [DEBG] [github.com/ysicing/ergo/vm/linux.go:66] check system 2020-05-23 15:20:15 [INFO] [github.com/ysicing/ergo/vm/linux.go:73] check system done. It looks good 2020-05-23 15:20:15 [DEBG] [github.com/ysicing/ergo/vm/linux.go:78] check vagrant 2020-05-23 15:20:15 [INFO] [github.com/ysicing/ergo/utils/command.go:15] [os]exec cmd is : which [vagrant] /usr/local/bin/vagrant 2020-05-23 15:20:15 [INFO] [github.com/ysicing/ergo/utils/command.go:15] [os]exec cmd is : which [VirtualBoxVM] /usr/local/bin/VirtualBoxVM 2020-05-23 15:20:15 [DEBG] [github.com/ysicing/ergo/vm/linux.go:85] write Vagrantfile 2020-05-23 15:20:15 [INFO] [github.com/ysicing/ergo/vm/linux.go:92] vagranfile: # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box_check_update = false config.vm.provider 'virtualbox' do |vb| vb.customize [ \u0026quot;guestproperty\u0026quot;, \u0026quot;set\u0026quot;, :id, \u0026quot;/VirtualBox/GuestAdd/VBoxService/--timesync-set-threshold\u0026quot;, 1000 ] end $num_instances = 3 (1..$num_instances).each do |i| config.vm.define \u0026quot;k8s#{i}\u0026quot; do |node| node.vm.box = \u0026quot;ysicing/debian\u0026quot; node.vm.hostname = \u0026quot;k8s#{i}\u0026quot; node.vm.network \u0026quot;public_network\u0026quot;, use_dhcp_assigned_default_route: true, bridge: 'en0: Wi-Fi (Wireless)' # node.vm.provision \u0026quot;shell\u0026quot;, run: \u0026quot;always\u0026quot;, inline: \u0026quot;ntpdate ntp.api.bz\u0026quot; node.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;11.11.11.11#{i}\u0026quot; node.vm.provision \u0026quot;shell\u0026quot;, run: \u0026quot;always\u0026quot;, inline: \u0026quot;echo hello from k8s#{i}\u0026quot; node.vm.provider \u0026quot;virtualbox\u0026quot; do |vb| vb.gui = false vb.memory = 4096 vb.cpus = 2 vb.name = \u0026quot;k8s#{i}\u0026quot; vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--natdnshostresolver1\u0026quot;, \u0026quot;on\u0026quot;] vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--ioapic\u0026quot;, \u0026quot;on\u0026quot;] # cpu 使用率50% vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--cpuexecutioncap\u0026quot;, \u0026quot;50\u0026quot;] end end end end 2020-05-23 15:20:15 [DEBG] [github.com/ysicing/ergo/vm/linux.go:117] vagrant up Bringing machine 'k8s1' up with 'virtualbox' provider... Bringing machine 'k8s2' up with 'virtualbox' provider... Bringing machine 'k8s3' up with 'virtualbox' provider... ... 2020-05-23 15:26:37 [INFO] [github.com/ysicing/ergo/vm/linux.go:130] ip: 11.11.11.111-11.11.11.113, root/vagrant 2020-05-23 15:26:37 [INFO] [github.com/ysicing/ergo/vm/linux.go:133] 销毁方式: cd /Users/ysicing/vm, vagrant destroy -f    初始化环境   ergo vm init --ip 11.11.11.111 --ip 11.11.11.112 --ip 11.11.11.113  =============================================================================== prepare : 安装基础软件 -------------------------------------------------------- 8.77s prepare : Install dbus for the hostname module -------------------------- 5.84s prepare : 删除默认安装 -------------------------------------------------------- 3.60s Gathering Facts --------------------------------------------------------- 1.15s prepare : 加载内核模块 -------------------------------------------------------- 0.92s prepare : gather facts -------------------------------------------------- 0.67s prepare : 增加内核模块开机加载配置 -------------------------------------------------- 0.63s prepare : 启用systemd自动加载模块服务 --------------------------------------------- 0.53s prepare : 禁用系统 swap ----------------------------------------------------- 0.47s prepare : 设置系统 ulimits -------------------------------------------------- 0.41s prepare : 优化设置 journal 日志 ----------------------------------------------- 0.37s prepare : 把SCTP列入内核模块黑名单 ------------------------------------------------ 0.36s prepare : 删除fstab swap 相关配置 --------------------------------------------- 0.34s prepare : 设置系统参数 -------------------------------------------------------- 0.32s prepare : 准备 journal 日志相关目录 --------------------------------------------- 0.32s prepare : 创建 systemd 配置目录 ----------------------------------------------- 0.30s prepare : 重启 journald 服务 ------------------------------------------------ 0.30s prepare : update /etc/security/limits.conf ------------------------------ 0.28s prepare : 生效系统参数 -------------------------------------------------------- 0.19s prepare : 设置 ulimits ---------------------------------------------------- 0.12s    安装docker   ergo install docker --ip 11.11.11.111 --ip 11.11.11.112 --ip 11.11.11.113 --pass vagrant  ... + sh -c 'docker version' Client: Docker Engine - Community Version: 19.03.9 API version: 1.40 Go version: go1.13.10 Git commit: 9d988398e7 Built: Fri May 15 00:25:25 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.9 API version: 1.40 (minimum version 1.12) Go version: go1.13.10 Git commit: 9d988398e7 Built: Fri May 15 00:23:57 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.13 GitCommit: 7ad184331fa3e55e52b890ea95e65ba581ae3429 runc: Version: 1.0.0-rc10 GitCommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init: Version: 0.18.0 GitCommit: fec3683 If you would like to use Docker as a non-root user, you should now consider adding your user to the \u0026quot;docker\u0026quot; group with something like: sudo usermod -aG docker your-user Remember that you will have to log out and back in for this to take effect! WARNING: Adding a user to the \u0026quot;docker\u0026quot; group will grant the ability to run containers which can be used to obtain root privileges on the docker host. Refer to https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface for more information. Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install. Executing: /lib/systemd/systemd-sysv-install enable docker \u0026quot;19.03.9\u0026quot; Unable to find image 'ysicing/tools:latest' locally latest: Pulling from ysicing/tools 0d6b48c80e92: Pull complete f28c196b1246: Pull complete 04bb1e97c214: Pull complete Digest: sha256:c689021150ec06af298db958d24c19fdbe3aea7c6717c5dee53f7e9071b4cd76 Status: Downloaded newer image for ysicing/tools:latest    安装常用小工具   ergo install tools --ip 11.11.11.111 --ip 11.11.11.112 --ip 11.11.11.113 --pass vagrant  I0523 15:35:59.919563 19284 root.go:51] Using config file: I0523 15:35:59.920024 19284 install.go:55] 🎉 安装 tools 2020-05-23 15:35:59 [INFO] [/Users/ysicing/go/pkg/mod/github.com/kunnos/sshcmd@v1.6.0/pkg/sshutil/ssh.go:12] [ssh][11.11.11.111]exec cmd is : docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir 2020-05-23 15:36:08 [DEBG] [/Users/ysicing/go/pkg/mod/github.com/kunnos/sshcmd@v1.6.0/pkg/sshutil/ssh.go:25] [ssh][11.11.11.111]command result is: 2020-05-23 15:36:08 [INFO] [/Users/ysicing/go/pkg/mod/github.com/kunnos/sshcmd@v1.6.0/pkg/sshutil/ssh.go:12] [ssh][11.11.11.112]exec cmd is : docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir 2020-05-23 15:36:17 [DEBG] [/Users/ysicing/go/pkg/mod/github.com/kunnos/sshcmd@v1.6.0/pkg/sshutil/ssh.go:25] [ssh][11.11.11.112]command result is: 2020-05-23 15:36:17 [INFO] [/Users/ysicing/go/pkg/mod/github.com/kunnos/sshcmd@v1.6.0/pkg/sshutil/ssh.go:12] [ssh][11.11.11.113]exec cmd is : docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir 2020-05-23 15:36:26 [DEBG] [/Users/ysicing/go/pkg/mod/github.com/kunnos/sshcmd@v1.6.0/pkg/sshutil/ssh.go:25] [ssh][11.11.11.113]command result is:    ergo shell --ip 11.11.11.111 --ip 11.11.11.112 --ip 11.11.11.113 --pass vagrant --cmd upgrade-tools\n或者在当前机器直接执行\nupgrade-tools 安装k8s  说明，我定制了sealos和calico版本为最新版本,且使用了在线安装包\n # 安装了k8s 1.18.4, 安装了ingress, 配置nfs，默认存储类为nfs-data ergo install k8s --enablenfs=true --mip 11.11.11.111 --wip 11.11.11.112-11.11.11.113 --pass vagrant --ingresstype --enablekuboard --regioncn 安装其他ingress 支持nginx-ingress, traefik, ingress-nginx\nergo install ingress --ip 11.11.11.111 --pk ~/.ssh/id_rsa --regioncn --ingresstype 验证 root@k8s1:~# kubectl get node NAME STATUS ROLES AGE VERSION k8s1 Ready master 2m2s v1.18.4 k8s2 Ready \u0026lt;none\u0026gt; 87s v1.18.4 k8s3 Ready \u0026lt;none\u0026gt; 87s v1.18.4 root@k8s1:~# kubectl get ns NAME STATUS AGE default Active 2m6s ingress-nginx Active 2m13s kube-node-lease Active 2m7s kube-public Active 2m7s kube-system Active 2m8s root@k8s1:~# kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\u0026quot;health\u0026quot;:\u0026quot;true\u0026quot;} root@k8s1:~# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nfs-data (default) nfs-provisioner Delete Immediate false 60s ","description":"本地使用ergo快速安装k8s1.18.4","id":17,"section":"posts","tags":["kubernetes"],"title":"使用ergo快速安装k8s1.18.4","uri":"https://ysicing.me/posts/k8s-install-ergo/"},{"content":"简介 k8s的LoadBalancer类型的Service依赖云服务商的Load Balancer, 如阿里云的slb。\n当我们把k8s部署在私有云时，需要简单的LoadBalancer来验证工作，开源的metallb就是一个不错的选择。\n通过k8s原生的方式提供LB类型的Service支持，开箱即用。\n原理  Layer2 (默认使用) BGP  Layer2 只需要一段跟K8s管理网相同网段的地址即可, 通常这种就可以了。\n在此模式下, 会从k8s节点中选一个Leader节点，在这个节点上面响应LB地址段的ARP请求，从而使上层路由把发往LB的流量都发到Leader节点。缺点也很明显，所有对LB的请求都会发往Leader节点。如果当前Service下面的Pod分布在不同节点，那么这个流量还会从Leader发往相应的节点。\n局限性: 单节点流量瓶颈和慢故障转移\nBGP 需要上层路由器支持BGP，不在需要Leader节点。\n安装 kubectl get ns | grep metallb-system \u0026amp;\u0026amp; exit 0 kubectl apply -f https://gitee.com/ysicing/ergo/raw/master/hack/k8s/metallb/metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026quot;$(openssl rand -base64 128)\u0026quot; kubectl apply -f https://gitee.com/ysicing/ergo/raw/master/hack/k8s/metallb/lbconfig.yaml # 或者使用ergo ergo install mlb --pass vagrant 配置说明 # https://gitee.com/ysicing/ergo/raw/master/hack/k8s/metallb/lbconfig.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: lbpool protocol: layer2 addresses: - 11.11.11.150-11.11.11.200 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # lb.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginxreplicas:2template:metadata:labels:app:nginxspec:containers:- name:nginximage:nginxports:- containerPort:80---apiVersion:v1kind:Servicemetadata:name:nginx-servicespec:selector:app:nginxports:- port:80targetPort:80type:LoadBalancer  测试\nroot@k8s1:~# kubectl apply -f lb.yaml root@k8s1:~# kubectl get pods,svc NAME READY STATUS RESTARTS AGE pod/nginx-deployment-d46f5678b-hm7r6 1/1 Running 0 96s pod/nginx-deployment-d46f5678b-zgfxd 1/1 Running 0 96s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 88m service/nginx-service LoadBalancer 10.98.184.75 11.11.11.150 80:32002/TCP 6s ","description":"","id":18,"section":"posts","tags":["kubernetes","LoadBalancer"],"title":"开源k8slb工具Metallb","uri":"https://ysicing.me/posts/k8s-slb-metallb/"},{"content":"简介 etcd是CoreOS团队发起的开源项目，目标是构建一个高可用的分布式键值(key-value)数据库。etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。\netcd比较多的应用场景是用于服务发现。\n使用helm安装etcd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # 查询 helm search repo etcd NAME CHART VERSION APP VERSION DESCRIPTION bitnami/etcd 4.8.2 3.4.9 etcd is a distributed key value store that prov... incubator/etcd 0.7.4 3.2.26 Distributed reliable key-value store for the mo... stable/etcd-operator 0.10.3 0.9.4 CoreOS etcd-operator Helm chart for Kubernetes stable/zetcd 0.1.9 0.0.3 CoreOS zetcd Helm chart for Kubernetes # 下载helm包,执行定制 helm pull bitnami/etcd # 使用nodeport, 禁用rbac，禁用存储 helm install etcd bitnami/etcd --set service.type=NodePort --set service.type=NodePort --set auth.rbac.enabled=false --set persistence.enabled=false -n ops-open export NODE_IP=$(kubectl get nodes --namespace ops-open -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) export NODE_PORT=$(kubectl get --namespace ops-open -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services etcd) export ETCDURL=http://$NODE_IP:$NODE_PORT echo \u0026#34;etcd URL: $ETCDURL\u0026#34; export ECTDCTL_API=3 etcdctl member list --endpoints=$ETCDURL   ","description":"Etcd使用入门","id":19,"section":"posts","tags":["etcd"],"title":"Etcd使用入门","uri":"https://ysicing.me/posts/etcd-op/"},{"content":"部署 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  # docker.yamlapiVersion:apps/v1kind:StatefulSetmetadata:name:docker-apinamespace:gaea-opannotations:k8s.gaea.me/workload:applabels:k8s.gaea.me/name:docker-apispec:replicas:1template:metadata:labels:k8s.gaea.me/name:docker-apispec:containers:- name:dockerdimage:\u0026#39;docker:18.09.9-dind\u0026#39;securityContext:privileged:true- name:docker-cliimage:\u0026#39;docker:18.09.9\u0026#39;env:- name:DOCKER_HOSTvalue:tcp://127.0.0.1:2375command:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;docker info \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; while [ $? -ne 0 ] ; do sleep 3; docker info \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; done; docker pull library/busybox:latest; docker save -o busybox-latest.tar library/busybox:latest; docker rmi library/busybox:latest; while true; do sleep 86400; done\u0026#34;]selector:matchLabels:k8s.gaea.me/name:docker-apiserviceName:docker-api---apiVersion:v1kind:Servicemetadata:name:docker-apinamespace:gaea-opspec:ports:- name:tcp-2375port:2375clusterIP:Noneselector:k8s.gaea.me/name:docker-api  1  kubectl apply -f docker.yaml   使用 1 2 3  # docker api地址: docker-api-0.docker-api.gaea-op.svc.cluster.local kubectl exec -it pods/docker-api-0 -n gaea-op -c docker-cli -- ash / # docker info   go api操作 ","description":"使用docker in docker","id":20,"section":"posts","tags":["docker","kubernetes"],"title":"在k8s中使用docker in docker","uri":"https://ysicing.me/posts/docker-in-k8s/"},{"content":" 通过Operator创建Prometheus\n 脚本部署 # 在之前的基础上，配置了存储storageclass git clone https://github.com/ysicing/prometheus.git cd prometheus # prometheus-service/prometheus-storageclass.yaml #配置存储 bash -x ./deploy.sh # 在管理节点执行，如有报错，在执行一下 与官方的区别支持了数据持久化和域名配置,部分监控组件如etcd等\nergo安装 ergo install prom --ip 10.147.20.42 --pass oac5eeWosie7aiP5Um2AeKahkaiCeigh --domain godu.dev ","description":"Prometheus Operator 初体验","id":21,"section":"posts","tags":["kubernetes","prometheus"],"title":"Prometheus Operator 初体验","uri":"https://ysicing.me/posts/prometheus-install/"},{"content":"安装  安装docker时默认已经安装了helm,如果不是最新版本请upgrade-tools\n 1 2 3 4  upgrade-tools # 版本 helm version version.BuildInfo{Version:\u0026#34;v3.2.1\u0026#34;, GitCommit:\u0026#34;fe51cd1e31e6a202cba7dead9552a6d418ded79a\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.13.10\u0026#34;}   配置helm镜像库 1 2 3 4 5 6  # 自动 helminit # 手动 helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/   ","description":"记录helm相关使用","id":22,"section":"posts","tags":["helm"],"title":"helm小记","uri":"https://ysicing.me/posts/helm/"},{"content":"域名相关 默认情况下, 本地环境域名测试\n*.k7s.xyz 11.11.11.111 *.slb.k7s.xyz 11.11.11.112 *.vlb.k7s.xyz 11.11.11.113 ergo安装 Usage: ergo install ingress [flags] Flags: -h, --help help for ingress --ingresstype string ingress: nginx-ingress, traefik, ingress-nginx (default \u0026quot;ingress-nginx\u0026quot;) Global Flags: --config string config file (default is $HOME/.doge/config.yaml) --ip strings 需要安装节点ip (default [11.11.11.111]) --pass string 管理员密码 --pk string 管理员私钥 --regioncn 默认使用gitee源 (default true) --user string 管理员 (default \u0026quot;root\u0026quot;) 示例: ergo install ingress --ip 10.147.20.43 --pk ~/.ssh/id_rsa --regioncn --ingresstype traefik\n历史安装 NGINX Ingress Controller (ingress-nginx) 使用helm方式安装\nkubectl apply -f https://sh.ysicing.me/k8s/helm/nginx-ingress/ns.yaml # 安装 helm install nginx-ingress -f https://sh.ysicing.me/k8s/helm/nginx-ingress/nginx-ingress-1.34.2.yaml stable/nginx-ingress -n ingress-nginx # 升级 helm upgrade nginx-ingress -f https://sh.ysicing.me/k8s/helm/nginx-ingress/nginx-ingress-1.34.2.yaml stable/nginx-ingress -n ingress-nginx NGINX Ingress Controllers (kubernetes-ingress) helm repo add nginx-stable https://helm.nginx.com/stable helm repo update helm install nginx-ingress -f https://sh.ysicing.me/k8s/helm/nginxnc-ingress/nginx-ingress-0.4.3.yaml nginx-stable/nginx-ingress -n ingress-nginx # 或者 kubectl apply -f https://sh.ysicing.me/k7s/install/nginx-ingress/nginx-ingress.yml 部署文档 nginxinc/kubernetes-ingress v1.6.0\n","description":"k8s部署ingress","id":23,"section":"posts","tags":["kubernetes","ingress"],"title":"ergo安装ingress安装","uri":"https://ysicing.me/posts/ingress-install/"},{"content":"1. 背景  云服务商提供的机器可能安装了一些服务组件，如监控等等，想要一个干净的环境。 一条命令快速重装干净的Linux环境 目前仅支持Debian(不会不打算支持其他系统) 基于萌咖大佬的二次魔改  2. 定制  默认root密码 vagrant(安装完成建议修改，禁止密码登录) 默认配置源为mirrors.tuna.tsinghua.edu.cn,默认添加了security,backports 默认时区为Asia/Shanghai 默认安装了curl wget openssh-server sudo sed apt-transport-https net-tools等常用工具 同时默认支持自定义密码  3. 安装 1 2 3 4 5  curl -sSL https://sh.ysicing.me/reinstall/install.sh | bash # 指定参数 # -p 默认密码vagrant # -m 默认源aliyun bash \u0026lt;(wget --no-check-certificate -qO- \u0026#39;https://sh.ysicing.me/reinstall/install.sh\u0026#39;) -p thah6oob7KieChie   3.1 特殊: 自定义硬盘  存在多个硬盘时，需要下载 https://sh.ysicing.me/reinstall/install-sdev.sh文件，编辑如下部分即可\n 1 2 3 4  # -p 默认密码vagrant # -m 默认源aliyun # -s 指定启动硬盘 bash \u0026lt;(wget --no-check-certificate -qO- \u0026#39;https://sh.ysicing.me/reinstall/install-sdev.sh\u0026#39;) -p thah6oob7KieChie -s /dev/sdb   4. 参考附录 [ Linux VPS ] Debian/Ubuntu/CentOS 网络安装/重装系统/纯净安装 一键脚本\n","description":"介绍如何给云服务商提供的机器在线重装Debian，以轻量云为例","id":24,"section":"posts","tags":["debian"],"title":"如何在线重装Debian11","uri":"https://ysicing.me/posts/debian-reinstall/"},{"content":"安装docker curl -fsSL https://sh.ysicing.me/install/docker.sh | bash 或者 ergo install docker -h 做了如下设置\n1 2 3 4 5 6 7 8 9 10 11 12 13  { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://dyucrs4l.mirror.aliyuncs.com\u0026#34;], \u0026#34;bip\u0026#34;: \u0026#34;169.254.0.1/24\u0026#34;, \u0026#34;max-concurrent-downloads\u0026#34;: 10, \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-level\u0026#34;: \u0026#34;warn\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;20m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;2\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; }   安装tools docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir 内置了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  etcdctl helm(v3) helminit docker-compose calicoctl ctop cclear (清理退出容器) din (进入容器 din \u0026lt;cname/\u0026gt;cid\u0026gt; ash/bash(默认)/sh) dps (容器状态) etcdcli (特别支持k8s集群的etcd) kbtoken (查看kuboard admin用户token) kdtoken (查看dashboard-admin 用户token) istioctl linkerd upgrade-tools // 升级二进制   ","description":"使用ergo快速安装docker","id":25,"section":"posts","tags":["docker"],"title":"docker快速安装","uri":"https://ysicing.me/posts/docker/"},{"content":"先已经创建好内网slb,获取slb的ID为lb-xxxx\n# intranet-slb-ingress.yml apiVersion: v1 kind: Service metadata: # 这里服务取名为nginx-ingress-lb-intranet name: nginx-ingress-lb-intranet namespace: kube-system labels: app: nginx-ingress-lb-intranet annotations: # 指明SLB实例地址类型为私网类型 service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet # 修改为您的私网SLB实例ID service.beta.kubernetes.io/alicloud-loadbalancer-id: lb-xxxx # 是否自动创建SLB端口监听（会覆写已有端口监听），也可手动创建端口监听 service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: 'true' spec: type: LoadBalancer # route traffic to other nodes externalTrafficPolicy: \u0026quot;Cluster\u0026quot; ports: - port: 80 name: http targetPort: 80 - port: 443 name: https targetPort: 443 selector: # select app=ingress-nginx pods app: ingress-nginx 生效内网slb\nkubectl apply -f intranet-slb-ingress.yml 查看slb\nnginx-ingress-lb LoadBalancer 172.x.x.x 39.x.x.x 80:31110/TCP,443:31574/TCP 20h nginx-ingress-lb-intranet LoadBalancer 172.x.x.x 10.x.x.x 80:30740/TCP,443:30852/TCP 73m ","description":"ACK 添加内网负载均衡","id":26,"section":"posts","tags":["ingress","kubernetes","ack"],"title":"ACK 添加内网负载均衡","uri":"https://ysicing.me/posts/aliyun-ack-ingress/"},{"content":"k3s 是Rancher推出的轻量级 k8s.\n升级内核版本 apt update apt dist-upgrade apt install -t buster-backports linux-image-amd64 -y update-grub reboot # 内核 Linux cn2 5.6.0-0.bpo.2-amd64 #1 SMP Debian 5.6.14-2~bpo10+1 (2020-06-09) x86_64 GNU/Linux 具体可以参考 Debian个人常用操作指南 升级内核部分。\n安装 wireguard # 所有节点需安装 apt install wireguard -y 部署控制平面master节点 cat \u0026gt; /etc/systemd/system/k3s.service \u0026lt;\u0026lt;EOF [Unit] Description=Lightweight Kubernetes Documentation=https://k3s.io Wants=network-online.target [Install] WantedBy=multi-user.target [Service] Type=notify EnvironmentFile=-/etc/systemd/system/k3s.service.env KillMode=process Delegate=yes # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity TimeoutStartSec=0 Restart=always RestartSec=5s ExecStartPre=-/sbin/modprobe br_netfilter ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/k3s \\ server \\ --tls-san 10.147.20.42 \\ --node-ip 10.147.20.42 \\ --node-external-ip 10.147.20.42 \\ --docker \\ --pause-image registry.cn-beijing.aliyuncs.com/k7scn/pause:3.2 \\ --flannel-backend wireguard \\ --kube-proxy-arg \u0026quot;proxy-mode=ipvs\u0026quot; \u0026quot;masquerade-all=true\u0026quot; \\ --kube-proxy-arg \u0026quot;metrics-bind-address=0.0.0.0\u0026quot; EOF 开机启动\n1  systemctl enable k3s --now   查看组件状态\nk3s kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok 部署计算worker节点 cat \u0026gt; /etc/systemd/system/k3s-agent.service \u0026lt;\u0026lt;EOF [Unit] Description=Lightweight Kubernetes Documentation=https://k3s.io Wants=network-online.target [Install] WantedBy=multi-user.target [Service] Type=exec EnvironmentFile=-/etc/systemd/system/k3s-agent.service.env KillMode=process Delegate=yes LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity TimeoutStartSec=0 Restart=always RestartSec=5s ExecStartPre=-/sbin/modprobe br_netfilter ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/k3s agent \\ --node-external-ip 10.147.20.43 \\ --node-ip 10.147.20.43 \\ --docker \\ --pause-image registry.cn-beijing.aliyuncs.com/k7scn/pause:3.2 \\ --kube-proxy-arg \u0026quot;proxy-mode=ipvs\u0026quot; \u0026quot;masquerade-all=true\u0026quot; \\ --kube-proxy-arg \u0026quot;metrics-bind-address=0.0.0.0\u0026quot; EOF  K3S_TOKEN : 加入集群所需的token，可以在控制节点上查看/var/lib/rancher/k3s/server/node-token 文件  cat \u0026gt; /etc/systemd/system/k3s-agent.service.env \u0026lt;\u0026lt;EOF K3S_URL=https://10.147.20.42:6443 K3S_TOKEN=K102f2c1f6c878f693700c24b741d309d2ff4038ade912f0a44248781c04376e878::server:bc39d44d89042011b985f267eebe2b2f EOF 开机启动\nsystemctl enable k3s-agent --now ","description":"debian10安装k3s","id":27,"section":"posts","tags":["kubernetes","k3s"],"title":"debian10安装k3s","uri":"https://ysicing.me/posts/k3s-install/"},{"content":"变量和常量 关键字\u0026amp;保留字\n# 25个保留字 break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var # 37个保留字 Constants: true false iota nil Types: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error Functions: make len cap new append copy close delete complex real imag panic recover 注意\n1. 函数外的每个语句都必须以关键字开始（var、const、func等） 2. :=不能使用在函数外。 3. _多用于占位，表示忽略值(匿名变量，不占用命名空间，不会分配内存，不需要存在重复申明) 变量 1 2 3 4 5 6 7 8 9 10 11 12 13  package main import ( \u0026#34;fmt\u0026#34; ) // 全局变量m var m = 100 func main() { n := 10 m := 200 // 此处声明局部变量m \tfmt.Println(m, n) }   常量 常量是恒定不变的值\n1 2 3 4 5 6 7 8  const pi = 3.1415 const ( // const声明多个常量时，如果省略了值则表示和上面一行的值相同  p1 = 3.1415 p2 // 3.1415  p3 // 3.1415 )   iota iota是go语言的常量计数器，只能在常量的表达式中使用。iota在const关键字出现时将被重置为0。const中每新增一行常量声明将使iota计数一次(iota可理解为const语句块中的行索引)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  const ( n1 = iota // 0  n2 // 1  n3 // 2  _ // 跳过某些值  n4 // 3  n5 = 10 // 10  n6 // 4 ) const ( _ = iota // 0  KB = 1 \u0026lt;\u0026lt; (10 * iota) // 1 1\u0026lt;\u0026lt;10 2^10 1024  MB = 1 \u0026lt;\u0026lt; (10 * iota) // 2 1\u0026lt;\u0026lt;20 2^20 1024*1024 ) const ( a, b = iota + 1, iota + 2 // 1,2  c, d // 2, 3 )   基本数据类型 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;math\u0026quot; ) func main() { var a int = 17 // int 型 (int64) fmt.Printf(\u0026quot;%d \\n\u0026quot;, a) // 17 十进制 fmt.Printf(\u0026quot;%b \\n\u0026quot;, a) // 10001 二进制 fmt.Printf(\u0026quot;%o \\n\u0026quot;, a) // 021 八进制 fmt.Printf(\u0026quot;%x \\n\u0026quot;, a) // 0x11 16禁止 fmt.Printf(\u0026quot;%.2f \\n\u0026quot;, math.Pi) // 默认都是浮点型float64 var s = \u0026quot;666\u0026quot; fmt.Printf(\u0026quot;%T \\n\u0026quot;, s) // 类型 string fmt.Printf(\u0026quot;%v \\n\u0026quot;, s) // 值 666 fmt.Printf(\u0026quot;%#v \\n\u0026quot;, s) // 值 \u0026quot;666\u0026quot; } 注:\n布尔类型变量的默认值为false。 Go 语言中不允许将整型强制转换为布尔型. 布尔型无法参与数值运算，也无法与其他类型进行转换 字符串 字符串需要使用\u0026quot;\ns1 := \u0026quot;ysicing\u0026quot; 多行字符串需要使用```\ns1 := `牛 牛 牛 牛 ` 字符串操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) func main() { a := \u0026#34;666\u0026#34; b := \u0026#34;777\u0026#34; c := a + \u0026#34;-\u0026#34; + b fmt.Println(len(a)) // len 长度 \tfmt.Println(fmt.Sprintf(\u0026#34;%s%s\u0026#34;, a, b)) // 拼接 \tfmt.Println(a + b) // 拼接 \tfmt.Print(strings.Split(c, \u0026#34;-\u0026#34;)[0]) // 分割 \tres := strings.Split(c, \u0026#34;-\u0026#34;) // 分割 \tfmt.Println(res) fmt.Println(strings.HasPrefix(c, \u0026#34;6\u0026#34;), strings.HasSuffix(c, \u0026#34;6\u0026#34;)) //前缀/后缀判断  s := \u0026#34;China万岁\u0026#34; for i := 0; i \u0026lt; len(s); i++ { // byte \tfmt.Printf(\u0026#34;%T %v(%c) \\n\u0026#34;, s[i], s[i], s[i]) } fmt.Println() for _, j := range s { // rune \tfmt.Printf(\u0026#34;%T %v(%c) \\n\u0026#34;, j, j, j) } fmt.Println() }    uint8类型, 或者叫 byte型 代表了ASCII码的一个字符. rune类型, 代表一个 UTF-8字符(中文), 实际是一个int32. 因为UTF8编码下一个中文汉字由3~4个字节组成,一个rune字符由一个或多个byte组成.  流程控制 if if 表达式1 { 分支1 } else if 表达式2 { 分支2 } else{ 分支3 } for 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  package main import \u0026#34;fmt\u0026#34; func main() { // 基本格式 \tfor i := 1; i \u0026lt;= 3; i++ { fmt.Println(i) } j := 1 for ; j \u0026lt;= 3; j++ { fmt.Println(j) } k := 1 for k \u0026lt;= 3 { fmt.Println(k) k++ } s := \u0026#34;HellGo\u0026#34; for i, j := range s { fmt.Printf(\u0026#34;%d---\u0026gt;%v(%T)--\u0026gt;%c\\n\u0026#34;, i, j, j, j) // rune \t} // 死循环 \t//for { \t//\tfmt.Println(\u0026#34;666\u0026#34;) \t//} }   switch case 简化判断\na := 4 switch a { case a \u0026lt; 1: fmt.Println(\u0026quot;1\u0026quot;) case 2,3: fmt.Println(\u0026quot;2\u0026quot;) default: fmt.Println(\u0026quot;3\u0026quot;) // 3 } b := 3 switch a { case a \u0026lt; 1: fmt.Println(\u0026quot;1\u0026quot;) case 2,3: fmt.Println(\u0026quot;2\u0026quot;) fallthrough // 可以执行满足条件的case的下一个case, 了解即可 default: fmt.Println(\u0026quot;3\u0026quot;) } // 23 goto 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  func main() { for i := 1; i \u0026lt;= 9999; i++ { for j := 3000; j \u0026lt;= i; j++ { if j \u0026lt; i { fmt.Printf(\u0026#34;%d * %d = %d \u0026#34;, i, j, i*j) } else if j == i { fmt.Printf(\u0026#34;%d * %d = %d\\n\u0026#34;, i, j, i*j) goto breakTag } } } breakTag: // label \tfmt.Println(\u0026#34;end\u0026#34;) }   运算符 算数运算符: + - * / %\n关系运算符: == != \u0026gt; \u0026gt;= \u0026lt; \u0026laquo; (结果为bool值)\n逻辑运算符: \u0026amp;\u0026amp; || ! (结果为bool值)\n位运算符: \u0026amp; (相与 同为1为1)｜(相或 有1为1) ^（相异 或不同为1） \u0026laquo; (乘2的n次方) \u0026raquo; (除2的n次方)\n赋值运算符: 先其他运算再赋值\n5 \u0026lt;\u0026lt; 2 // 5 --- 101 左移 2 10100 --- 10 5 * 2^ 1 5 \u0026gt;\u0026gt; 2 // 5 --- 101 右移 2 001 --- 1 注: ++,-- 单独语句，不是运算符\n数组 var 数组变量名 [元素数量]T // 一旦定义，长度不能变 var a [3]int 数组初始化\nvar a1 [3]int //数组会初始化为int类型的零值 var a2 = [3]int{1,2} //使用指定的初始值完成初始化 var a3 = [...]int{1, 2} // 让编译器根据初始值的个数自行推断数组的长度 a4 := [...]int{1: 1, 3: 5} // 指定索引值的方式来初始化数组 [0, 1, 0 ,3] 数组遍历\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  package main import \u0026#34;fmt\u0026#34; func main() { var a = [...]int{1, 2, 3, 4, 5} // for循环遍历 \tfor i := 0; i \u0026lt; len(a); i++ { fmt.Println(a[i]) } // range \tfor i, j := range a { fmt.Println(i, j) } b := [...][2]int{ {1,2}, {2,1}, } for _, i := range b { for _, j := range i { fmt.Printf(\u0026#34;%d\\t\u0026#34;, j) } } }    数组是值类型，赋值和传参会复制整个数组。因此改变副本的值，不会改变本身的值。\n 切片slice 切片是引用类型，拥有相同类型元素的可变长度的序列\nvar 切片名 []切片类型 var name []string # 动态创建 make([]T, size, cap) // T 类型，size 长度， cap容量(从第一个到最后容量数)，cap 可省却 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  package main import \u0026#34;fmt\u0026#34; func main() { //var a []int \tvar c = []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;} fmt.Println(len(c), cap(c)) d := make([]int, 2, 10) fmt.Println(len(d), cap(d)) e := d[3:6] fmt.Println(len(e), cap(e)) // 3， 3-10 7  s1 := make([]int, 3) // [0,0,0]  s2 := s1 // 拷贝前后两个变量共享底层数组，对一个切片的修改会影响另一个切片的内容 \ts2[0] = 100 fmt.Println(s1) fmt.Println(s2) // 遍历和for一样  for i := 0; i \u0026lt; len(s1); i++ { fmt.Printf(\u0026#34;%d\\t\u0026#34;, s1[i]) } for _, i := range s2 { fmt.Printf(\u0026#34;%d\\t\u0026#34;, i) } }    切片的本质就是对底层数组的封装\n append 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  package main import \u0026#34;fmt\u0026#34; func main() { var num []int for i := 0; i \u0026lt; 20; i++ { // 追加一个元素 \tnum = append(num, i) fmt.Printf(\u0026#34;%v len:%d cap:%d ptr:%p\\n\u0026#34;, num, len(num), cap(num), num) } // 追加多个元素  num = append(num, 1, 2, 3) fmt.Printf(\u0026#34;%v, \\n\u0026#34;, num) num1 := []int{99, 98} // 追加切片 \tnum = append(num, num1...) fmt.Println(num) }   copy copy(目标切片,源切片) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  func main() { // copy()复制切片 \ta := []int{1, 2, 3, 4, 5} c := make([]int, 5, 5) copy(c, a) //使用copy()函数将切片a中的元素复制到切片c \tfmt.Println(a) //[1 2 3 4 5] \tfmt.Println(c) //[1 2 3 4 5] \tc[0] = 1000 fmt.Println(a) //[1 2 3 4 5]  fmt.Println(c) //[1000 2 3 4 5]  var a = make([]string, 5, 10) // 已经有5个了 \tfor i := 0; i \u0026lt; 10; i++ { a = append(a, fmt.Sprintf(\u0026#34;%v\u0026#34;, i)) } fmt.Println(a, cap(a), len(a)) // [ 0 1 2 3 4 5 6 7 8 9] 20 15 }   指针  取地址 \u0026amp;a 根据地址取值 *b  make \u0026amp; new 都用来申请内存\nmake: slice，map， chan申请内存，返回类型本身 new: 基本数据类型申请内存, 返回指针, 且内存对应的值为类型零值 a := new(int) map 散列表(hash)实现, 无序的基于key-value的数据结构,引用类型，必须初始化才能使用.\nmap[KeyType]ValueType make(map[keytype]valuetype, [cap]) code := make(map[string]string, 10) code[\u0026quot;a\u0026quot;] = \u0026quot;a\u0026quot; code[\u0026quot;b\u0026quot;] = \u0026quot;b\u0026quot; # 判断 值是否存在 value, ok := map[key] if ok { // 存在 } else { // 不存在 } // 循环 for k, v := range map { // k,v v可省略 } 函数 func 函数名(参数)(返回值){ } 示例\nfunc sum1(x, y int) int { sum := x + y return sum } // 返回值定义了返回值名，return可省却 func sum2(x, y int) (sum int) { sum = x + y return } // 可变参数，可变参数可有可无 func sum3(x int, y ...int) { fmt.Println(x) fmt.Println(y) // 切片 } func main() { r1 := sum1(1, 2) r2 := sum2(1, 2) fmt.Println(r1, r2) sum3(1) // 1 [] sum3(1, 2) 1 [2] sum3(1, 2, 3) 1 [2,3] } defer 延迟处理, 多个defer 按照先进后出处理。\n函数return分成两步\n 返回值赋值 defer return  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63  package main import \u0026#34;fmt\u0026#34; func deferdemo(name string) { fmt.Println(name) } func f1() int { x := 5 defer func() { x++ // 修改x不是 返回值 \t}() // 返回值赋值 5 \t// 修改x值为 6 \t// retrun 5 \treturn x } func f2() (x int) { defer func() { x++ }() // 返回值赋值 x=5 \t// 修改x值为 x=6 \t// return 6  return 5 // 返回值 } func f3() (y int) { x := 5 defer func() { x++ }() // y = x = 5 \t// x = 6 \t// return y = 6  return x } func f4() (x int) { defer func(x int) { x++ }(x) // x = 5 \t// 副本 x = 6 \t// return x \treturn 5 } func main() { deferdemo(\u0026#34;666\u0026#34;) defer deferdemo(\u0026#34;777\u0026#34;) deferdemo(\u0026#34;888\u0026#34;) defer deferdemo(\u0026#34;999\u0026#34;) fmt.Println(f1(), f2(), f3(), f4()) // 5,6,5,5  }   匿名函数 1 2 3 4 5 6 7 8 9 10 11 12 13  // 多次执行 \te1 := func(x, y int) { fmt.Println(x) fmt.Println(y) fmt.Println(x + y) } e1(2, 4) // 立即执行，只执行一次 \tfunc(x, y int) { fmt.Println(x * y) }(1, 2)   闭包  函数作为返回值 外部变量引用  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  package main import \u0026#34;strings\u0026#34; import \u0026#34;fmt\u0026#34; // checkMail func checkMail(domain string) func(string) string { return func(name string) string { if strings.HasSuffix(name, domain) { return name } return name + \u0026#34;@\u0026#34; + domain } } func main() { talkFunc := checkMail(\u0026#34;@ysicing.me\u0026#34;) fmt.Println(talkFunc(\u0026#34;i\u0026#34;)) fmt.Println(talkFunc(\u0026#34;root@ysicing.me\u0026#34;)) }   panic/recover  recover()和defer一起使用 defer在panic语句前  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  package main import \u0026#34;fmt\u0026#34; func funcA() { fmt.Println(\u0026#34;a\u0026#34;) } func funcB() { defer func() { err := recover() fmt.Println(err) }() panic(\u0026#34;error\u0026#34;) fmt.Println(\u0026#34;b\u0026#34;) } func funC() { fmt.Println(\u0026#34;c\u0026#34;) } func main() { funcA() funcB() funC() }   占位符 \tm1 := make(map[string]int, 5) m1[\u0026quot;demo\u0026quot;] = 1 fmt.Printf(\u0026quot;%v\\n\u0026quot;, m1) // 值的默认格式 fmt.Printf(\u0026quot;%+v\\n\u0026quot;, m1) // 类似%v, 结构体会加字段名 fmt.Printf(\u0026quot;%#v\\n\u0026quot;, m1)\t// map[string]int{\u0026quot;demo\u0026quot;:1} 值Go语法 fmt.Printf(\u0026quot;%T\\n\u0026quot;, m1) // map[string]int 打印类型 结构体 ","description":"Go基础","id":29,"section":"posts","tags":["go"],"title":"Go基础","uri":"https://ysicing.me/posts/go-study-ch1/"},{"content":"部署docker registry  最近大陆push镜像老是超时，于是想自建一个，使用helm方式, 和drone安装方式类似\n 部署 helm pull stable/docker-registry tar xf docker-registry-1.9.1.tgz \u0026amp;\u0026amp; cd docker-registry/ # 编辑values.yaml,自定义配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  # 注释部分为调整replicaCount:1updateStrategy:podAnnotations:{}podLabels:{}image:repository:registrytag:2.7.1pullPolicy:IfNotPresentservice:name:registrytype:ClusterIPport:5000annotations:{}ingress:enabled:true# 启用ingresspath:/hosts:- hub.local.godu.dev# 域名annotations:nginx.ingress.kubernetes.io/proxy-body-size:\u0026#34;0\u0026#34;# k8s ingress 413nginx.org/client-max-body-size:\u0026#34;0\u0026#34;# nginxnc ingress 413labels:{}tls:- secretName:godu.dev# 证书hosts:- hub.local.godu.dev# 域名resources:{}persistence:accessMode:\u0026#39;ReadWriteOnce\u0026#39;enabled:truesize:5GiexistingClaim:registry# 同dronestorage:filesystemsecrets:haSharedSecret:\u0026#34;\u0026#34;htpasswd:\u0026#34;\u0026#34;configData:version:0.1log:fields:service:registrystorage:cache:blobdescriptor:inmemoryhttp:addr::5000headers:X-Content-Type-Options:[nosniff]health:storagedriver:enabled:trueinterval:10sthreshold:3securityContext:enabled:truerunAsUser:1000fsGroup:1000priorityClassName:\u0026#34;\u0026#34;podDisruptionBudget:{}nodeSelector:{}tolerations:[]extraVolumeMounts:[]extraVolumes:[]  deploy\nhelm install registry -f values.yaml stable/docker-registry ","description":"helm部署docker registry","id":30,"section":"posts","tags":[""],"title":"helm部署docker registry","uri":"https://ysicing.me/posts/helm-docker-registry/"},{"content":"使用Helm安装Drone  一个比较热门的轻量级CI/CD开源工具：Drone\n 简介  Drone是用Go开发的开源轻量级CI/CD工具 使用简单的 YAML 配置文件来定义和执行 Docker 容器中定义的 Pipeline 构成简单,服务占用资源少  Server端负责身份认证，仓库配置，用户、Secrets 以及 Webhook 相关的配置 Agent端用于接受构建的作业和真正用于运行的 Pipeline 工作流   安装简单，支持主流Git托管服务(github,gitea等) 官方文档也很全  环境  helm v3.0.2 drone 1.6.1 k8s 1.17.0  准备工作 注册Github OAuth应用\n获取到github oauth Client ID,Client Secret留用。\n创建持久化存储 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # https://sh.ysicing.me/k8s/demo/pvc.yamlapiVersion:v1kind:PersistentVolumemetadata:name:dronepvspec:accessModes:- ReadWriteOncecapacity:storage:2GipersistentVolumeReclaimPolicy:DeletestorageClassName:nfs-testnfs:server:192.168.100.101path:/k8sdata/default-drone-pvc---kind:PersistentVolumeClaimapiVersion:v1metadata:name:dronepvcnamespace:defaultspec:accessModes:- ReadWriteOnceresources:requests:storage:2GistorageClassName:nfs-test  示例\n# nfs 地址需要按需调整一下 kubectl apply -f https://sh.ysicing.me/k8s/demo/pvc.yaml kubectl get pvc 安装  本文使用helm方式安装，因为需要对配置做些调整，需要自定义一些配置\n 1 2 3 4 5  helm repo update # 获取最新的离线drone包 helm pull stable/drone tar xf drone-2.4.0.tgz \u0026amp;\u0026amp; cd drone # 编辑values.yaml   示例values.yaml如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102  # 有注释的地方说明有修改images:server:repository:\u0026#34;docker.io/drone/drone\u0026#34;tag:1.6.1pullPolicy:IfNotPresentagent:repository:\u0026#34;docker.io/drone/agent\u0026#34;tag:1.6.1pullPolicy:IfNotPresentdind:repository:\u0026#34;docker.io/library/docker\u0026#34;tag:18.06.1-ce-dindpullPolicy:IfNotPresentservice:httpPort:80type:ClusterIPexposeGRPC:falseingress:enabled:true# 启用hosts:- drone.godu.dev# 域名tls:- secretName:godu.dev# 证书，集群内已经证书的secret了，可以不启用，因为我的dev域名必须https访问hosts:- drone.godu.dev# 域名sourceControl:provider:github# githubsecret:github:clientID:xxx# github oauth id xxxclientSecretKey:clientSecretclientSecretValue:xxxx# github oauth secret xxxxserver:https://github.comgitlab:clientID:clientSecretKey:clientSecretclientSecretValue:server:gitea:clientID:clientSecretKey:clientSecretclientSecretValue:server:gogs:server:bitbucketCloud:clientID:clientSecretKey:clientSecretclientSecretValue:bitbucketServer:server:consumerKey:consumerKeyprivateKey:privateKeyusername:passwordKey:passwordserver:host:\u0026#34;drone.godu.dev\u0026#34;protocol:httpsrpcProtocol:httpadminUser:ysicing# github 登录后就具有管理员权限alwaysAuth:falsekubernetes:enabled:true# 运行 Drone 的任务的时候就是直接使用 Kubernetes 的 Job 资源对象来执行，而不是 Drone 的 agent.env:DRONE_LOGS_DEBUG:\u0026#34;false\u0026#34;DRONE_DATABASE_DRIVER:\u0026#34;sqlite3\u0026#34;DRONE_DATABASE_DATASOURCE:\u0026#34;/var/lib/drone/drone.sqlite\u0026#34;annotations:{}resources:{}affinity:{}nodeSelector:{}tolerations:[]extraContainers:|extraVolumes:|agent:env:DRONE_LOGS_DEBUG:\u0026#34;false\u0026#34;replicas:1annotations:{}resources:{}livenessProbe:{}readinessProbe:{}affinity:{}nodeSelector:{}tolerations:[]dind:enabled:truedriver:overlay2resources:{}metrics:prometheus:enabled:truepersistence:enabled:trueexistingClaim:dronepvc# 刚刚创建的持久化 pvcrbac:create:trueapiVersion:v1serviceAccount:create:truename:  部署drone\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ helm install drone -f values.yaml stable/drone $ helm status drone NAME: drone LAST DEPLOYED: Sun Jan 5 20:30:00 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ********************************************************************************* *** PLEASE BE PATIENT: drone may take a few minutes to install *** ********************************************************************************* From outside the cluster, the server URL(s) are: http://drone.godu.dev   最后\nroot@k8s.cn1:~/drone/drone# kubectl get pods -l app=drone NAME READY STATUS RESTARTS AGE drone-drone-server-5bffbc56df-qzk28 1/1 Running 0 35m root@k8s.cn1:~/drone/drone# kubectl get ing -l app=drone NAME HOSTS ADDRESS PORTS AGE drone-drone drone.godu.dev 80, 443 36m 触发CI构建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # .drone.ymlkind:pipelinename:defaultsteps:- name:buildimage:golang:latestenvironment:GOPROXY:https://goproxy.cncommands:- CGO_ENABLED=0 go build - name:dockerimage:plugins/dockersettings:repo:ysicing/godemouse_cache:trueusername:from_secret:dockeruserpassword:from_secret:dockertags:- latestwhen:event:pushbranch:master  代码地址: BeidouCloudPlatform/demo\nCli工具 brew install drone-cli export DRONE_SERVER=https://drone.godu.dev export DRONE_TOKEN=\u0026lt;token\u0026gt; drone info # 修复webhook，如果调整了域名，可以通过此命令修复webhook drone repo repair BeidouCloudPlatform/demo Job清理问题 因为使用Job的方式进行pipline操作，如果不启用TTLAfterFinished会导致job不会被自动清理。 开启feature请参考 feature开启\n默认drone清理是300s\n","description":"helm部署drone","id":31,"section":"posts","tags":["drone"],"title":"helm部署drone","uri":"https://ysicing.me/posts/helm-drone/"},{"content":"Ucloud 网络 ucloud默认mtu是 1454\n所以calico需要改mtu\nkubectl patch configmap/calico-config -n kube-system --type merge -p '{\u0026quot;data\u0026quot;:{\u0026quot;veth_mtu\u0026quot;: \u0026quot;1404\u0026quot;}}' 参考 Configure MTU to maximize network performance\n","description":"k8s实践之calico mtu","id":32,"section":"posts","tags":["calico"],"title":"k8s实践之calico mtu","uri":"https://ysicing.me/posts/k8s-calico-mtu/"},{"content":"概念篇  Kubernetes 基本概念和使用方法\n 架构 etcd保存了整个集群的状态； apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制； controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上； kubelet负责维护容器的生命周期，同时也负责Volume（CSI）和网络（CNI）的管理； Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）； kube-proxy负责为Service提供cluster内部的服务发现和负载均衡； CoreDns 集群dns解析服务 Ingress Controller 应用层负载均衡提供对外访问服务 Kube-state-metrics 集群监控 Prometheus 资源监控 Dashboard Web UI Master(Control Plane)节点 Master是集群的控制平面:\n 负载集群的全局决策(如调度等) 探测响应集群事件(如探测应用的实例数是否符合预期) 通常master节点不调度业务服务  Master节点主要包括kube-apiserver、kube-scheduler、kube-controller-manager和etcd四个组件。\n1 2 3 4 5  kube-apiserver: API服务, Kubernetes控制平面的前端，所有请求入口 kube-scheduler: watch API资源状态，根据相关条件调度到合适的node节点创建相关资源 kube-controller-manager: 生命周期管理和API业务逻辑(如: 节点控制器,副本控制器) etcd: k8s数据存储组件 cloud-controller-manager: 云服务商组件，对接各家云资源(云服务商维护)   Worker(Node)节点 包括kubelet、kube-proxy和Container Runtime三个组件。\nkubelet: 运行在集群每个节点的客户端，需要确保相关容器运行在pod中； 通过PodSpecs标签，描述容器的运行状态； kubelet只管理通过kubernetes创建的容器。 kube-proxy： 是一个运行在集群每个节点的网络代理组件,主要是维护网络规则，保证集群内外与Pod通信。 Container Runtime： 支持运行容器底层环境的软件； 支持： CRI(Container Runtime Interface) 如Docker/Containerd。 核心Addons  Addons 使用 Kubernetes 资源（DaemonSet、Deployment等）实现集群的功能特性。由于他们提供集群级别的功能特性，addons使用到的Kubernetes资源都放置在 kube-system名称空间下。\n  CNI(Calico等) DNS(CoreDNS等) UI(Dashboard) \u0026hellip;  官方Addons\n","description":"k8s实践之基础概念","id":33,"section":"posts","tags":["kubernetes"],"title":"k8s实践之基础概念","uri":"https://ysicing.me/posts/k8s-intro-base/"},{"content":"安装linkerd v2 安装二进制 docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir 获取相关镜像 1 2 3  linkerd install --ignore-cluster | grep image: | sed -e \u0026#39;s/^ *//\u0026#39; | sort | uniq | awk \u0026#39;{print $2}\u0026#39; | xargs -I {} docker pull {} linkerd install --ignore-cluster | grep image: | sed -e \u0026#39;s/^ *//\u0026#39; | sort | uniq | awk \u0026#39;{print $2}\u0026#39; | grep \u0026#34;gcr.io/linkerd-io\u0026#34; | awk -F/ \u0026#39;{print $NF}\u0026#39; | xargs -I {} docker tag gcr.io/linkerd-io/{} ysicing/linkerd-io-{} linkerd install --ignore-cluster | grep image: | sed -e \u0026#39;s/^ *//\u0026#39; | sort | uniq | awk \u0026#39;{print $2}\u0026#39; | grep \u0026#34;gcr.io/linkerd-io\u0026#34; | awk -F/ \u0026#39;{print $NF}\u0026#39; | xargs -I {} docker push ysicing/linkerd-io-{}   同步到内网环境 1 2 3  linkerd install --ignore-cluster | grep image: | sed -e \u0026#39;s/^ *//\u0026#39; | sort | uniq | awk \u0026#39;{print $2}\u0026#39; | grep \u0026#34;gcr.io/linkerd-io\u0026#34; | awk -F/ \u0026#39;{print $NF}\u0026#39; | xargs -I {} docker pull ysicing/linkerd-io-{} linkerd install --ignore-cluster | grep image: | sed -e \u0026#39;s/^ *//\u0026#39; | sort | uniq | awk \u0026#39;{print $2}\u0026#39; | grep \u0026#34;gcr.io/linkerd-io\u0026#34; | awk -F/ \u0026#39;{print $NF}\u0026#39; | xargs -I {} docker tag ysicing/linkerd-io-{} hub.godu.dev/linkerd/{} linkerd install --ignore-cluster | grep image: | sed -e \u0026#39;s/^ *//\u0026#39; | sort | uniq | awk \u0026#39;{print $2}\u0026#39; | grep \u0026#34;gcr.io/linkerd-io\u0026#34; | awk -F/ \u0026#39;{print $NF}\u0026#39; | xargs -I {} docker push hub.godu.dev/linkerd/{}   安装 1 2 3  linkerd check --pre linkerd install --registry hub.godu.dev/linkerd | kubectl apply -f - linkerd check   启用 # 已有应用 kubectl get -n emojivoto deploy/web -o yaml | linkerd inject - | kubectl apply -f - # go 创建应用 func (app *TalkController) CustomAnnotations() map[string]string { return map[string]string{ \u0026quot;linkerd.io/inject\u0026quot;: \u0026quot;enabled\u0026quot;, //\u0026quot;prometheus.io/port\u0026quot;: \u0026quot;12306\u0026quot;, //\u0026quot;prometheus.io/scrape\u0026quot;: \u0026quot;true\u0026quot;, } } # deploy template: metadata: annotations: linkerd.io/inject: enabled prometheus.io/port: \u0026quot;80\u0026quot; prometheus.io/scrape: \u0026quot;true\u0026quot; ","description":"k8s实践之安装linkerdv2","id":34,"section":"posts","tags":["linkerd"],"title":"k8s实践之安装linkerdv2","uri":"https://ysicing.me/posts/k8s-linkerd-install/"},{"content":"k8s 资源管理的权限控制 在k8s中，由系统自身的接口来创建和管理的账号类型只有一种，叫做ServiceAccount。\n可以使用下面的命令来查看目前系统已有的ServiceAccount，简写sa\n➜ ~ kubectl get sa NAME SECRETS AGE default 1 2d8h 查看sa具体定义\n➜ ~ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026quot;2020-01-05T06:27:59Z\u0026quot; name: default namespace: default resourceVersion: \u0026quot;428\u0026quot; selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 9881e9a3-4097-44b4-a9e3-e5a9d4e516e0 secrets: - name: default-token-8xcpl # 引用了名为default-token-8xcpl的secrets ➜ ~ kubectl get secrets default-token-8xcpl -o yaml apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5akNDQWJLZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQ0FYRFRJd01ERXdOVEEyTWpjeE5Gb1lEekl4TVRneE1qRXlNRFl5TnpFMFdqQVZNUk13RVFZRApWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBCnoyS0ttbzh2emdXWE1WR09hSy9DeWk2cGZ2T0psWUMxd1paRTdWS3IwTENFZDBQWXVKbk10NStQdVhKUkw4OFUKK2t2U0RGRm1RS3J6QUhCN0IwNGVybEwyd0NHeDZwa2FIOFBMMTJqKzdUUW5VS2doa1lHb1ZxUjNKV3NTSS9jcwo0ampGWTJyVmY3Z3pDNG9LbHlBc2VRdXFRaERPV004emtCalFYa3gyZVdnSSthRFNpOHd5SHNSNXZwK0Y4TkNOCnEreHY3bFczOERBcTB2SGlSYnBlZHVCTWpUTksyaHlYYWpyeFpWNTZxTTdnRUJWcVBjRCtUWmQySGs2SGQ2dlgKaGhTeTdUQ3lOY1kvbi9HNjRscTBUaG9JZTBWaFBncG8rU3JzRXVOVTBqWlFmRllDMDRWZlluSU0yZmxuSkY5YQpVWjkraVEza25ZR3RmSTA4cXRScjh3SURBUUFCb3lNd0lUQU9CZ05WSFE4QkFmOEVCQU1DQXFRd0R3WURWUjBUCkFRSC9CQVV3QXdFQi96QU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFOWHdQUkRLd05yMEFzMVpiN1NWWnVQeSsKMVhtbWhWS3dVUmVROXF2QjQwWU5sTVBKMVVqbUNyVmdVRmM4WldmUEhwTmZ6Z2dXTUJUbFFrZDhOK1NhTHB3bAo0bTJtMlpmTTV6Q3R2QVg2MzhHZUVPYTViVkNHcXhudUNVOWQxb2p0M2JZSkYwZGxSMy9VY3FpaDRaeEdUL0syCkJweEZ5QXBRRUZ0elhVZmE0dTBYcFBwUU1aa2txK2hkd01mZjQ3VlBma3NUazUvN2ZaK08vUXk4SElSM00wWFEKUWZyRWpIRXRwL2VSU1hENm95ZHF4R2RKL0pOWU4rKzJYU05lRm51bDFOakh2bG1IT1JpRDI2S2Rqb24vT1Y5cgplYlp0T1oxRmJPVVVFNktJRG9CWTJIK0JCWVppNkwzajJBRkpFTFozWE5tNUdlSTB0NGduWW8zSXU0SnRmZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K namespace: ZGVmYXVsdA== token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkltUnBTa2RCWlZwcVMwOVdOVmxCVDJkemQySTBkRlpPVVVkRlVHOVZYMXB0VDA1Q1dXeG5UVGhhTVdjaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbVJsWm1GMWJIUXRkRzlyWlc0dE9IaGpjR3dpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdWbVlYVnNkQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJams0T0RGbE9XRXpMVFF3T1RjdE5EUmlOQzFoT1dVekxXVTFZVGxrTkdVMU1UWmxNQ0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT21SbFptRjFiSFFpZlEub1NwZ0w4V3liYlJEb0tvYUpxclJ1RkxGOUN6SW1tdlo3VEtWU1RXdVpnWVpCWTRRZ092NjVCbGFLTF9qQU9DSFNyQzd6WW5pSjlXVDVCcVd2N3dfQU9XdEhURVJWSU00Y3I5LXh6LUxMRHZ6bl9qZlNIR0VoSnBPZHZnMGppRFZydi0xanJOb2g1S3VKMGw0TlNULVBnemtUQTU5bWhfNzdodFRtdzJfaDJCWHNDWDBROFg1dm5uMTBMVFJaeHRtNnZTank5dVRIcUZpa0pkU2pTX2c1SjJiS3BXVW1sZnl6OWNKTkliNWhDS0hYVEYyWGlDaG9vMlI3RmdvcHc5X1ltNWxLc2JudGh0bHA5TDUzZ2M2UV9IUkRvb05hSk04RHZndTNybXJheUhPa193RkFTbE1XZ2NkZzF0OGNjYXIwM1V0aWZ5RElDS2s1OTBMM2d2T0hR kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 9881e9a3-4097-44b4-a9e3-e5a9d4e516e0 creationTimestamp: \u0026quot;2020-01-05T06:27:59Z\u0026quot; name: default-token-8xcpl namespace: default resourceVersion: \u0026quot;421\u0026quot; selfLink: /api/v1/namespaces/default/secrets/default-token-8xcpl uid: 2f07379e-80bb-4c24-a3e3-8a37d1c4c9fb type: kubernetes.io/service-account-token 创建sa ➜ ~ kubectl create sa ysicing serviceaccount/ysicing created ➜ ~ kubectl get sa ysicing -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026quot;2020-01-07T14:40:07Z\u0026quot; name: ysicing namespace: default resourceVersion: \u0026quot;134248\u0026quot; selfLink: /api/v1/namespaces/default/serviceaccounts/ysicing uid: e5f19802-a196-47d6-b194-3684642357ef secrets: - name: ysicing-token-zvgr4 # 获取token,这里token是原始token，未经过base64加密 kubectl describe secrets ysicing-token-zvgr4 测试请求k8s api,发现还是403.\n➜ ~ curl -k https://192.168.100.101:6443/healthz/ping -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ' { \u0026quot;kind\u0026quot;: \u0026quot;Status\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;metadata\u0026quot;: { }, \u0026quot;status\u0026quot;: \u0026quot;Failure\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;forbidden: User \\\u0026quot;system:serviceaccount:default:ysicing\\\u0026quot; cannot get path \\\u0026quot;/healthz/ping\\\u0026quot;\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;Forbidden\u0026quot;, \u0026quot;details\u0026quot;: { }, \u0026quot;code\u0026quot;: 403 } k8s对资源操作权限划分的比较详细:\n 写权限  create update delete patch   读权限  get list watch    配置sa ysicing访问资源,即配置 .kube/config\n# 配置sa ysicing token鉴权信息 kubectl config set-credentials ysicing --token eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ User \u0026quot;ysicing\u0026quot; set. # 配置集群访问信息 # .kube/ca.crt 从192.168.100.101:/etc/kubernetes/pki/ca.crt获取的 ➜ ~ kubectl config set-cluster local --server https://192.168.100.101:6443 --certificate-authority .kube/ca.crt --embed-certs=true Cluster \u0026quot;local\u0026quot; set. # 配置context, 将cluster \u0026amp; user 绑定 ➜ ~ kubectl config set-context local-ctx --cluster local --user ysicing Context \u0026quot;local-ctx\u0026quot; created. # 切换ctx ➜ ~ kubectl config use-context local-ctx Switched to context \u0026quot;local-ctx\u0026quot;. 请求接口\n➜ ~ kubectl get pods Error from server (Forbidden): pods is forbidden: User \u0026quot;system:serviceaccount:default:ysicing\u0026quot; cannot list resource \u0026quot;pods\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;default\u0026quot; 授权配置  创建角色，定义资源操作权限(ClusterRole/Role) 角色绑定，将角色与Sa绑定(ClusterRoleBinding/RoleBinding)  # 创建角色 ➜ ~ kubectl create role ysicing-role --resource pod,service,deployment,secret,ingress --verb create,update,delete,patch,get,list,watch role.rbac.authorization.k8s.io/ysicing-role created ➜ ~ kubectl get roles NAME AGE ysicing-role 34s # 角色绑定 ➜ ~ kubectl create rolebinding ysbot.ysicing-binding --role ysicing-role --serviceaccount default:ysicing rolebinding.rbac.authorization.k8s.io/ysbot.ysicing-binding created 再次获取pods信息\n➜ ~ kubectl config use-context local-ctx Switched to context \u0026quot;local-ctx\u0026quot;. ➜ ~ kubectl get pods NAME READY STATUS RESTARTS AGE demo-deployment-6c5664f4d6-s6w8v 1/1 Running 0 2d4h 再次请求https://192.168.100.101:6443/healthz/ping发现依旧403，那么给ysicing sa通过ClusterRoleBinding 授予一个名为 cluster-admin 的 ClusterRole\n➜ ~ kubectl config use-context kubernetes-admin@kubernetes Switched to context \u0026quot;kubernetes-admin@kubernetes\u0026quot;. ➜ ~ kubectl create clusterrolebinding cluster-ysicing --clusterrole cluster-admin --serviceaccount default:ysicing clusterrolebinding.rbac.authorization.k8s.io/cluster-ysicing created ➜ ~ curl -k https://192.168.100.101:6443/healthz/ping -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ' ok% # 真香哈哈哈哈 cluster-admin权限特别大,实际使用还需要谨慎操作。\n# 可以查看具体定义 kubectl get role ysicing-role ","description":"k8s实践之资源管理的权限控制","id":35,"section":"posts","tags":["kubernetes"],"title":"k8s实践之资源管理的权限控制","uri":"https://ysicing.me/posts/k8s-sa/"},{"content":"部署第一个应用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # https://sh.ysicing.me/k8s/demo/deploy.yamlapiVersion:apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本kind:Deployment #该配置的类型，我们使用的是 Deploymentmetadata:#译名为元数据，即 Deployment 的一些基本属性和信息namespace:default# 命名空间name:demo-deployment #Deployment 的名称labels:#标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解app:demo #为该Deployment设置key为app，value为demo的标签spec:#这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用replicas:1#使用该Deployment创建一个应用程序实例#strategy: #滚动策略 最多新增一个，最小下线一个# rollingUpdate:# maxSurge: 1# maxUnavailable: 1# type: RollingUpdateselector:#标签选择器，与上面的标签共同作用，目前不需要理解matchLabels:#选择包含标签app:demo的资源app:demotemplate:#这是选择或创建的Pod的模板metadata:#Pod的元数据labels:#Pod的标签，上面的selector即选择包含标签app:demo的Podapp:demospec:#期望Pod实现的功能（即在pod中部署）containers:#生成container，与docker中的container是同一种- name:godemo #container的名称image:ysicing/godemo #使用镜像godemo创建container，该container默认80端口可访问  部署应用\nkubectl apply -f https://sh.ysicing.me/k8s/demo/deploy.yaml deployment.apps/demo-deployment created 查看部署结果\n# 默认ns就是default可省却 kubectl get deployments -n default NAME READY UP-TO-DATE AVAILABLE AGE demo-deployment 1/1 1 1 2m54s kubectl get pods NAME READY STATUS RESTARTS AGE demo-deployment-59cd96d4d5-cjjwr 1/1 Running 0 3m17s Deployment是pod控制器\n常见pod控制器\n守护型: 无状态非系统级应用: Deployment (如nginx) 无状态系统级应用: DaemonSet (如日志监控收集端，每个node节点仅且需要跑一个pod) 有状态应用: StatefulSet (数据库类应用如mysql等) 非守护型: 一次性任务: Job 定时任务: CronJob 常用命令 # 获取资料 kubectl explain 类型(如pods) # 获取资源信息 kubectl get 资源类型 kubectl get pods 获取default租户的所有pods资源列表 kubectl get nodes 获取节点资源列表 kubectl get deploy 获取default租户类型为deployment的资源列表 # 显示资源的详细信息 kubectl describe 资源类型 资源名称 kubectl describe deploy demo-deployment 获取default租户deployment类型且名为demo-deployment的详细信息 # 看pod日志，类似docker logs kubectl logs Pod名称 kubectl logs demo-deployment-59cd96d4d5-cjjwr 查看default租户下pod名为demo-deployment-59cd96d4d5-cjjwr的日志 # 进入容器，类型docker exec kubectl exec -it Pod名称 操作命令 kubectl exec -it demo-deployment-59cd96d4d5-cjjwr ash / # 访问部署的应用 那么，部署完第一个应用又该如何访问?\nKubernetes 中的 Service（服务） 提供了这样的一个抽象层，它选择具备某些特征的 Pod（容器组）并为它们定义一个访问方式。\n在创建Service的时候，通过设置配置文件中的 spec.type 字段的值，可以以不同方式向外部暴露应用程序，这里介绍常用的3种：\nClusterIP(默认): 集群内部可访问 NodePort: NAT方式,可以通过访问集群中任意节点+端口号的方式访问服务 \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;,此时ClusterIP的访问方式仍然可用。 LoadBalancer: 负载均衡(依赖云访问)。此时 ClusterIP 和 NodePort 的访问方式仍然可用。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  kubectl explain svc# https://sh.ysicing.me/k8s/demo/svc.yamlapiVersion:v1kind:Servicemetadata:name:demo-service\t#Service 的名称labels:#Service 自己的标签app:demo\t#为该 Service 设置 key 为 app，value 为 demo 的标签spec:\t#这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问selector:\t#标签选择器app:demo\t#选择包含标签 app:demo 的 Podports:- name:demo-port\t#端口的名字protocol:TCP\t#协议类型 TCP/UDPport:80#集群内的其他容器组可通过 80 端口访问 ServicenodePort:32600#通过任意节点的 32600 端口访问 ServicetargetPort:80#将请求转发到匹配 Pod 的 80 端口type:NodePort\t#Serive的类型，ClusterIP/NodePort/LoaderBalancer  生效\nkubectl apply -f https://sh.ysicing.me/k8s/demo/svc.yaml service/demo-service created 查看service，通过之前的文档\nkubectl get svc -l app=demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-service NodePort 10.96.37.87 \u0026lt;none\u0026gt; 80:32600/TCP 55s --- kubectl describe svc/demo-service Name: demo-service Namespace: default Labels: app=demo Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Service\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;app\u0026quot;:\u0026quot;demo\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;demo-service\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spe... Selector: app=demo Type: NodePort IP: 10.96.37.87 Port: demo-port 80/TCP TargetPort: 80/TCP NodePort: demo-port 32600/TCP Endpoints: 172.16.219.5:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; 测试服务访问\n# 在集群节点 root@k8s1:~# curl 10.96.37.87 {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-cjjwr\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.219.5/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} root@k8s1:~# curl 172.16.219.5 {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-cjjwr\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.219.5/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} root@k8s1:~# curl 192.168.100.101:32600 {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-cjjwr\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.219.5/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} root@k8s1:~# curl 192.168.100.102:32600 {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-cjjwr\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.219.5/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} 那么问题来了，如果想通过clusterip方式提供对外服务，该怎么做？\n# https://sh.ysicing.me/k8s/demo/ing.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: labels: app: demo name: demo-ingress # ingress名 namespace: default spec: rules: - host: godemo.slb.k7s.xyz # 域名 http: paths: - backend: serviceName: demo-service # godemo的 service名 servicePort: demo-port # godemo的service定义的port path: / #路径 生效\nkubectl apply -f https://sh.ysicing.me/k8s/demo/ing.yaml ingress.networking.k8s.io/demo-ingress created kubectl get ing NAME HOSTS ADDRESS PORTS AGE demo-ingress godemo.slb.k7s.xyz 80 91s 验证ingress\ncurl godemo.slb.k7s.xyz {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-cjjwr\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.219.5/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} 这时候流量增加，怎么快速伸缩应用。\n伸缩应用 伸缩 的实现可以通过更改 deploy.yaml 文件中部署的 replicas（副本数）来完成\n# replicas: 1 ---\u0026gt; replicas: 4 # 改完生效 kubectl apply -f https://sh.ysicing.me/k8s/demo/deploy2.yaml deployment.apps/demo-deployment configured # 查看pod kubectl get pods -l app=demo NAME READY STATUS RESTARTS AGE demo-deployment-59cd96d4d5-78v28 1/1 Running 0 16s demo-deployment-59cd96d4d5-cjjwr 1/1 Running 0 15m demo-deployment-59cd96d4d5-mn7r8 1/1 Running 0 16s demo-deployment-59cd96d4d5-mvxk2 1/1 Running 0 16s # 修改了 Deployment 的 replicas 为 4 后，Kubernetes 又为该 Deployment 创建了 3 新的 Pod，这 4 个 Pod 有相同的标签。因此nginx Service 通过标签选择器与新的 Pod建立了对应关系，将访问流量通过负载均衡在 4 个 Pod 之间进行转发 Name: demo-service Namespace: default Labels: app=demo Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Service\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;app\u0026quot;:\u0026quot;demo\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;demo-service\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spe... Selector: app=demo Type: NodePort IP: 10.96.37.87 Port: demo-port 80/TCP TargetPort: 80/TCP NodePort: demo-port 32600/TCP Endpoints: 172.16.109.71:80,172.16.109.72:80,172.16.219.5:80 + 1 more... Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; 验证效果,流量是负载到后端不同pod上\nroot@k8s1:~# curl godemo.slb.k7s.xyz {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-mn7r8\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.109.71/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} root@k8s1:~# curl godemo.slb.k7s.xyz {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-cjjwr\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.219.5/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} root@k8s1:~# curl godemo.slb.k7s.xyz {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-59cd96d4d5-78v28\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.219.6/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} 最后 1 2 3 4 5  陈述式：kubectl create -f xx.yaml申明式（建议使用）：kubectl apply -f xx.yaml   pod容器如果未发生调度，重启容器ip是不会改变的  另外除了Service这种网络，还有hostPort,hostNetwork\nhostPort：直接将容器的端口与所调度的节点上的端口路由，这样可以通过宿主机的IP加上来访问Pod了, Ingress就是这样的 hostNetwork：共享宿主机的网络名称空间 这里可以这么测试使用hostPort\nkubectl apply -f https://sh.ysicing.me/k8s/demo/deploy3.yaml kubectl get pods -l app=demo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deployment-6c5664f4d6-s6w8v 1/1 Running 0 112s 172.16.109.68 k8s2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; curl 666.slb.k7s.xyz:28080 {\u0026quot;hostname\u0026quot;:\u0026quot;demo-deployment-6c5664f4d6-s6w8v\u0026quot;,\u0026quot;ip\u0026quot;:{\u0026quot;eth0\u0026quot;:\u0026quot;172.16.109.68/32\u0026quot;,\u0026quot;lo\u0026quot;:\u0026quot;127.0.0.1/8\u0026quot;}} ","description":"k8s实践之部署第一个应用","id":36,"section":"posts","tags":["kubernetes"],"title":"k8s实践之部署第一个应用","uri":"https://ysicing.me/posts/k8s-intro-deploy/"},{"content":" 扩展了Kubernetes的功能\n 部署kuboard kubectl apply -f https://sh.ysicing.me/k7s/install/kuboard/deploy.yml # 因为ingress部署关系，所以配置解析域名为ui.slb.k7s.xyz # 管理节点执行，获取token kbtoken 部署metrics-server kubectl apply -f https://sh.ysicing.me/k7s/install/metrics-server/deploy.yaml 部署Dashboard sealos install --pkg-url https://github.com/sealstore/dashboard/releases/download/v2.0.0-bata5/dashboard.tar kdtoken ","description":"k8s常用扩展组件","id":37,"section":"posts","tags":["kubernetes"],"title":"k8s常用扩展组件","uri":"https://ysicing.me/posts/k8s-addons/"},{"content":"格式要求：\n有效标签值必须为 63 个字符或更少，并且必须为空或以字母数字字符（[a-z0-9A-Z]）开头和结尾，中间可以包含破折号（-）、下划线（_）、点（.）和字母或数字 节点调度nodeSelector 1 2 3 4  # 打label kubectl label nodes \u0026lt;node-name\u0026gt; \u0026lt;label-key\u0026gt;=\u0026lt;label-value\u0026gt; # 显示label kubectl get node --show-labels   pod调度到节点\n1 2 3 4 5 6 7 8 9 10 11 12 13  apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresentnodeSelector:disktype:ssd  ","description":"k8s标签和选择器","id":38,"section":"posts","tags":["kubernetes"],"title":"k8s标签和选择器","uri":"https://ysicing.me/posts/k8s-labels/"},{"content":" 配置默认存储\n # 集群已有存储类型（StorageClass），执行 kubectl get sc看下当前是否设置了默认的 storageclass kubectl get sc kubectl patch storageclass nfs-data -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;:\u0026quot;true\u0026quot;}}}' ","description":"k8s配置默认存储","id":39,"section":"posts","tags":["存储"],"title":"k8s配置默认存储","uri":"https://ysicing.me/posts/k8s-default-sc/"},{"content":"kubectl plugin插件机制初体验之krew 插件机制 这个早就已经GA了，其作用允许开发者以独立的二进制或脚本形式发布自定义的kubectl子命令, 灵活快速操作k8s\n插件不限制语言，只需将脚本或二进制可执行文件以kubectl-的前缀放到PATH中即可.\n官方示例 官方Go示例辅助库 https://github.com/kubernetes/cli-runtime.git\n不太简单的官方示例插件 https://github.com/kubernetes/sample-cli-plugin\n示例插件 mkdir /usr/local/k8s export PATH=$PATH:/usr/local/k8s 示例插件如下：\nroot@k8s1:~# cat /usr/local/k8s/kubectl-demo #!/bin/bash echo $1 赋予执行权限\nchmod +x /usr/local/k8s/kubectl-demo 运行测试:\nroot@k8s1:~# kubectl demo 666 666 root@k8s1:~# kubectl plugin list The following compatible plugins are available: /usr/local/k8s/kubectl-demo kubectl plugin list 可以查看PATH中查看有哪些插件\nkrew Package manager for kubectl plugins \u0026ndash; krew\n安装krew # Bash and ZSH ( set -x; cd \u0026quot;$(mktemp -d)\u0026quot; \u0026amp;\u0026amp; curl -fsSLO \u0026quot;https://github.com/kubernetes-sigs/krew/releases/download/v0.3.4/krew.{tar.gz,yaml}\u0026quot; \u0026amp;\u0026amp; tar zxvf krew.tar.gz \u0026amp;\u0026amp; KREW=./krew-linux_amd64 \u0026amp;\u0026amp; \u0026quot;$KREW\u0026quot; install --manifest=krew.yaml --archive=krew.tar.gz \u0026amp;\u0026amp; \u0026quot;$KREW\u0026quot; update ) # .bashrc/.zshrc export PATH=$PATH:/usr/local/k8s:${KREW_ROOT:-$HOME/.krew}/bin 查看当前可用的kubectl plugin，发现多了一个kubect-krew\nroot@k8s1:~# kubectl plugin list The following compatible plugins are available: /usr/local/k8s/kubectl-demo /root/.krew/bin/kubectl-krew krew使用 kubectl krew krew is the kubectl plugin manager. You can invoke krew through kubectl: \u0026quot;kubectl krew [command]...\u0026quot; Usage: krew [command] Available Commands: help Help about any command info Show information about a kubectl plugin install Install kubectl plugins list List installed kubectl plugins search Discover kubectl plugins uninstall Uninstall plugins update Update the local copy of the plugin index upgrade Upgrade installed plugins to newer versions version Show krew version and diagnostics Flags: -h, --help help for krew -v, --v Level number for the log level verbosity Use \u0026quot;krew [command] --help\u0026quot; for more information about a command. 初体验 root@k8s1:~# kubectl get pods No resources found in default namespace. root@k8s1:~# kubectl krew install change-ns Installing plugin: change-ns Installed plugin: change-ns root@k8s1:~# kubectl krew list PLUGIN VERSION change-ns v1.0.0 krew v0.3.3 root@k8s1:~# kubectl change-ns nginx-ingress namespace changed to \u0026quot;nginx-ingress\u0026quot; root@k8s1:~# kubectl get pod root@k8s1:~# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ingress-rfswh 1/1 Running 0 2m6s nginx-ingress-v6c4l 1/1 Running 0 2m6s ","description":"kubectl plugin插件机制初体验之krew","id":40,"section":"posts","tags":["krew"],"title":"kubectl plugin插件机制初体验之krew","uri":"https://ysicing.me/posts/k8s-kubectl-plugins/"},{"content":"minikube安装k8s MacOS安装 1  brew install minikube   1 2  # PROXY可以根据自己需要设置，可不设置 minikube start --cpus=2 --memory 4g --disk-size=40g --driver=virtualbox --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --nodes=2 --docker-env HTTP_PROXY=http://192.168.99.1:7890 --docker-env HTTPS_PROXY=http://192.168.99.1:7890 --docker-env NO_PROXY=127.0.0.1/32,192.168.0.0/16,10.0.0.0/8,172.16.0.0/12,localhost   ","description":"minikube安装k8s","id":41,"section":"posts","tags":["kubernetes","安装"],"title":"minikube安装k8s","uri":"https://ysicing.me/posts/k8s-minikube-install/"},{"content":"NFS存储 Debian # 安装 apt update apt install -y nfs-kernel-server # 配置 mkdir /k8sdata echo \u0026quot;/k8sdata/ *(insecure,rw,sync,no_root_squash,no_subtree_check)\u0026quot; \u0026gt; /etc/exports # 启动nfs systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r # 测试 showmount -e 127.0.0.1 CentOS # 安装nfs yum install -y nfs-utils # 配置共享目录 mkdir /k8sdata echo \u0026quot;/k8sdata *(insecure,rw,sync,no_root_squash)\u0026quot; \u0026gt; /etc/exports # 启动nfs systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r # 测试 showmount -e 127.0.0.1 一键部署nfs且配置默认存储 # 未安装存储 curl https://sh.ysicing.me/k7s/install/nfs/deploy.sh | bash # 已有存储 wget https://sh.ysicing.me/k7s/install/nfs/deploy_exist_nfs.sh ./deploy_exist_nfs.sh \u0026lt;ip\u0026gt; \u0026lt;path\u0026gt; ","description":"NFS存储","id":42,"section":"posts","tags":["kubernetes","安装"],"title":"NFS存储","uri":"https://ysicing.me/posts/nfs-install-k8s/"},{"content":" nginx ingress 配置域名证书(默认以ingress-nginx为例)\n 创建证书 默认已经签发证书\n创建 secret 创建好证书以后，需要将证书内容放到 secret 中，secret 中全部内容需要 base64 编码\n1 2 3 4 5 6 7 8 9 10  # ingress-secret.ymlapiVersion:v1kind:Secretmetadata:name:ingress-secret-promnamespace:monitoringtype:kubernetes.io/tlsdata:tls.crt:\u0026lt;base64 encoded cert\u0026gt;tls.key:\u0026lt;base64 encoded key\u0026gt;  完成创建\n1 2 3 4  ~# kubectl apply -f ingress-secret.yml -n monitoring secret/ingress-secret created ~# kubectl apply -f ingress-secret.yml -n kube-system secret/ingress-secret created   或者通过如下方式\n1  kubectl create secret tls ingress-secret --key tls.key.pem --cert tls.pem   配置ingress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:annotations:kubernetes.io/ingress.class:nginxname:promnamespace:monitoringspec:tls:- hosts:- prom.k7s.xyz- grafana.k7s.xyz- alter.k7s.xyzsecretName:ingress-secretrules:- host:prom.k7s.xyzhttp:paths:- backend:serviceName:prometheus-k8sservicePort:9090- host:grafana.k7s.xyzhttp:paths:- backend:serviceName:grafanaservicePort:3000- host:alter.k7s.xyzhttp:paths:- backend:serviceName:alertmanager-mainservicePort:9093  ","description":"nginx ingress 配置域名证书","id":43,"section":"posts","tags":["ingress","kubernetes"],"title":"nginx ingress 配置域名证书","uri":"https://ysicing.me/posts/k8s-ingress-tls-config/"},{"content":"OpenLDAP安装初体验  简介这里不在描述，google即可\n docke快速部署 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  version:\u0026#39;2\u0026#39;services:openldap:image:osixia/openldap:1.3.0container_name:openldapenvironment:LDAP_LOG_LEVEL:\u0026#34;256\u0026#34;LDAP_ORGANISATION:\u0026#34;Godu Inc.\u0026#34;LDAP_DOMAIN:\u0026#34;ysicing.me\u0026#34;LDAP_BASE_DN:\u0026#34;\u0026#34;LDAP_ADMIN_PASSWORD:\u0026#34;meadmin\u0026#34;LDAP_CONFIG_PASSWORD:\u0026#34;meconfig\u0026#34;LDAP_READONLY_USER:\u0026#34;true\u0026#34;LDAP_READONLY_USER_USERNAME:\u0026#34;readonly\u0026#34;LDAP_READONLY_USER_PASSWORD:\u0026#34;readonly\u0026#34;LDAP_RFC2307BIS_SCHEMA:\u0026#34;false\u0026#34;LDAP_BACKEND:\u0026#34;mdb\u0026#34;LDAP_TLS:\u0026#34;false\u0026#34;LDAP_REPLICATION:\u0026#34;false\u0026#34;KEEP_EXISTING_CONFIG:\u0026#34;false\u0026#34;LDAP_REMOVE_CONFIG_AFTER_SETUP:\u0026#34;true\u0026#34;LDAP_SSL_HELPER_PREFIX:\u0026#34;ldap\u0026#34;tty:truestdin_open:trueports:- \u0026#34;389:389\u0026#34;- \u0026#34;636:636\u0026#34;domainname:\u0026#34;ldap.ysicing.me\u0026#34;# important: same as hostname# hostname: \u0026#34;ldap.ysicing.me\u0026#34;command:[\u0026#34;--copy-service\u0026#34;,\u0026#34;--loglevel\u0026#34;,\u0026#34;debug\u0026#34;]phpldapadmin:image:osixia/phpldapadmin:latestcontainer_name:phpldapadminenvironment:PHPLDAPADMIN_LDAP_HOSTS:\u0026#34;openldap\u0026#34;PHPLDAPADMIN_HTTPS:\u0026#34;false\u0026#34;ports:- \u0026#34;8080:80\u0026#34;depends_on:- openldap  验证 din openldap ldapsearch -x -H ldap://localhost -b dc=ysicing,dc=me -D 'cn=admin,dc=ysicing,dc=me' -w meadmin 或者访问127.0.0.1:8080\n","description":"OpenLDAP安装初体验","id":44,"section":"posts","tags":["ldap"],"title":"OpenLDAP安装初体验","uri":"https://ysicing.me/posts/openldap-install/"},{"content":"定时任务，如每65分钟执行一次 crontab 定时任务 65 分钟执行一次，怎么写？\n这个时候，用系统自带的crontab就不好实现了,这时候就是systemd该上场了\n参考Systemd 定时器教程\ncat /etc/systemd/system/example.timer [Unit] Description=example timer [Timer] OnUnitActiveSec=1h Unit=example.service [Install] WantedBy=multi-user.target cat /etc/systemd/system/example.service [Unit] Description=example [Service] ExecStart=\u0026lt;相关命令\u0026gt; [Install] WantedBy=multi-user.target 定制定时器\nOnActiveSec：定时器生效后，多少时间开始执行任务 OnBootSec：系统启动后，多少时间开始执行任务 OnStartupSec：Systemd 进程启动后，多少时间开始执行任务 OnUnitActiveSec：该单元上次执行后，等多少时间再次执行 OnUnitInactiveSec： 定时器上次关闭后多少时间，再次执行 OnCalendar：基于绝对时间，而不是相对时间执行 AccuracySec：如果因为各种原因，任务必须推迟执行，推迟的最大秒数，默认是60秒 Unit：真正要执行的任务，默认是同名的带有.service后缀的单元 Persistent：如果设置了该字段，即使定时器到时没有启动，也会自动执行相应的单元 WakeSystem：如果系统休眠，是否自动唤醒系统 Timer和Service大体用法一致\n","description":"Systemd 定时器教程","id":45,"section":"posts","tags":["冷知识"],"title":"Systemd 定时器教程","uri":"https://ysicing.me/posts/systemd-timers/"},{"content":"安装vim plug curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 写配置 向~/.vimrc写入\n\u0026quot; autocmd BufWritePost $MYVIMRC source $MYVIMRC \u0026quot; 关闭兼容模式 set nocompatible \u0026quot; 设置行号 set nu \u0026quot;突出显示当前行,列 set cursorline set cursorcolumn \u0026quot;显示括号匹配 set showmatch \u0026quot; tab 缩进 set tabstop=4 \u0026quot; 设置Tab长度为4空格 set shiftwidth=4 \u0026quot; 设置自动缩进长度为4空格 set autoindent \u0026quot; 继承前一行的缩进方式，适用于多行注释 \u0026quot; 定义快捷键的前缀，即\u0026lt;Leader\u0026gt; let mapleader=\u0026quot;;\u0026quot; \u0026quot; ==== 系统剪切板复制粘贴 ==== \u0026quot; v 模式下复制内容到系统剪切板 vmap \u0026lt;Leader\u0026gt;c \u0026quot;+yy \u0026quot; n 模式下复制一行到系统剪切板 nmap \u0026lt;Leader\u0026gt;c \u0026quot;+yy \u0026quot; n 模式下粘贴系统剪切板的内容 nmap \u0026lt;Leader\u0026gt;v \u0026quot;+p \u0026quot; 开启实时搜索 set incsearch \u0026quot; 搜索时大小写不敏感 set ignorecase syntax enable syntax on \u0026quot; 开启文件类型侦测 filetype plugin indent on \u0026quot; 启用自动补全 \u0026quot; 退出插入模式指定类型的文件自动保存 au InsertLeave *.go,*.sh,*.php write \u0026quot; 插件 call plug#begin('~/.vim/plugged') \u0026quot; 可以快速对齐的插件 Plug 'junegunn/vim-easy-align' \u0026quot; 用来提供一个导航目录的侧边栏 Plug 'scrooloose/nerdtree' \u0026quot; 可以在导航目录中看到 git 版本信息 Plug 'Xuyuanp/nerdtree-git-plugin' \u0026quot; 自动补全括号的插件，包括小括号，中括号，以及花括号 Plug 'jiangmiao/auto-pairs' \u0026quot; 可以在 vim 中使用 tab 补全 Plug 'vim-scripts/SuperTab' Plug 'majutsushi/tagbar' \u0026quot; go 主要插件 Plug 'fatih/vim-go', { 'tag': '*' } \u0026quot; go 中的代码追踪，输入 gd 就可以自动跳转 Plug 'dgryski/vim-godef' call plug#end() 安装插件 vim ~/.vimrc # 安装插件 :PlugInstall # 更新插件 :PlugUpdate ","description":"vim配置","id":46,"section":"posts","tags":["vim"],"title":"vim配置","uri":"https://ysicing.me/posts/vim-config/"},{"content":"vscode配置 安装好Go相关插件，配置工具包 使用command+shift+P快捷键,然后键入Go: Install/Update Tools选中全部(根据需要),稍等片刻，就会安装完成。\n自定义配置项 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  { \u0026#34;git.autofetch\u0026#34;: true, \u0026#34;files.autoSave\u0026#34;: \u0026#34;onWindowChange\u0026#34;, \u0026#34;workbench.colorTheme\u0026#34;: \u0026#34;Go - Sources\u0026#34;, \u0026#34;terminal.integrated.shell.osx\u0026#34;: \u0026#34;/bin/zsh\u0026#34;, \u0026#34;go.autocompleteUnimportedPackages\u0026#34;: true, \u0026#34;go.gocodeAutoBuild\u0026#34;: true, \u0026#34;go.useLanguageServer\u0026#34;: true, \u0026#34;goOutliner.enableDebugChannel\u0026#34;: true, \u0026#34;goOutliner.extendExplorerTab\u0026#34;: true, \u0026#34;go.inferGopath\u0026#34;: true, \u0026#34;go.docsTool\u0026#34;: \u0026#34;godoc\u0026#34;, \u0026#34;go.gocodePackageLookupMode\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;go.gotoSymbol.includeImports\u0026#34;: true, \u0026#34;go.useCodeSnippetsOnFunctionSuggest\u0026#34;: true, //使用代码片段提示 \u0026#34;go.useCodeSnippetsOnFunctionSuggestWithoutType\u0026#34;: true, \u0026#34;go.formatTool\u0026#34;: \u0026#34;goimports\u0026#34;, //代码格式化 // \u0026#34;go.buildOnSave\u0026#34;: true, //保存代码时自动编译 \u0026#34;go.lintOnSave\u0026#34;: \u0026#34;file\u0026#34;, //保存代码时优化 \u0026#34;go.vetOnSave\u0026#34;: \u0026#34;package\u0026#34;, //保存代码时检查潜在错误 \u0026#34;go.coverOnSave\u0026#34;: false //保存代码时执行测试 }   ","description":"vscode配置","id":47,"section":"posts","tags":["vscode","go"],"title":"vscode配置","uri":"https://ysicing.me/posts/vscode-config/"},{"content":"使用nvm进行node版本管理 安装nvm 1 2 3 4 5 6 7 8 9 10 11 12 13  # 安装 curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.2/install.sh | bash # 默认会写.zshrc ### .zshrc nvm start  export NVM_DIR=\u0026#34;$HOME/.nvm\u0026#34; [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/nvm.sh\u0026#34; # This loads nvm [ -s \u0026#34;$NVM_DIR/bash_completion\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/bash_completion\u0026#34; # This loads nvm bash_completion ### .zshrc nvm end source .zshrc nvm install node # \u0026#34;node\u0026#34; is an alias for the latest version   ","description":"使用nvm进行node版本管理","id":48,"section":"posts","tags":["nodejs"],"title":"使用nvm进行node版本管理","uri":"https://ysicing.me/posts/nodejs-install/"},{"content":" 内网穿透之wireguard\n 安装 Debian # Debian Bullseye 之前版本都需要启用backports源来支持安装 apt install wireguard -y macOS brew install wireguard-tools 配置服务端 cd /etc/wireguard # 创建服务端密钥对 umask 077 wg genkey | tee privatekey | wg pubkey \u0026gt; publickey # 创建wg0.conf cat \u0026gt; /etc/wireguard/wg0.conf \u0026lt;\u0026lt;EOF [Interface] PrivateKey = \u0026lt;Private Key\u0026gt; Address = 10.0.0.1/24, fd86:ea04:1115::1/64 ListenPort = 51820 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o eth0 -j MASQUERADE SaveConfig = true EOF # 启动 wg-quick up wg0 # 开机启动 systemctl enable wg-quick@wg0 # 检查 wg show 配置客户端 umask 077 wg genkey | tee privatekey | wg pubkey \u0026gt; publickey cat \u0026gt; /etc/wireguard/wg0.conf \u0026lt;\u0026lt;EOF [Interface] PrivateKey = \u0026lt;Output of privatekey file that contains your private key\u0026gt; Address = 10.0.0.2/24, fd86:ea04:1115::5/64 EOF 连接客户端和服务端 法一 客户端操作  wg-quick down wg0 直接编辑客户端配置文件 启动wg-quick up wg0;systemctl enable wg-quick@wg0  # /etc/wireguard/wg0.conf 新增 [Peer] PublicKey = \u0026lt;Server Public key\u0026gt; Endpoint = \u0026lt;Server Public IP\u0026gt;:51820 AllowedIPs = 10.0.0.2/24, fd86:ea04:1115::5/64 法二 服务端操作 wg set wg0 peer \u0026lt;Client Public Key\u0026gt; endpoint \u0026lt;Client IP address\u0026gt;:51820 allowed-ips 203.0.113.12/24,fd86:ea04:1115::5/64 问题解决 # 1. 内核模块没有 lsmod | grep wireguard \u0026amp;\u0026amp; echo yes || echo no no modprobe wireguard modprobe: FATAL: Module wireguard not found in directory /lib/modules/5.4.0-0.bpo.4-amd64 dkms status wireguard, 0.0.20200318, 4.19.0-8-amd64, x86_64: installed uname -a Linux cn3 5.4.0-0.bpo.4-amd64 #1 SMP Debian 5.4.19-1~bpo10+1 (2020-03-09) x86_64 GNU/Linux dkms autoinstall Error! Your kernel headers for kernel 5.4.0-0.bpo.4-amd64 cannot be found. Please install the linux-headers-5.4.0-0.bpo.4-amd64 package # 解决方式 apt install -y linux-headers-5.4.0-0.bpo.4-amd64 # ipv6 RTNETLINK answers: Permission denied 启用ipv6即可 net.ipv6.conf.all.disable_ipv6=0 ","description":"内网穿透之wireguard","id":49,"section":"posts","tags":["wireguard","内网穿透"],"title":"内网穿透之wireguard","uri":"https://ysicing.me/posts/wireguard-install/"},{"content":"ttlSecondsAfterFinished 自动清理完成和失败的Job，目前该特性默认不启用。如何判断未启用,查看job资源，在spec里未发现ttlSecondsAfterFinished定义则表示未启用。\n启用 我的集群使用sealos安装的，其配置文件在 /etc/kubernetes/manifests下,分别调整 kube-apiserver.yaml,kube-controller-manager.yaml,kube-scheduler.yaml配置，新增\n# 示例 spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true - --feature-gates=TTLAfterFinished=true # 新增配置 image: k8s.gcr.io/kube-scheduler:v1.17.0 调整完成后稍等片刻,相关pod会重建。\n","description":"开启集群feature","id":50,"section":"posts","tags":["kubernetes"],"title":"开启集群feature开启集群feature","uri":"https://ysicing.me/posts/k8s-feature/"},{"content":"docker npm安装问题 npm config set unsafe-perm true git操作 # 改崩了 git fetch --all git reset --hard origin/master # 放弃本地全部/单个 git checkout . git checkout -- filename ","description":"知识点","id":51,"section":"posts","tags":["冷知识"],"title":"知识点","uri":"https://ysicing.me/posts/faq/"},{"content":"macOS 常用工具  个人常用工具列表\n  iTerm2(终端) Sourcetree(git工具) Dash(文档工具) iHosts(hosts编辑器) Microsoft To Do(Todo) pap.er(壁纸) ZeroTier(内网穿透) Xmind Pixiu(记账)  ","description":"记录自用macOS常用工具","id":54,"section":"links","tags":[""],"title":"macOS常用工具","uri":"https://ysicing.me/links/macos-apps/"},{"content":" 用于记录发现的有用/有趣的Github项目\n 20191219  book prometheus  20191208  kubewatch Watch k8s events and trigger Handlers Go  20191121  overlay networking tool slackhq/nebula  20191104  KubeOperator 一键部署和管理生产级别的 Kubernetes 集群  20191010  Status Page statusfy NodeJS  20191009  Status Page statping Go  20191006  iptv  20190927  Collection of Prometheus alerting rules  20190926  listen1 听歌神器  20190925  一个基于Go的Telegram RSS Bot机器人，支持应用内阅读预览 kubernetes app kubeapp  20190924  octant k8s Go  20190920  gopub 发布系统(不维护) PrometheusAlert Go  20190919  felix SSH and RESTful scaffold for Backend and DevOps engineers Go kplcloud基于Kubernetes的应用管理平台 Go Git服务webhook Go  20190916  🕵️‍♀️ 监视我的手机：数据都去哪儿了？ Python  20190915  🍭 集合多家 API 的新一代图床auxpi Go  20190913  TeaWeb-可视化的Web代理服务 Go  20190912  SmartPing 一款开源、高效、便捷的网络质量监控神器！Go  20190911  Mysql web端sql审核平台Yearning Go 简单可信赖的任务管理工具 Go  20190908  Linux透明代理 运维管理平台flask Python  20190906  kubernetes高可用安装工具sealos GO 基于Vue框架构建的github数据可视化平台GitDataV VUE  20190904  nging 基于caddy的网站服务程序，带图形化管理界面  20190903  Aria2-AriaNg-X docker-compose  20190823  trivy 容器的简单而全面的漏洞扫描程序 Go  20190813  anytunnel 开源内网穿透商用平台系统 Go 扩展企业安全测试主动诱导型蜜罐框架系统 Go  ","description":"用于记录发现的有用/有趣的Github项目","id":55,"section":"links","tags":[""],"title":"有趣的Github项目","uri":"https://ysicing.me/links/links/"},{"content":" 上周有遇到过这个问题 awk '{print $$1}' 这个$$是什么用法呢\n 间接字段寻址,其类似\nawk '{print $$1}' ===\u0026gt; awk '{print $($1)}' ===\u0026gt; awk '{Nr=$1; print $Nr}' 示例:\n$ echo -e \u0026quot;1 b c d\\n2 b c d\\n3 b c d\\n4 b c d\u0026quot; 1 b c d 2 b c d 3 b c d 4 b c d $ echo -e \u0026quot;1 b c d\\n2 b c d\\n3 b c d\\n4 b c d\u0026quot; | awk '{print $$1}' 1 # since filed #1=1 result print first field b # since filed #1=2 result print secondfield c # since filed #1=3 result print third field d # since filed #1=4 result print fourth field 类似$可以根据需要添加更多\nawk '{print $$$1}' ===\u0026gt; awk '{print $$($1)}' ===\u0026gt; awk '{print $($($1))}' ","description":"awk 中关于多个$的用法","id":56,"section":"posts","tags":["shell"],"title":"awk 中关于多个$的用法","uri":"https://ysicing.me/posts/awk-func/"},{"content":"Mac安装 如果已经安装了brew，就可以快速安装了\n# 安装git brew install git # 安装go brew install go # add GOBIN path to your PATH in ~/.zshrc export GO111MODULE=on export GOPROXY=https://goproxy.cn export GOPATH=\u0026quot;/Users/ysicing/go\u0026quot; # 示例 export GOBIN=\u0026quot;$GOPATH/bin\u0026quot; export PATH=$HOME/bin:/usr/local/bin:/usr/local/opt/mysql-client/bin:$GOBIN:$PATH 配置Visual Studio Code Editor 快捷键cmd+shift+p 键入: go install 选择 \u0026quot;Go: Install/Update Tools\u0026quot; Check all the checkboxes Linux安装 访问 golang中国 获取最新go版本\n1 2 3 4 5 6 7 8 9 10 11 12 13  # 下载 wget https://dl.google.com/go/go1.17.linux-amd64.tar.gz # 解压 tar -C /usr/local -xzf go1.17.linux-amd64.tar.gz # 配置 .bashrc export GO111MODULE=on export GOPROXY=https://goproxy.cn export GOPATH=\u0026#34;/root/go\u0026#34; export GOBIN=\u0026#34;$GOPATH/bin\u0026#34; export PATH=$PATH:$GOBIN:/usr/local/go/bin source .bashrc # 验证 go env   一键脚本 1 2  # 20210819 废弃 curl https://sh.ysicing.me/install/go.sh | bash   ","description":"一键安装go1.17","id":57,"section":"posts","tags":["go"],"title":"一键安装go1.17","uri":"https://ysicing.me/posts/go-install/"},{"content":"阿里云轻量应用服务器升级内核  升级有风险请慎重哦\n 配置说明  阿里云HK 1核1G1TB30Mbps ¥24/m\nDebian 9.9\n 更新源  需要移除默认源，使用如下源\n # 需要添加buster-backports源 sed -i \u0026quot;s#stretch#buster#g\u0026quot; /etc/apt/sources.list # 示例 deb http://mirrors.a li yun c/debian/ buster main contrib non-free deb http://mirrors.aliyun.com/debian/ buster-updates main contrib non-free deb http://mirrors.aliyun.com/debian/ buster-proposed-updates main non-free contrib deb http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib deb http://mirrors.aliyun.com/debian-security/ buster/updates main non-free contrib 更新升级\n1 2  apt-get update apt-get dist-upgrade -y   升级到最新内核 apt-get install -t buster-backports linux-image-amd64 -y update-grub apt autoclean apt autoremove -y reboot 升级完成后docker无法启动 prior storage driver aufs failed: driver not supported 删除/var/lib/docker，docker存储由aufs变成overlay2\n","description":"阿里云轻量应用服务器升级内核","id":58,"section":"posts","tags":["debian",""],"title":"阿里云轻量应用服务器升级内核","uri":"https://ysicing.me/posts/aliyun-debian-upgrade-kernel/"},{"content":" AdGuard Home使用Golang开发，因此安装非常简单，这里以容器的方式部署为例，其它方式可参考官方帮助文档。\n 主要功能  拦截AD 号称隐私保护 家庭保护模式 自定义过滤(劫持) (😂我用的最多的是这个)  部署 二话不说直接上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  # docker-compose.yamlversion:\u0026#39;2.1\u0026#39;services:caddy:image:spanda/caddycontainer_name:caddyvolumes:- ./ssl:/root/.caddy- /var/log/caddy:/var/log/caddy- ./Caddyfile:/etc/Caddyfilenetwork_mode:hostrestart:alwaysdns:image:adguard/adguardhomecontainer_name:dnsvolumes:- ./ad/conf:/opt/adguardhome/conf- ./ad/work:/opt/adguardhome/worknetwork_mode:hostrestart:always# Caddyfiledns.ysicing.net {gziptls root@ysicing.netlog / /var/log/caddy/dns.ysicing.net.log \u0026#34;{remote} {when} {method} {uri} {proto} {status} {size} {\u0026gt;User-Agent} {latency}\u0026#34; {rotate_size 50rotate_age 90rotate_keep 20rotate_compress}header / {Strict-Transport-Security \u0026#34;max-age=31536000;includeSubDomains;preload\u0026#34;Access-Control-Allow-Origin *Access-Control-Allow-Methods \u0026#34;GET, POST, OPTIONS\u0026#34;-Server}proxy / 127.0.0.1:7070 {transparentwebsocket}}  访问公网IP:3000,按着无脑一顿猛操作。修改默认web监听端口为127.0.0.1:7070,53端口默认监听全部。\n使用 Windows  打开网络和Internet设置 打开网络和共享中心 打开以太网 打开属性 编辑TCP/IPV4 使用下面的DNS服务器 首 59.110.220.53 备 8.8.8.8  Mac 跳过很简单\n安卓 自己折腾吧，私有dns搞不定，官方APP可以\n","description":"AdGuard使用姿势","id":59,"section":"posts","tags":["dns"],"title":"AdGuard使用姿势","uri":"https://ysicing.me/posts/dns-adguard-install/"},{"content":" 不会具体讲怎么部署，原理都类似,将原先amd64换成mips64el\n 中标麒麟龙芯CPU源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # /etc/yum.repos.d/ns7-mips.repo [ns7-mips64el-os] name=NeoKylin Linux Advanced Server 7 - $basearch - Os baseurl=http://download.cs2c.com.cn/neokylin/server/releases/7.0/ls_64/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-neokylin-release enabled=1 [ns7-mips64el-extras] name=NeoKylin Linux Advanced Server 7 - Addons baseurl=http://download.cs2c.com.cn/neokylin/server/everything/7.0/ls_64/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-neokylin-release enabled=0 [ns7-mips64el-updates] name=NeoKylin Linux Advanced Server 7 - Updates baseurl=http://download.cs2c.com.cn/neokylin/server/updates/7.0/ls_64/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-neokylin-release enabled=1   还有一个就是龙芯自己的仓库比较迷，但是软件包比较新\n1  http://ftp.loongnix.org/os/loongnix/1.0/   基础镜像 neokylin基础镜像 1 2 3 4 5 6 7  wget https://raw.githubusercontent.com/docker/docker/master/contrib/mkimage-yum.sh chmod +x ./mkimage-yum.sh ./mkimage-yum.sh -y /etc/yum.conf nk-base wget http://list.isoftos.win/script/create_docker_image.loogson chmod +x create_docker_image.loogson ./create_docker_image.loogson nk-base   debian基础镜像 可以通过异构构建镜像方式\n可以参考大佬项目 debian-debootstrap\nk8s编译 宿主机编译  4核16G,性能太差,源码编译安装go新版本差不多两小时\n 通过异构镜像编译 大概修改印象:\n1 2 3 4 5 6 7 8 9 10 11 12 13  # 第一处 mips64*) host_arch=mips64le ;; # 第二处 \u0026#34;linux/mips64le\u0026#34;) export CGO_ENABLED=1 export CC=mips64el-linux-gnu-gcc ;; # 第三处 linux/mips64le # 第四处 uint64转换一下   具体可以通过make \u0026lt;xxx组件名\u0026gt;\npause镜像 这个得注意一下, 不能用空镜像，可以使用基础镜像\n","description":"龙芯Mips64el平台上部署K8s","id":60,"section":"posts","tags":[""],"title":"龙芯Mips64el平台上部署K8s","uri":"https://ysicing.me/posts/mips64el-loongson-k8s/"},{"content":"Debian 11  赶紧升级Debian 11, 好处多多\n 升级debian10(buster)内核版本 1 2 3 4 5 6 7 8 9  sed -i \u0026#39;s/buster\\/updates/bullseye-security/g;s/buster/bullseye/g\u0026#39; /etc/apt/sources.list apt update apt dist-upgrade -y # apt install -t bullseye-backports linux-image-amd64 -y # update-grub # reboot # 内核 Linux bj01 5.10.0-0.bpo.8-amd64 #1 SMP Debian 5.10.46-2~bpo10+1 (2021-07-22) x86_64 GNU/Linux   Debian 10存档 创建Debian虚拟机 1 2 3 4 5 6 7 8 9 10  Usage: ergo vm create [flags] Flags: -h, --help help for create --path string Vagrantfile所在目录, $HOME/vm --vmcpus string 虚拟机CPU数 (default \u0026#34;2\u0026#34;) --vmmem string 虚拟机Mem MB数 (default \u0026#34;4096\u0026#34;) --vmname string 虚拟机名 --vmnum string 虚拟机副本数 (default \u0026#34;1\u0026#34;)   默认使用ergo创建虚拟机\n1  ergo vm create --vmcpus 4 --vmmem 8192 --vmname debian   或者自行使用Vagrantfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box_check_update = false config.vm.provider \u0026#39;virtualbox\u0026#39; do |vb| vb.customize [ \u0026#34;guestproperty\u0026#34;, \u0026#34;set\u0026#34;, :id, \u0026#34;/VirtualBox/GuestAdd/VBoxService/--timesync-set-threshold\u0026#34;, 1000 ] end $num_instances = 1 (1..$num_instances).each do |i| config.vm.define \u0026#34;debian#{i}\u0026#34; do |node| node.vm.box = \u0026#34;ysicing/debian\u0026#34; node.vm.hostname = \u0026#34;debian#{i}\u0026#34; node.vm.network \u0026#34;public_network\u0026#34;, use_dhcp_assigned_default_route: true, bridge: \u0026#39;en0: Wi-Fi (Wireless)\u0026#39; # node.vm.provision \u0026#34;shell\u0026#34;, run: \u0026#34;always\u0026#34;, inline: \u0026#34;ntpdate ntp.api.bz\u0026#34; node.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;11.11.11.11#{i}\u0026#34; node.vm.provision \u0026#34;shell\u0026#34;, run: \u0026#34;always\u0026#34;, inline: \u0026#34;echo hello from debian#{i}\u0026#34; node.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.gui = false vb.memory = 8192 vb.cpus = 4 vb.name = \u0026#34;debian#{i}\u0026#34; vb.customize [\u0026#34;modifyvm\u0026#34;, :id, \u0026#34;--natdnshostresolver1\u0026#34;, \u0026#34;on\u0026#34;] vb.customize [\u0026#34;modifyvm\u0026#34;, :id, \u0026#34;--ioapic\u0026#34;, \u0026#34;on\u0026#34;] # cpu 使用率50% vb.customize [\u0026#34;modifyvm\u0026#34;, :id, \u0026#34;--cpuexecutioncap\u0026#34;, \u0026#34;50\u0026#34;] end end end end   初始化Debian ergo vm init --ip 11.11.11.111 --docker # 默认会使用ysicing/ansible镜像，执行ansible脚本初始化debian，--docker参数默认表示安装docker 手动执行之宿主机初始化 git clone https://github.com/ysicing/play-ansible.git cd play-ansible # 安装ansible,如果已安装可跳过 ./install.sh # 配置初始化机器 cp inventory.ini.example inventory.ini # 执行初始化 ansible-playbook init.yml 手动执行之容器化方式初始化 1 2 3 4 5 6  docker pull ysicing/ansible docker run -it --rm ysicing/ansible bash cp inventory.ini.example inventory.ini # 初始化系统 ansible-playbook init.yml exit   Debian 10升级内核  添加buster-backports源，如果你使用ysicin/debian镜像可跳过\n # 默认 cat \u0026gt;/etc/apt/sources.list \u0026lt;\u0026lt;EOF deb http://mirrors.aliyun.com/debian buster-backports main contrib non-free deb http://mirrors.aliyun.com/debian-security buster/updates main contrib non-free EOF # Ucloud cat \u0026gt;/etc/apt/sources.list \u0026lt;\u0026lt;EOF deb http://mirrors.ucloud.cn/debian/ buster-backports main contrib non-free deb http://mirrors.ucloud.cn/debian-security/ buster/updates main contrib non-free EOF 更新升级\n1 2 3 4 5 6 7  apt update apt dist-upgrade -y apt-get install -t buster-backports linux-image-amd64 -y update-grub apt autoclean apt autoremove -y reboot   ","description":"Debian常用操作指南","id":61,"section":"posts","tags":["debian"],"title":"Debian个人常用操作指南","uri":"https://ysicing.me/posts/debian-op/"},{"content":"开始之前 在阅读本书之前希望您掌握以下知识和准备以下环境：\n Debian 常用命令 Docker 基本操作 Mac/Linux 皆可  系列主题  Kubernetes(1.18.19)实践 Go(1.17)开发实践 运维实践(主要是基于Debian(bullseye)发行版)  ？ 欢迎交流, 一起努力.\n如果有任何疑问或错误，欢迎在 issues 进行提问或给予修正意见\n如果喜欢或对你有所帮助，欢迎 Star，对我是一种鼓励和推进 😀\n","description":"你好,我的博客","id":62,"section":"posts","tags":["站点"],"title":"README","uri":"https://ysicing.me/posts/blog-notice/"}]