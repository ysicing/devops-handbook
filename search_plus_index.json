{"./":{"url":"./","title":"序言","keywords":"","body":"DevOps运维指南/云原生实践手册 开始之前 在阅读本书之前希望您掌握以下知识和准备以下环境： Debian 常用命令 Docker 基本操作 Mac/Linux 皆可 系列主题 Kubernetes(1.17+)实践 Go(1.14+)开发实践 运维实践(主要是基于Debian(buster)发行版) ? 欢迎交流, 一起努力. 如果有任何疑问或错误，欢迎在 issues 进行提问或给予修正意见 如果喜欢或对你有所帮助，欢迎 Star，对我是一种鼓励和推进 &#x1F600; Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"install/docker.html":{"url":"install/docker.html","title":"docker","keywords":"","body":" docker快速安装 curl -fsSL https://ysicing.me/hack/docker/install.sh | bash 做了如下设置 { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\",\"https://reg-mirror.qiniu.com\"], \"bip\": \"172.30.42.1/16\", \"max-concurrent-downloads\": 10, \"log-driver\": \"json-file\", \"log-level\": \"warn\", \"log-opts\": { \"max-size\": \"20m\", \"max-file\": \"2\" }, \"storage-driver\": \"overlay2\" } docker常用工具安装 docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir 内置了 etcdctl helm(v3) helminit // 初始化helm, 默认配置的是mirror.azure.cn docker-compose calicoctl ctop cclear (清理退出容器) din (进入容器 din cid> ash/bash(默认)/sh) dps (容器状态) etcdcli (特别支持k8s集群的etcd) kbtoken (查看kuboard admin用户token) kdtoken (查看dashboard-admin 用户token) upgrade-tools // 升级二进制 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/init.html":{"url":"install/init.html","title":"debian初始化","keywords":"","body":"初始化Debian 基础环境配置和安装docker配置 all.yaml: 系统+docker docker.yaml: 安装docker init.yaml: 系统 宿主机初始化 git clone https://github.com/ysicing/play-ansible.git cd play-ansible # 安装ansible,如果已安装可跳过 ./install.sh # 配置初始化机器 cp inventory.ini.example inventory.ini # 执行初始化 ansible-playbook init.yml 容器化方式初始化 docker pull ysicing/ansible docker run -it --rm ysicing/ansible bash cp inventory.ini.example inventory.ini # 初始化系统 ansible-playbook init.yml exit Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/sealos.html":{"url":"install/sealos.html","title":"k8s之sealos","keywords":"","body":"sealos安装k8s 推荐使用sealos, 一键高可用，简单上手, 安装k8s 1.17.4版本 准备工作 虚拟机 环境: 3台机器(debian/buster, 192.168.100.101~192.168.100.103, 2核4G80G存储) mkdir k8s && cd k8s wget https://ysicing.me/hack/vm/Vagrantfile vagrant up 初始化环境 docker pull ysicing/ansible docker run -it --rm ysicing/ansible bash # 如果是其他机器需要自行修改inventory.ini cp inventory.ini.example inventory.ini # 初始化系统,安装docker ansible-playbook all.yml exit 安装k8s 源码编译安装sealos,使用新特性 说明，我定制了sealos和calico版本为最新版本 git clone https://github.com/ysicing/sealos.git --depth 1 ./build-in-docker.sh # 同步sealos到相关节点，如果你用上述步骤初始化系统，默认已经内置了sealos&#x1F602; mv sealos /usr/local/bin/ # 下载k7s文件. 需要自行下载k8s二进制文件 docker run --rm -v /root:/sysdir ysicing/k7s cp /kube.tgz /sysdir # 安装1master2worker sealos init --passwd vagrant --podcidr 172.16.0.0/16 --repo registry.cn-hangzhou.aliyuncs.com/google_containers --master 192.168.100.101 --node 192.168.100.102 --node 192.168.100.103 --version 1.17.4 --pkg-url /root/kube.tgz # 清除 sealos clean # 成功示例 kubectl get node NAME STATUS ROLES AGE VERSION cn1.vps.godu.dev Ready 22h v1.17.4 cn2.vps.godu.dev Ready 22h v1.17.4 cn3.vps.godu.dev Ready master 22h v1.17.4 内网网段与calico冲突,故调整calico和vagrant虚拟机网段 桥接网络: 192.168.199.0/24 hostonly: 192.168.100.0/24 podcidr: 172.16.0.0/16 svccidr: 10.96.0.0/12 允许管理节点调度pod kubectl taint nodes --all node-role.kubernetes.io/master- Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/minikube.html":{"url":"install/minikube.html","title":"k8s之minikube","keywords":"","body":"minikube安装k8s # PROXY可以根据自己需要设置，可不设置 minikube start --memory 4096 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --docker-env HTTP_PROXY=http://192.168.99.1:7890 --docker-env HTTPS_PROXY=http://192.168.99.1:7890 --docker-env NO_PROXY=127.0.0.1/32,192.168.0.0/16,10.0.0.0/8,172.16.0.0/12,localhost Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/helm.html":{"url":"install/helm.html","title":"helm","keywords":"","body":"helm 安装 安装docker时默认已经安装了helm,如果不是最新版本请upgrade-tools docker pull ysicing/tools docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir # 版本 helm version version.BuildInfo{Version:\"v3.1.2\", GitCommit:\"d878d4d45863e42fd5cff6743294a11d28a9abce\", GitTreeState:\"clean\", GoVersion:\"go1.13.8\"} 配置国内helm镜像库 # 自动 helminit # 手动 helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/nfs.html":{"url":"install/nfs.html","title":"存储之NFS","keywords":"","body":"NFS存储 Debian # 安装 apt update apt install -y nfs-kernel-server # 配置 mkdir /k8sdata echo \"/k8sdata/ *(insecure,rw,sync,no_root_squash,no_subtree_check)\" > /etc/exports # 启动nfs systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r # 测试 showmount -e 127.0.0.1 CentOS # 安装nfs yum install -y nfs-utils # 配置共享目录 mkdir /k8sdata echo \"/k8sdata *(insecure,rw,sync,no_root_squash)\" > /etc/exports # 启动nfs systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r # 测试 showmount -e 127.0.0.1 一键部署nfs且配置默认存储 # 未安装存储 curl https://ysicing.me/hack/k7s/install/nfs/deploy.sh | bash # 已有存储 wget https://ysicing.me/hack/k7s/install/nfs/deploy_exist_nfs.sh ./deploy_exist_nfs.sh Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/ingress.html":{"url":"install/ingress.html","title":"Ingress Controller部署","keywords":"","body":" ingress安装 域名相关 默认情况下, 本地环境域名测试 *.k7s.xyz 192.168.100.101 *.slb.k7s.xyz 192.168.100.102 *.vlb.k7s.xyz192.168.100.103 NGINX Ingress Controller (ingress-nginx) 使用helm方式安装 kubectl apply -f https://ysicing.me/hack/helm/nginx-ingress/ns.yaml # 安装 helm install nginx-ingress -f https://ysicing.me/hack/helm/nginx-ingress/nginx-ingress-1.34.2.yaml stable/nginx-ingress -n ingress-nginx # 升级 helm upgrade nginx-ingress -f https://ysicing.me/hack/helm/nginx-ingress/nginx-ingress-1.34.2.yaml stable/nginx-ingress -n ingress-nginx NGINX Ingress Controllers (kubernetes-ingress) helm repo add nginx-stable https://helm.nginx.com/stable helm repo update helm install nginx-ingress -f https://ysicing.me/hack/helm/hack/helm/nginxnc-ingress/nginx-ingress-0.4.3.yaml nginx-stable/nginx-ingress -n ingress-nginx # 或者 kubectl apply -f https://ysicing.me/hack/k7s/install/nginx-ingress/nginx-ingress.yml 部署文档 nginxinc/kubernetes-ingress v1.6.0 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/prometheus.html":{"url":"install/prometheus.html","title":"Promethues部署","keywords":"","body":" 通过Operator创建Prometheus 为什么需要prometheus-operator 因为是prometheus主动去拉取的,所以在k8s里pod因为调度的原因导致pod的ip会发生变化,人工不可能去维持,自动发现有基于DNS的,但是新增还是有点麻烦。Prometheus-operator的本职就是一组用户自定义的CRD资源以及Controller的实现，Prometheus Operator这个controller有RBAC权限下去负责监听这些自定义资源的变化，并且根据这些资源的定义自动化的完成如Prometheus Server自身以及配置的自动化管理工作。在Kubernetes中我们使用Deployment、DamenSet，StatefulSet来管理应用Workload，使用Service，Ingress来管理应用的访问方式，使用ConfigMap和Secret来管理应用配置。我们在集群中对这些资源的创建，更新，删除的动作都会被转换为事件(Event)，Kubernetes的Controller Manager负责监听这些事件并触发相应的任务来满足用户的期望。这种方式我们成为声明式，用户只需要关心应用程序的最终状态，其它的都通过Kubernetes来帮助我们完成，通过这种方式可以大大简化应用的配置管理复杂度。而除了这些原生的Resource资源以外，Kubernetes还允许用户添加自己的自定义资源(Custom Resource)。并且通过实现自定义Controller来实现对Kubernetes的扩展,不需要用户去二开k8s也能达到给k8s添加功能和对象。因为svc的负载均衡,所以在K8S里监控metrics基本最小单位都是一个svc背后的pod为target,所以prometheus-operator创建了对应的CRD: kind: ServiceMonitor ,创建的ServiceMonitor里声明需要监控选中的svc的label以及metrics的url路径的和namespaces即可 (摘自@张馆长) 部署 # 在之前的基础上，配置了存储storageclass git clone https://github.com/ysicing/prometheus.git cd prometheus kubectl apply -f . bash -x ./deploy.sh kubectl apply -f . 与官方的区别支持了数据持久化和域名配置,部分监控组件如etcd等 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/feature.html":{"url":"install/feature.html","title":"k8s启用feature特性","keywords":"","body":"开启集群feature ttlSecondsAfterFinished 自动清理完成和失败的Job，目前该特性默认不启用。如何判断未启用,查看job资源，在spec里未发现ttlSecondsAfterFinished定义则表示未启用。 启用 我的集群使用sealos安装的，其配置文件在 /etc/kubernetes/manifests下,分别调整 kube-apiserver.yaml,kube-controller-manager.yaml,kube-scheduler.yaml配置，新增 # 示例 spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true - --feature-gates=TTLAfterFinished=true # 新增配置 image: k8s.gcr.io/kube-scheduler:v1.17.0 调整完成后稍等片刻,相关pod会重建。 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"install/k3s.html":{"url":"install/k3s.html","title":"k3s安装","keywords":"","body":" k3s 安装小记 k3s 是Rancher推出的轻量级 k8s. 下载安装脚本 # 境外 curl -sfL https://get.k3s.io > install.sh # 大陆 curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh > k3s-install.sh 安装master节点 INSTALL_K3S_EXEC=\"--no-deploy traefik --node-ip 10.147.20.41 --docker\" ./install.sh # 配置kubeconfig cp -a /etc/rancher/k3s/k3s.yaml /root/.kube/config 安装worker节点 # token 是从 master 节点的 /var/lib/rancher/k3s/server/node-token 文件里获取的。 K3S_TOKEN=K107941e2fbb3596e5678ee39c0ac875fe83bf97b05535f898e06d8881bf1a65212::server:380bb2b3064b115f110260aec43a72e3 K3S_URL=https://10.147.20.41:6443 INSTALL_K3S_MIRROR=cn INSTALL_K3S_EXEC=\"--node-ip 10.147.20.43 --docker\" ./k3s-install.sh 安装ingress Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/intro/":{"url":"kubernetes/intro/","title":"概念篇","keywords":"","body":"概念篇 Kubernetes 基本概念和使用方法 架构 图片 - k8s架构 图片 - k8s整体架构 etcd保存了整个集群的状态； apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制； controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上； kubelet负责维护容器的生命周期，同时也负责Volume（CSI）和网络（CNI）的管理； Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）； kube-proxy负责为Service提供cluster内部的服务发现和负载均衡； CoreDns 集群dns解析服务 Ingress Controller 应用层负载均衡提供对外访问服务 Kube-state-metrics 集群监控 Prometheus 资源监控 Dashboard Web UI Master(Control Plane)节点 图片 - master节点架构 Master是集群的控制平面: 负载集群的全局决策(如调度等) 探测响应集群事件(如探测应用的实例数是否符合预期) 通常master节点不调度业务服务 Master节点主要包括kube-apiserver、kube-scheduler、kube-controller-manager和etcd四个组件。 kube-apiserver: API服务, Kubernetes控制平面的前端，所有请求入口 kube-scheduler: watch API资源状态，根据相关条件调度到合适的node节点创建相关资源 kube-controller-manager: 生命周期管理和API业务逻辑(如: 节点控制器,副本控制器) etcd: k8s数据存储组件 cloud-controller-manager: 云服务商组件，对接各家云资源(云服务商维护) Worker(Node)节点 图片 - worker节点架构 包括kubelet、kube-proxy和Container Runtime三个组件。 kubelet: 运行在集群每个节点的客户端，需要确保相关容器运行在pod中； 通过PodSpecs标签，描述容器的运行状态； kubelet只管理通过kubernetes创建的容器。 kube-proxy： 是一个运行在集群每个节点的网络代理组件,主要是维护网络规则，保证集群内外与Pod通信。 Container Runtime： 支持运行容器底层环境的软件； 支持： CRI(Container Runtime Interface) 如Docker/Containerd。 核心Addons Addons 使用 Kubernetes 资源（DaemonSet、Deployment等）实现集群的功能特性。由于他们提供集群级别的功能特性，addons使用到的Kubernetes资源都放置在 kube-system名称空间下。 CNI(Calico等) DNS(CoreDNS等) UI(Dashboard) ... 官方Addons Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/intro/deploy.html":{"url":"kubernetes/intro/deploy.html","title":"部署第一个应用","keywords":"","body":"部署第一个应用 # https://ysicing.me/hack/demo/deploy.yaml apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本 kind: Deployment #该配置的类型，我们使用的是 Deployment metadata: #译名为元数据，即 Deployment 的一些基本属性和信息 namespace: default # 命名空间 name: demo-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: demo #为该Deployment设置key为app，value为demo的标签 spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 #strategy: #滚动策略 最多新增一个，最小下线一个 # rollingUpdate: # maxSurge: 1 # maxUnavailable: 1 # type: RollingUpdate selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:demo的资源 app: demo template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:demo的Pod app: demo spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: godemo #container的名称 image: ysicing/godemo #使用镜像godemo创建container，该container默认80端口可访问 部署应用 kubectl apply -f https://ysicing.me/hack/demo/deploy.yaml deployment.apps/demo-deployment created 查看部署结果 # 默认ns就是default可省却 kubectl get deployments -n default NAME READY UP-TO-DATE AVAILABLE AGE demo-deployment 1/1 1 1 2m54s kubectl get pods NAME READY STATUS RESTARTS AGE demo-deployment-59cd96d4d5-cjjwr 1/1 Running 0 3m17s Deployment是pod控制器 常见pod控制器 守护型: 无状态非系统级应用: Deployment (如nginx) 无状态系统级应用: DaemonSet (如日志监控收集端，每个node节点仅且需要跑一个pod) 有状态应用: StatefulSet (数据库类应用如mysql等) 非守护型: 一次性任务: Job 定时任务: CronJob 常用命令 # 获取资料 kubectl explain 类型(如pods) # 获取资源信息 kubectl get 资源类型 kubectl get pods 获取default租户的所有pods资源列表 kubectl get nodes 获取节点资源列表 kubectl get deploy 获取default租户类型为deployment的资源列表 # 显示资源的详细信息 kubectl describe 资源类型 资源名称 kubectl describe deploy demo-deployment 获取default租户deployment类型且名为demo-deployment的详细信息 # 看pod日志，类似docker logs kubectl logs Pod名称 kubectl logs demo-deployment-59cd96d4d5-cjjwr 查看default租户下pod名为demo-deployment-59cd96d4d5-cjjwr的日志 # 进入容器，类型docker exec kubectl exec -it Pod名称 操作命令 kubectl exec -it demo-deployment-59cd96d4d5-cjjwr ash / # 访问部署的应用 那么，部署完第一个应用又该如何访问? Kubernetes 中的 Service（服务） 提供了这样的一个抽象层，它选择具备某些特征的 Pod（容器组）并为它们定义一个访问方式。 在创建Service的时候，通过设置配置文件中的 spec.type 字段的值，可以以不同方式向外部暴露应用程序，这里介绍常用的3种： ClusterIP(默认): 集群内部可访问 NodePort: NAT方式,可以通过访问集群中任意节点+端口号的方式访问服务 :,此时ClusterIP的访问方式仍然可用。 LoadBalancer: 负载均衡(依赖云访问)。此时 ClusterIP 和 NodePort 的访问方式仍然可用。 kubectl explain svc # https://ysicing.me/hack/demo/svc.yaml apiVersion: v1 kind: Service metadata: name: demo-service #Service 的名称 labels: #Service 自己的标签 app: demo #为该 Service 设置 key 为 app，value 为 demo 的标签 spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: demo #选择包含标签 app:demo 的 Pod ports: - name: demo-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 生效 kubectl apply -f https://ysicing.me/hack/demo/svc.yaml service/demo-service created 查看service，通过之前的文档 kubectl get svc -l app=demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-service NodePort 10.96.37.87 80:32600/TCP 55s --- kubectl describe svc/demo-service Name: demo-service Namespace: default Labels: app=demo Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"demo\"},\"name\":\"demo-service\",\"namespace\":\"default\"},\"spe... Selector: app=demo Type: NodePort IP: 10.96.37.87 Port: demo-port 80/TCP TargetPort: 80/TCP NodePort: demo-port 32600/TCP Endpoints: 172.16.219.5:80 Session Affinity: None External Traffic Policy: Cluster Events: 测试服务访问 # 在集群节点 root@k8s1:~# curl 10.96.37.87 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl 172.16.219.5 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl 192.168.100.101:32600 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl 192.168.100.102:32600 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} 那么问题来了，如果想通过clusterip方式提供对外服务，该怎么做？ # https://ysicing.me/hack/demo/ing.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: labels: app: demo name: demo-ingress # ingress名 namespace: default spec: rules: - host: godemo.slb.k7s.xyz # 域名 http: paths: - backend: serviceName: demo-service # godemo的 service名 servicePort: demo-port # godemo的service定义的port path: / #路径 生效 kubectl apply -f https://ysicing.me/hack/demo/ing.yaml ingress.networking.k8s.io/demo-ingress created kubectl get ing NAME HOSTS ADDRESS PORTS AGE demo-ingress godemo.slb.k7s.xyz 80 91s 验证ingress curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} 这时候流量增加，怎么快速伸缩应用。 伸缩应用 伸缩 的实现可以通过更改 deploy.yaml 文件中部署的 replicas（副本数）来完成 # replicas: 1 ---> replicas: 4 # 改完生效 kubectl apply -f https://ysicing.me/hack/demo/deploy2.yaml deployment.apps/demo-deployment configured # 查看pod kubectl get pods -l app=demo NAME READY STATUS RESTARTS AGE demo-deployment-59cd96d4d5-78v28 1/1 Running 0 16s demo-deployment-59cd96d4d5-cjjwr 1/1 Running 0 15m demo-deployment-59cd96d4d5-mn7r8 1/1 Running 0 16s demo-deployment-59cd96d4d5-mvxk2 1/1 Running 0 16s # 修改了 Deployment 的 replicas 为 4 后，Kubernetes 又为该 Deployment 创建了 3 新的 Pod，这 4 个 Pod 有相同的标签。因此nginx Service 通过标签选择器与新的 Pod建立了对应关系，将访问流量通过负载均衡在 4 个 Pod 之间进行转发 Name: demo-service Namespace: default Labels: app=demo Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"demo\"},\"name\":\"demo-service\",\"namespace\":\"default\"},\"spe... Selector: app=demo Type: NodePort IP: 10.96.37.87 Port: demo-port 80/TCP TargetPort: 80/TCP NodePort: demo-port 32600/TCP Endpoints: 172.16.109.71:80,172.16.109.72:80,172.16.219.5:80 + 1 more... Session Affinity: None External Traffic Policy: Cluster Events: 验证效果,流量是负载到后端不同pod上 root@k8s1:~# curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-mn7r8\",\"ip\":{\"eth0\":\"172.16.109.71/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-78v28\",\"ip\":{\"eth0\":\"172.16.219.6/32\",\"lo\":\"127.0.0.1/8\"}} 最后 陈述式： kubectl create -f xx.yaml 申明式（建议使用）： kubectl apply -f xx.yaml pod容器如果未发生调度，重启容器ip是不会改变的 另外除了Service这种网络，还有hostPort,hostNetwork hostPort：直接将容器的端口与所调度的节点上的端口路由，这样可以通过宿主机的IP加上来访问Pod了, Ingress就是这样的 hostNetwork：共享宿主机的网络名称空间 这里可以这么测试使用hostPort kubectl apply -f https://ysicing.me/hack/demo/deploy3.yaml kubectl get pods -l app=demo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deployment-6c5664f4d6-s6w8v 1/1 Running 0 112s 172.16.109.68 k8s2 curl 666.slb.k7s.xyz:28080 {\"hostname\":\"demo-deployment-6c5664f4d6-s6w8v\",\"ip\":{\"eth0\":\"172.16.109.68/32\",\"lo\":\"127.0.0.1/8\"}} Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/intro/sa.html":{"url":"kubernetes/intro/sa.html","title":"资源权限控制","keywords":"","body":"k8s 资源管理的权限控制 在k8s中，由系统自身的接口来创建和管理的账号类型只有一种，叫做ServiceAccount。 可以使用下面的命令来查看目前系统已有的ServiceAccount，简写sa ➜ ~ kubectl get sa NAME SECRETS AGE default 1 2d8h 查看sa具体定义 ➜ ~ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-01-05T06:27:59Z\" name: default namespace: default resourceVersion: \"428\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 9881e9a3-4097-44b4-a9e3-e5a9d4e516e0 secrets: - name: default-token-8xcpl # 引用了名为default-token-8xcpl的secrets ➜ ~ kubectl get secrets default-token-8xcpl -o yaml apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5akNDQWJLZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQ0FYRFRJd01ERXdOVEEyTWpjeE5Gb1lEekl4TVRneE1qRXlNRFl5TnpFMFdqQVZNUk13RVFZRApWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBCnoyS0ttbzh2emdXWE1WR09hSy9DeWk2cGZ2T0psWUMxd1paRTdWS3IwTENFZDBQWXVKbk10NStQdVhKUkw4OFUKK2t2U0RGRm1RS3J6QUhCN0IwNGVybEwyd0NHeDZwa2FIOFBMMTJqKzdUUW5VS2doa1lHb1ZxUjNKV3NTSS9jcwo0ampGWTJyVmY3Z3pDNG9LbHlBc2VRdXFRaERPV004emtCalFYa3gyZVdnSSthRFNpOHd5SHNSNXZwK0Y4TkNOCnEreHY3bFczOERBcTB2SGlSYnBlZHVCTWpUTksyaHlYYWpyeFpWNTZxTTdnRUJWcVBjRCtUWmQySGs2SGQ2dlgKaGhTeTdUQ3lOY1kvbi9HNjRscTBUaG9JZTBWaFBncG8rU3JzRXVOVTBqWlFmRllDMDRWZlluSU0yZmxuSkY5YQpVWjkraVEza25ZR3RmSTA4cXRScjh3SURBUUFCb3lNd0lUQU9CZ05WSFE4QkFmOEVCQU1DQXFRd0R3WURWUjBUCkFRSC9CQVV3QXdFQi96QU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFOWHdQUkRLd05yMEFzMVpiN1NWWnVQeSsKMVhtbWhWS3dVUmVROXF2QjQwWU5sTVBKMVVqbUNyVmdVRmM4WldmUEhwTmZ6Z2dXTUJUbFFrZDhOK1NhTHB3bAo0bTJtMlpmTTV6Q3R2QVg2MzhHZUVPYTViVkNHcXhudUNVOWQxb2p0M2JZSkYwZGxSMy9VY3FpaDRaeEdUL0syCkJweEZ5QXBRRUZ0elhVZmE0dTBYcFBwUU1aa2txK2hkd01mZjQ3VlBma3NUazUvN2ZaK08vUXk4SElSM00wWFEKUWZyRWpIRXRwL2VSU1hENm95ZHF4R2RKL0pOWU4rKzJYU05lRm51bDFOakh2bG1IT1JpRDI2S2Rqb24vT1Y5cgplYlp0T1oxRmJPVVVFNktJRG9CWTJIK0JCWVppNkwzajJBRkpFTFozWE5tNUdlSTB0NGduWW8zSXU0SnRmZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K namespace: ZGVmYXVsdA== token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkltUnBTa2RCWlZwcVMwOVdOVmxCVDJkemQySTBkRlpPVVVkRlVHOVZYMXB0VDA1Q1dXeG5UVGhhTVdjaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbVJsWm1GMWJIUXRkRzlyWlc0dE9IaGpjR3dpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdWbVlYVnNkQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJams0T0RGbE9XRXpMVFF3T1RjdE5EUmlOQzFoT1dVekxXVTFZVGxrTkdVMU1UWmxNQ0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT21SbFptRjFiSFFpZlEub1NwZ0w4V3liYlJEb0tvYUpxclJ1RkxGOUN6SW1tdlo3VEtWU1RXdVpnWVpCWTRRZ092NjVCbGFLTF9qQU9DSFNyQzd6WW5pSjlXVDVCcVd2N3dfQU9XdEhURVJWSU00Y3I5LXh6LUxMRHZ6bl9qZlNIR0VoSnBPZHZnMGppRFZydi0xanJOb2g1S3VKMGw0TlNULVBnemtUQTU5bWhfNzdodFRtdzJfaDJCWHNDWDBROFg1dm5uMTBMVFJaeHRtNnZTank5dVRIcUZpa0pkU2pTX2c1SjJiS3BXVW1sZnl6OWNKTkliNWhDS0hYVEYyWGlDaG9vMlI3RmdvcHc5X1ltNWxLc2JudGh0bHA5TDUzZ2M2UV9IUkRvb05hSk04RHZndTNybXJheUhPa193RkFTbE1XZ2NkZzF0OGNjYXIwM1V0aWZ5RElDS2s1OTBMM2d2T0hR kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 9881e9a3-4097-44b4-a9e3-e5a9d4e516e0 creationTimestamp: \"2020-01-05T06:27:59Z\" name: default-token-8xcpl namespace: default resourceVersion: \"421\" selfLink: /api/v1/namespaces/default/secrets/default-token-8xcpl uid: 2f07379e-80bb-4c24-a3e3-8a37d1c4c9fb type: kubernetes.io/service-account-token 创建sa ➜ ~ kubectl create sa ysicing serviceaccount/ysicing created ➜ ~ kubectl get sa ysicing -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-01-07T14:40:07Z\" name: ysicing namespace: default resourceVersion: \"134248\" selfLink: /api/v1/namespaces/default/serviceaccounts/ysicing uid: e5f19802-a196-47d6-b194-3684642357ef secrets: - name: ysicing-token-zvgr4 # 获取token,这里token是原始token，未经过base64加密 kubectl describe secrets ysicing-token-zvgr4 测试请求k8s api,发现还是403. ➜ ~ curl -k https://192.168.100.101:6443/healthz/ping -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ' { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"forbidden: User \\\"system:serviceaccount:default:ysicing\\\" cannot get path \\\"/healthz/ping\\\"\", \"reason\": \"Forbidden\", \"details\": { }, \"code\": 403 } k8s对资源操作权限划分的比较详细: 写权限 create update delete patch 读权限 get list watch 配置sa ysicing访问资源,即配置 .kube/config # 配置sa ysicing token鉴权信息 kubectl config set-credentials ysicing --token eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ User \"ysicing\" set. # 配置集群访问信息 # .kube/ca.crt 从192.168.100.101:/etc/kubernetes/pki/ca.crt获取的 ➜ ~ kubectl config set-cluster local --server https://192.168.100.101:6443 --certificate-authority .kube/ca.crt --embed-certs=true Cluster \"local\" set. # 配置context, 将cluster & user 绑定 ➜ ~ kubectl config set-context local-ctx --cluster local --user ysicing Context \"local-ctx\" created. # 切换ctx ➜ ~ kubectl config use-context local-ctx Switched to context \"local-ctx\". 请求接口 ➜ ~ kubectl get pods Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:default:ysicing\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\" 授权配置 创建角色，定义资源操作权限(ClusterRole/Role) 角色绑定，将角色与Sa绑定(ClusterRoleBinding/RoleBinding) # 创建角色 ➜ ~ kubectl create role ysicing-role --resource pod,service,deployment,secret,ingress --verb create,update,delete,patch,get,list,watch role.rbac.authorization.k8s.io/ysicing-role created ➜ ~ kubectl get roles NAME AGE ysicing-role 34s # 角色绑定 ➜ ~ kubectl create rolebinding ysbot.ysicing-binding --role ysicing-role --serviceaccount default:ysicing rolebinding.rbac.authorization.k8s.io/ysbot.ysicing-binding created 再次获取pods信息 ➜ ~ kubectl config use-context local-ctx Switched to context \"local-ctx\". ➜ ~ kubectl get pods NAME READY STATUS RESTARTS AGE demo-deployment-6c5664f4d6-s6w8v 1/1 Running 0 2d4h 再次请求https://192.168.100.101:6443/healthz/ping发现依旧403，那么给ysicing sa通过ClusterRoleBinding 授予一个名为 cluster-admin 的 ClusterRole ➜ ~ kubectl config use-context kubernetes-admin@kubernetes Switched to context \"kubernetes-admin@kubernetes\". ➜ ~ kubectl create clusterrolebinding cluster-ysicing --clusterrole cluster-admin --serviceaccount default:ysicing clusterrolebinding.rbac.authorization.k8s.io/cluster-ysicing created ➜ ~ curl -k https://192.168.100.101:6443/healthz/ping -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ' ok% # 真香哈哈哈哈 cluster-admin权限特别大,实际使用还需要谨慎操作。 # 可以查看具体定义 kubectl get role ysicing-role Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/addons/ingress/tls-config.html":{"url":"kubernetes/addons/ingress/tls-config.html","title":"配置域名证书","keywords":"","body":" nginx ingress 配置域名证书(默认以ingress-nginx为例) 创建证书 默认已经签发证书 创建 secret 创建好证书以后，需要将证书内容放到 secret 中，secret 中全部内容需要 base64 编码 # ingress-secret.yml apiVersion: v1 kind: Secret metadata: name: ingress-secret-prom namespace: monitoring type: kubernetes.io/tls data: tls.crt: tls.key: 完成创建 ~# kubectl apply -f ingress-secret.yml -n monitoring secret/ingress-secret created ~# kubectl apply -f ingress-secret.yml -n kube-system secret/ingress-secret created 或者通过如下方式 kubectl create secret tls ingress-secret --key tls.key.pem --cert tls.pem 配置ingress apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: prom namespace: monitoring spec: tls: - hosts: - prom.k7s.xyz - grafana.k7s.xyz - alter.k7s.xyz secretName: ingress-secret rules: - host: prom.k7s.xyz http: paths: - backend: serviceName: prometheus-k8s servicePort: 9090 - host: grafana.k7s.xyz http: paths: - backend: serviceName: grafana servicePort: 3000 - host: alter.k7s.xyz http: paths: - backend: serviceName: alertmanager-main servicePort: 9093 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/network/":{"url":"kubernetes/network/","title":"网络篇","keywords":"","body":"Ucloud 网络 ucloud默认mtu是 1454 所以calico需要改mtu kubectl patch configmap/calico-config -n kube-system --type merge -p '{\"data\":{\"veth_mtu\": \"1404\"}}' 参考 Configure MTU to maximize network performance Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/storage/":{"url":"kubernetes/storage/","title":"存储篇","keywords":"","body":" 配置默认存储 # 集群已有存储类型（StorageClass），执行 kubectl get sc看下当前是否设置了默认的 storageclass kubectl get sc kubectl patch storageclass nfs-data -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/plugins/":{"url":"kubernetes/plugins/","title":"插件篇","keywords":"","body":"kubectl plugin插件机制初体验之krew 插件机制 这个早就已经GA了，其作用允许开发者以独立的二进制或脚本形式发布自定义的kubectl子命令, 灵活快速操作k8s 插件不限制语言，只需将脚本或二进制可执行文件以kubectl-的前缀放到PATH中即可. 官方示例 官方Go示例辅助库 https://github.com/kubernetes/cli-runtime.git 不太简单的官方示例插件 https://github.com/kubernetes/sample-cli-plugin 示例插件 mkdir /usr/local/k8s export PATH=$PATH:/usr/local/k8s 示例插件如下： root@k8s1:~# cat /usr/local/k8s/kubectl-demo #!/bin/bash echo $1 赋予执行权限 chmod +x /usr/local/k8s/kubectl-demo 运行测试: root@k8s1:~# kubectl demo 666 666 root@k8s1:~# kubectl plugin list The following compatible plugins are available: /usr/local/k8s/kubectl-demo kubectl plugin list 可以查看PATH中查看有哪些插件 krew Package manager for kubectl plugins -- krew 安装krew # Bash and ZSH ( set -x; cd \"$(mktemp -d)\" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/download/v0.3.4/krew.{tar.gz,yaml}\" && tar zxvf krew.tar.gz && KREW=./krew-linux_amd64 && \"$KREW\" install --manifest=krew.yaml --archive=krew.tar.gz && \"$KREW\" update ) # .bashrc/.zshrc export PATH=$PATH:/usr/local/k8s:${KREW_ROOT:-$HOME/.krew}/bin 查看当前可用的kubectl plugin，发现多了一个kubect-krew root@k8s1:~# kubectl plugin list The following compatible plugins are available: /usr/local/k8s/kubectl-demo /root/.krew/bin/kubectl-krew krew使用 kubectl krew krew is the kubectl plugin manager. You can invoke krew through kubectl: \"kubectl krew [command]...\" Usage: krew [command] Available Commands: help Help about any command info Show information about a kubectl plugin install Install kubectl plugins list List installed kubectl plugins search Discover kubectl plugins uninstall Uninstall plugins update Update the local copy of the plugin index upgrade Upgrade installed plugins to newer versions version Show krew version and diagnostics Flags: -h, --help help for krew -v, --v Level number for the log level verbosity Use \"krew [command] --help\" for more information about a command. 初体验 root@k8s1:~# kubectl get pods No resources found in default namespace. root@k8s1:~# kubectl krew install change-ns Installing plugin: change-ns Installed plugin: change-ns root@k8s1:~# kubectl krew list PLUGIN VERSION change-ns v1.0.0 krew v0.3.3 root@k8s1:~# kubectl change-ns nginx-ingress namespace changed to \"nginx-ingress\" root@k8s1:~# kubectl get pod root@k8s1:~# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ingress-rfswh 1/1 Running 0 2m6s nginx-ingress-v6c4l 1/1 Running 0 2m6s Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/addons/":{"url":"kubernetes/addons/","title":"Addons篇","keywords":"","body":"附加组件 扩展了Kubernetes的功能 部署kuboard kubectl apply -f https://ysicing.me/hack/k7s/install/kuboard/deploy.yml # 因为ingress部署关系，所以配置解析域名为ui.slb.k7s.xyz # 管理节点执行，获取token kbtoken 部署metrics-server kubectl apply -f https://ysicing.me/hack/k7s/install/metrics-server/deploy.yaml 部署Dashboard sealos install --pkg-url https://github.com/sealstore/dashboard/releases/download/v2.0.0-bata5/dashboard.tar kdtoken Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/helm/drone.html":{"url":"kubernetes/helm/drone.html","title":"安装Drone","keywords":"","body":"使用Helm安装Drone 一个比较热门的轻量级CI/CD开源工具：Drone 简介 Drone是用Go开发的开源轻量级CI/CD工具 使用简单的 YAML 配置文件来定义和执行 Docker 容器中定义的 Pipeline 构成简单,服务占用资源少 Server端负责身份认证，仓库配置，用户、Secrets 以及 Webhook 相关的配置 Agent端用于接受构建的作业和真正用于运行的 Pipeline 工作流 安装简单，支持主流Git托管服务(github,gitea等) 官方文档也很全 环境 helm v3.0.2 drone 1.6.1 k8s 1.17.0 准备工作 注册Github OAuth应用 图片 - drone github oauth应用 获取到github oauth Client ID,Client Secret留用。 创建持久化存储 # https://ysicing.me/hack/demo/pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: dronepv spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi persistentVolumeReclaimPolicy: Delete storageClassName: nfs-test nfs: server: 192.168.100.101 path: /k8sdata/default-drone-pvc --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: dronepvc namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: nfs-test 示例 # nfs 地址需要按需调整一下 kubectl apply -f https://ysicing.me/hack/demo/pvc.yaml kubectl get pvc 安装 本文使用helm方式安装，因为需要对配置做些调整，需要自定义一些配置 helm repo update # 获取最新的离线drone包 helm pull stable/drone tar xf drone-2.4.0.tgz && cd drone # 编辑values.yaml 示例values.yaml如下 # 有注释的地方说明有修改 images: server: repository: \"docker.io/drone/drone\" tag: 1.6.1 pullPolicy: IfNotPresent agent: repository: \"docker.io/drone/agent\" tag: 1.6.1 pullPolicy: IfNotPresent dind: repository: \"docker.io/library/docker\" tag: 18.06.1-ce-dind pullPolicy: IfNotPresent service: httpPort: 80 type: ClusterIP exposeGRPC: false ingress: enabled: true # 启用 hosts: - drone.godu.dev # 域名 tls: - secretName: godu.dev # 证书，集群内已经证书的secret了，可以不启用，因为我的dev域名必须https访问 hosts: - drone.godu.dev # 域名 sourceControl: provider: github # github secret: github: clientID: xxx # github oauth id xxx clientSecretKey: clientSecret clientSecretValue: xxxx # github oauth secret xxxx server: https://github.com gitlab: clientID: clientSecretKey: clientSecret clientSecretValue: server: gitea: clientID: clientSecretKey: clientSecret clientSecretValue: server: gogs: server: bitbucketCloud: clientID: clientSecretKey: clientSecret clientSecretValue: bitbucketServer: server: consumerKey: consumerKey privateKey: privateKey username: passwordKey: password server: host: \"drone.godu.dev\" protocol: https rpcProtocol: http adminUser: ysicing # github 登录后就具有管理员权限 alwaysAuth: false kubernetes: enabled: true # 运行 Drone 的任务的时候就是直接使用 Kubernetes 的 Job 资源对象来执行，而不是 Drone 的 agent. env: DRONE_LOGS_DEBUG: \"false\" DRONE_DATABASE_DRIVER: \"sqlite3\" DRONE_DATABASE_DATASOURCE: \"/var/lib/drone/drone.sqlite\" annotations: {} resources: {} affinity: {} nodeSelector: {} tolerations: [] extraContainers: | extraVolumes: | agent: env: DRONE_LOGS_DEBUG: \"false\" replicas: 1 annotations: {} resources: {} livenessProbe: {} readinessProbe: {} affinity: {} nodeSelector: {} tolerations: [] dind: enabled: true driver: overlay2 resources: {} metrics: prometheus: enabled: true persistence: enabled: true existingClaim: dronepvc # 刚刚创建的持久化 pvc rbac: create: true apiVersion: v1 serviceAccount: create: true name: 部署drone $ helm install drone -f values.yaml stable/drone $ helm status drone NAME: drone LAST DEPLOYED: Sun Jan 5 20:30:00 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ********************************************************************************* *** PLEASE BE PATIENT: drone may take a few minutes to install *** ********************************************************************************* From outside the cluster, the server URL(s) are: http://drone.godu.dev 最后 root@k8s.cn1:~/drone/drone# kubectl get pods -l app=drone NAME READY STATUS RESTARTS AGE drone-drone-server-5bffbc56df-qzk28 1/1 Running 0 35m root@k8s.cn1:~/drone/drone# kubectl get ing -l app=drone NAME HOSTS ADDRESS PORTS AGE drone-drone drone.godu.dev 80, 443 36m 触发CI构建 # .drone.yml kind: pipeline name: default steps: - name: build image: golang:latest environment: GOPROXY: https://goproxy.cn commands: - CGO_ENABLED=0 go build - name: docker image: plugins/docker settings: repo: ysicing/godemo use_cache: true username: from_secret: dockeruser password: from_secret: docker tags: - latest when: event: push branch: master 代码地址: BeidouCloudPlatform/demo 图片 - drone ci 图片 - drone 详情页 Cli工具 brew install drone-cli export DRONE_SERVER=https://drone.godu.dev export DRONE_TOKEN= drone info # 修复webhook，如果调整了域名，可以通过此命令修复webhook drone repo repair BeidouCloudPlatform/demo Job清理问题 因为使用Job的方式进行pipline操作，如果不启用TTLAfterFinished会导致job不会被自动清理。 开启feature请参考 feature开启 默认drone清理是300s Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/helm/docker-registry.html":{"url":"kubernetes/helm/docker-registry.html","title":"安装Registry","keywords":"","body":"部署docker registry 最近大陆push镜像老是超时，于是想自建一个，使用helm方式, 和drone安装方式类似 部署 helm pull stable/docker-registry tar xf docker-registry-1.9.1.tgz && cd docker-registry/ # 编辑values.yaml,自定义配置 # 注释部分为调整 replicaCount: 1 updateStrategy: podAnnotations: {} podLabels: {} image: repository: registry tag: 2.7.1 pullPolicy: IfNotPresent service: name: registry type: ClusterIP port: 5000 annotations: {} ingress: enabled: true # 启用ingress path: / hosts: - hub.local.godu.dev # 域名 annotations: nginx.ingress.kubernetes.io/proxy-body-size: \"0\" # k8s ingress 413 nginx.org/client-max-body-size: \"0\" # nginxnc ingress 413 labels: {} tls: - secretName: godu.dev # 证书 hosts: - hub.local.godu.dev # 域名 resources: {} persistence: accessMode: 'ReadWriteOnce' enabled: true size: 5Gi existingClaim: registry # 同drone storage: filesystem secrets: haSharedSecret: \"\" htpasswd: \"\" configData: version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 securityContext: enabled: true runAsUser: 1000 fsGroup: 1000 priorityClassName: \"\" podDisruptionBudget: {} nodeSelector: {} tolerations: [] extraVolumeMounts: [] extraVolumes: [] deploy helm install registry -f values.yaml stable/docker-registry Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/ack/":{"url":"kubernetes/ack/","title":"ACK实践","keywords":"","body":"阿里云ACK实践 重点介绍阿里云ACK集群实践 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/ack/ingress.html":{"url":"kubernetes/ack/ingress.html","title":"新增内网slb ingress","keywords":"","body":"ACK 添加内网负载均衡 先已经创建好内网slb,获取slb的ID为lb-xxxx # intranet-slb-ingress.yml apiVersion: v1 kind: Service metadata: # 这里服务取名为nginx-ingress-lb-intranet name: nginx-ingress-lb-intranet namespace: kube-system labels: app: nginx-ingress-lb-intranet annotations: # 指明SLB实例地址类型为私网类型 service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet # 修改为您的私网SLB实例ID service.beta.kubernetes.io/alicloud-loadbalancer-id: lb-xxxx # 是否自动创建SLB端口监听（会覆写已有端口监听），也可手动创建端口监听 service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: 'true' spec: type: LoadBalancer # route traffic to other nodes externalTrafficPolicy: \"Cluster\" ports: - port: 80 name: http targetPort: 80 - port: 443 name: https targetPort: 443 selector: # select app=ingress-nginx pods app: ingress-nginx 生效内网slb kubectl apply -f intranet-slb-ingress.yml 查看slb nginx-ingress-lb LoadBalancer 172.x.x.x 39.x.x.x 80:31110/TCP,443:31574/TCP 20h nginx-ingress-lb-intranet LoadBalancer 172.x.x.x 10.x.x.x 80:30740/TCP,443:30852/TCP 73m Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"kubernetes/tutorials/setup_efk_logging_stack.html":{"url":"kubernetes/tutorials/setup_efk_logging_stack.html","title":"部署efk(fluentd)","keywords":"","body":"Kubernetes 上搭建 EFK 日志收集系统 Kubernetes 中比较流行的日志收集解决方案是 Elasticsearch、Fluentd 和 Kibana（EFK）技术栈，也是官方现在比较推荐的一种方案。 Elasticsearch是一个实时的、分布式的可扩展的搜索引擎，允许进行全文、结构化搜索，它通常用于索引和搜索大量日志数据，也可用于搜索许多不同类型的文档。 Kibana是Elasticsearch的一个功能强大的数据可视化 Dashboard，Kibana允许你通过 web 界面来浏览Elasticsearch日志数据。 Fluentd是一个流行的开源数据收集器，我们将在 Kubernetes 集群节点上安装 Fluentd，通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch集群，在该集群中对其进行索引和存储。 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"linux/":{"url":"linux/","title":"Linux","keywords":"","body":"Linux姿势 linux management note Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"linux/linux.html":{"url":"linux/linux.html","title":"Debian使用技巧","keywords":"","body":"Debian Linux,你值得拥有 快速启动一个Debian buster环境 Debian 10.3 vagrant init ysicing/debian vagrant up Debian 9.11 不在维护了 vagrant init ysicing/debian --box-version 9.11.0 vagrant up 源码 构建box源码 debian-vagrant Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"linux/debian-buster-op.html":{"url":"linux/debian-buster-op.html","title":"Debian Buster日常操作","keywords":"","body":"Debian 10操作指南 操作有风险 更新源 需要移除默认源，使用如下源 # 默认 cat >/etc/apt/sources.list /etc/apt/sources.list 更新升级 apt update apt dist-upgrade -y 升级到最新内核 apt-get install -t buster-backports linux-image-amd64 -y update-grub apt autoclean apt autoremove -y reboot Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"linux/reinstall.html":{"url":"linux/reinstall.html","title":"在线重装","keywords":"","body":"在线重装Debian 背景 一条命令快速重装干净的Linux环境 目前仅支持Debian(不会不打算支持其他系统) 基于萌咖大佬的二次魔改 默认做了 默认root密码 vagrant(安装完成建议修改，禁止密码登录) 默认配置源为mirrors.tuna.tsinghua.edu.cn,默认添加了security,backports 默认时区为Asia/Shanghai 默认安装了curl wget openssh-server sudo sed apt-transport-https net-tools等常用工具 同时默认支持自定义密码 安装 curl -sSL https://ysicing.me/hack/reinstall/install.sh | bash # 指定参数 bash 自定义硬盘 存在多个硬盘时，需要下载 https://ysicing.me/hack/reinstall/installhk.sh文件，编辑如下部分即可 d-i partman-auto/disk string /dev/sdb 参考附录 [ Linux VPS ] Debian/Ubuntu/CentOS 网络安装/重装系统/纯净安装 一键脚本 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"linux/aliyun_debian_upgrade_kernel.html":{"url":"linux/aliyun_debian_upgrade_kernel.html","title":"轻量云升级内核","keywords":"","body":"阿里云轻量应用服务器升级内核 升级有风险请慎重哦 配置说明 阿里云HK 1核1G1TB30Mbps ¥24/m Debian 9.9 更新源 需要移除默认源，使用如下源 # 需要添加buster-backports源 sed -i \"s#stretch#buster#g\" /etc/apt/sources.list # 示例 deb http://mirrors.a li yun c/debian/ buster main contrib non-free deb http://mirrors.aliyun.com/debian/ buster-updates main contrib non-free deb http://mirrors.aliyun.com/debian/ buster-proposed-updates main non-free contrib deb http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib deb http://mirrors.aliyun.com/debian-security/ buster/updates main non-free contrib 更新升级 apt-get update apt-get dist-upgrade -y 升级到最新内核 apt-get install -t buster-backports linux-image-amd64 -y update-grub apt autoclean apt autoremove -y reboot 升级完成后docker无法启动 prior storage driver aufs failed: driver not supported 删除/var/lib/docker，docker存储由aufs变成overlay2 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"linux/timers.html":{"url":"linux/timers.html","title":"定时器","keywords":"","body":"定时任务，如每65分钟执行一次 crontab 定时任务 65 分钟执行一次，怎么写？ 这个时候，用系统自带的crontab就不好实现了,这时候就是systemd该上传了 参考Systemd 定时器教程 cat /etc/systemd/system/example.timer [Unit] Description=example timer [Timer] OnUnitActiveSec=1h Unit=example.service [Install] WantedBy=multi-user.target cat /etc/systemd/system/example.service [Unit] Description=example [Service] ExecStart= [Install] WantedBy=multi-user.target 定制定时器 OnActiveSec：定时器生效后，多少时间开始执行任务 OnBootSec：系统启动后，多少时间开始执行任务 OnStartupSec：Systemd 进程启动后，多少时间开始执行任务 OnUnitActiveSec：该单元上次执行后，等多少时间再次执行 OnUnitInactiveSec： 定时器上次关闭后多少时间，再次执行 OnCalendar：基于绝对时间，而不是相对时间执行 AccuracySec：如果因为各种原因，任务必须推迟执行，推迟的最大秒数，默认是60秒 Unit：真正要执行的任务，默认是同名的带有.service后缀的单元 Persistent：如果设置了该字段，即使定时器到时没有启动，也会自动执行相应的单元 WakeSystem：如果系统休眠，是否自动唤醒系统 Timer和Service大体用法一致 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"linux/faq.html":{"url":"linux/faq.html","title":"FAQ","keywords":"","body":"FAQ docker npm安装问题 npm config set unsafe-perm true git操作 # 改崩了 git fetch --all git reset --hard origin/master # 放弃本地全部/单个 git checkout . git checkout -- filename Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"soft/dns/adguard.html":{"url":"soft/dns/adguard.html","title":"DNS-Adguard","keywords":"","body":"AdGuard使用姿势 AdGuard Home使用Golang开发，因此安装非常简单，这里以容器的方式部署为例，其它方式可参考官方帮助文档。 主要功能 拦截AD 号称隐私保护 家庭保护模式 自定义过滤(劫持) (&#x1F602;我用的最多的是这个) 部署 二话不说直接上 # docker-compose.yaml version: '2.1' services: caddy: image: spanda/caddy container_name: caddy volumes: - ./ssl:/root/.caddy - /var/log/caddy:/var/log/caddy - ./Caddyfile:/etc/Caddyfile network_mode: host restart: always dns: image: adguard/adguardhome container_name: dns volumes: - ./ad/conf:/opt/adguardhome/conf - ./ad/work:/opt/adguardhome/work network_mode: host restart: always # Caddyfile dns.ysicing.net { gzip tls root@ysicing.net log / /var/log/caddy/dns.ysicing.net.log \"{remote} {when} {method} {uri} {proto} {status} {size} {>User-Agent} {latency}\" { rotate_size 50 rotate_age 90 rotate_keep 20 rotate_compress } header / { Strict-Transport-Security \"max-age=31536000;includeSubDomains;preload\" Access-Control-Allow-Origin * Access-Control-Allow-Methods \"GET, POST, OPTIONS\" -Server } proxy / 127.0.0.1:7070 { transparent websocket } } 访问公网IP:3000,按着无脑一顿猛操作。修改默认web监听端口为127.0.0.1:7070,53端口默认监听全部。 使用 Windows 打开网络和Internet设置 打开网络和共享中心 打开以太网 打开属性 编辑TCP/IPV4 使用下面的DNS服务器 首 59.110.220.53 备 8.8.8.8 Mac 跳过很简单 安卓 自己折腾吧，私有dns搞不定，官方APP可以 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"soft/ldap/openldap_install.html":{"url":"soft/ldap/openldap_install.html","title":"OpenLDAP","keywords":"","body":"OpenLDAP安装初体验 简介这里不在描述，google即可 docke快速部署 version: '2' services: openldap: image: osixia/openldap:1.3.0 container_name: openldap environment: LDAP_LOG_LEVEL: \"256\" LDAP_ORGANISATION: \"Godu Inc.\" LDAP_DOMAIN: \"ysicing.me\" LDAP_BASE_DN: \"\" LDAP_ADMIN_PASSWORD: \"meadmin\" LDAP_CONFIG_PASSWORD: \"meconfig\" LDAP_READONLY_USER: \"true\" LDAP_READONLY_USER_USERNAME: \"readonly\" LDAP_READONLY_USER_PASSWORD: \"readonly\" LDAP_RFC2307BIS_SCHEMA: \"false\" LDAP_BACKEND: \"mdb\" LDAP_TLS: \"false\" LDAP_REPLICATION: \"false\" KEEP_EXISTING_CONFIG: \"false\" LDAP_REMOVE_CONFIG_AFTER_SETUP: \"true\" LDAP_SSL_HELPER_PREFIX: \"ldap\" tty: true stdin_open: true ports: - \"389:389\" - \"636:636\" domainname: \"ldap.ysicing.me\" # important: same as hostname # hostname: \"ldap.ysicing.me\" command: [\"--copy-service\", \"--loglevel\", \"debug\"] phpldapadmin: image: osixia/phpldapadmin:latest container_name: phpldapadmin environment: PHPLDAPADMIN_LDAP_HOSTS: \"openldap\" PHPLDAPADMIN_HTTPS: \"false\" ports: - \"8080:80\" depends_on: - openldap 验证 din openldap ldapsearch -x -H ldap://localhost -b dc=ysicing,dc=me -D 'cn=admin,dc=ysicing,dc=me' -w meadmin 或者访问127.0.0.1:8080 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"soft/wireguard/install.html":{"url":"soft/wireguard/install.html","title":"wireguard","keywords":"","body":" 内网穿透之wireguard 安装 Debian # Debian Bullseye 之前版本都需要启用backports源来支持安装 apt install wireguard -y macOS brew install wireguard-tools 配置服务端 cd /etc/wireguard # 创建服务端密钥对 umask 077 wg genkey | tee privatekey | wg pubkey > publickey # 创建wg0.conf cat > /etc/wireguard/wg0.conf Address = 10.0.0.1/24, fd86:ea04:1115::1/64 ListenPort = 51820 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o eth0 -j MASQUERADE SaveConfig = true EOF # 启动 wg-quick up wg0 # 开机启动 systemctl enable wg-quick@wg0 # 检查 wg show 配置客户端 umask 077 wg genkey | tee privatekey | wg pubkey > publickey cat > /etc/wireguard/wg0.conf Address = 10.0.0.2/24, fd86:ea04:1115::5/64 EOF 连接客户端和服务端 法一 客户端操作 wg-quick down wg0 直接编辑客户端配置文件 启动wg-quick up wg0;systemctl enable wg-quick@wg0 # /etc/wireguard/wg0.conf 新增 [Peer] PublicKey = Endpoint = :51820 AllowedIPs = 10.0.0.2/24, fd86:ea04:1115::5/64 法二 服务端操作 wg set wg0 peer endpoint :51820 allowed-ips 203.0.113.12/24,fd86:ea04:1115::5/64 问题解决 # 1. 内核模块没有 lsmod | grep wireguard && echo yes || echo no no modprobe wireguard modprobe: FATAL: Module wireguard not found in directory /lib/modules/5.4.0-0.bpo.4-amd64 dkms status wireguard, 0.0.20200318, 4.19.0-8-amd64, x86_64: installed uname -a Linux cn3 5.4.0-0.bpo.4-amd64 #1 SMP Debian 5.4.19-1~bpo10+1 (2020-03-09) x86_64 GNU/Linux dkms autoinstall Error! Your kernel headers for kernel 5.4.0-0.bpo.4-amd64 cannot be found. Please install the linux-headers-5.4.0-0.bpo.4-amd64 package # 解决方式 apt install -y linux-headers-5.4.0-0.bpo.4-amd64 # ipv6 RTNETLINK answers: Permission denied 启用ipv6即可 net.ipv6.conf.all.disable_ipv6=0 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"develop/env/nodejs.html":{"url":"develop/env/nodejs.html","title":"安装Node","keywords":"","body":"使用nvm进行node版本管理 安装nvm # 安装 curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.2/install.sh | bash # 默认会写.zshrc ### .zshrc nvm start export NVM_DIR=\"$HOME/.nvm\" [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm [ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\" # This loads nvm bash_completion ### .zshrc nvm end source .zshrc nvm install node # \"node\" is an alias for the latest version Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"develop/env/go.html":{"url":"develop/env/go.html","title":"安装Go","keywords":"","body":"安装golang 主要是用于linux安装 安装,配置 # 下载 wget https://dl.google.com/go/go1.13.6.linux-amd64.tar.gz # 解压 tar -C /usr/local -xzf go1.13.3.linux-amd64.tar.gz # 配置 .bashrc export GO111MODULE=on export GOPROXY=https://goproxy.cn export GOPATH=\"/root/go\" export GOBIN=\"$GOPATH/bin\" export PATH=$PATH:$GOBIN:/usr/local/go/bin source .bashrc # 验证 go env 一键脚本 curl https://ysicing.me/hack/install/go.sh | bash Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"develop/env/vim_config.html":{"url":"develop/env/vim_config.html","title":"vim配置","keywords":"","body":"vim配置 安装vim plug curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 写配置 向~/.vimrc写入 \" autocmd BufWritePost $MYVIMRC source $MYVIMRC \" 关闭兼容模式 set nocompatible \" 设置行号 set nu \"突出显示当前行,列 set cursorline set cursorcolumn \"显示括号匹配 set showmatch \" tab 缩进 set tabstop=4 \" 设置Tab长度为4空格 set shiftwidth=4 \" 设置自动缩进长度为4空格 set autoindent \" 继承前一行的缩进方式，适用于多行注释 \" 定义快捷键的前缀，即 let mapleader=\";\" \" ==== 系统剪切板复制粘贴 ==== \" v 模式下复制内容到系统剪切板 vmap c \"+yy \" n 模式下复制一行到系统剪切板 nmap c \"+yy \" n 模式下粘贴系统剪切板的内容 nmap v \"+p \" 开启实时搜索 set incsearch \" 搜索时大小写不敏感 set ignorecase syntax enable syntax on \" 开启文件类型侦测 filetype plugin indent on \" 启用自动补全 \" 退出插入模式指定类型的文件自动保存 au InsertLeave *.go,*.sh,*.php write \" 插件 call plug#begin('~/.vim/plugged') \" 可以快速对齐的插件 Plug 'junegunn/vim-easy-align' \" 用来提供一个导航目录的侧边栏 Plug 'scrooloose/nerdtree' \" 可以在导航目录中看到 git 版本信息 Plug 'Xuyuanp/nerdtree-git-plugin' \" 自动补全括号的插件，包括小括号，中括号，以及花括号 Plug 'jiangmiao/auto-pairs' \" 可以在 vim 中使用 tab 补全 Plug 'vim-scripts/SuperTab' Plug 'majutsushi/tagbar' \" go 主要插件 Plug 'fatih/vim-go', { 'tag': '*' } \" go 中的代码追踪，输入 gd 就可以自动跳转 Plug 'dgryski/vim-godef' call plug#end() 安装插件 vim ~/.vimrc # 安装插件 :PlugInstall # 更新插件 :PlugUpdate Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"develop/env/vscode_config.html":{"url":"develop/env/vscode_config.html","title":"vscode配置","keywords":"","body":"vscode配置 安装好Go相关插件，配置工具包 使用command+shift+P快捷键,然后键入Go: Install/Update Tools选中全部(根据需要),稍等片刻，就会安装完成。 自定义配置项 { \"git.autofetch\": true, \"files.autoSave\": \"onWindowChange\", \"workbench.colorTheme\": \"Go - Sources\", \"terminal.integrated.shell.osx\": \"/bin/zsh\", \"go.autocompleteUnimportedPackages\": true, \"go.gocodeAutoBuild\": true, \"go.useLanguageServer\": true, \"goOutliner.enableDebugChannel\": true, \"goOutliner.extendExplorerTab\": true, \"go.inferGopath\": true, \"go.docsTool\": \"godoc\", \"go.gocodePackageLookupMode\": \"go\", \"go.gotoSymbol.includeImports\": true, \"go.useCodeSnippetsOnFunctionSuggest\": true, //使用代码片段提示 \"go.useCodeSnippetsOnFunctionSuggestWithoutType\": true, \"go.formatTool\": \"goimports\", //代码格式化 // \"go.buildOnSave\": true, //保存代码时自动编译 \"go.lintOnSave\": \"file\", //保存代码时优化 \"go.vetOnSave\": \"package\", //保存代码时检查潜在错误 \"go.coverOnSave\": false //保存代码时执行测试 } Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"develop/shell/awk-func.html":{"url":"develop/shell/awk-func.html","title":"awk 中关于多个$的用法","keywords":"","body":"awk 中关于多个$的用法 上周有遇到过这个问题 awk '{print $$1}' 这个$$是什么用法呢 间接字段寻址,其类似 awk '{print $$1}' ===> awk '{print $($1)}' ===> awk '{Nr=$1; print $Nr}' 示例: $ echo -e \"1 b c d\\n2 b c d\\n3 b c d\\n4 b c d\" 1 b c d 2 b c d 3 b c d 4 b c d $ echo -e \"1 b c d\\n2 b c d\\n3 b c d\\n4 b c d\" | awk '{print $$1}' 1 # since filed #1=1 result print first field b # since filed #1=2 result print secondfield c # since filed #1=3 result print third field d # since filed #1=4 result print fourth field 类似$可以根据需要添加更多 awk '{print $$$1}' ===> awk '{print $$($1)}' ===> awk '{print $($($1))}' Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"develop/gostudy/project-layout.html":{"url":"develop/gostudy/project-layout.html","title":"Go项目结构","keywords":"","body":"Go项目结构 golang-standards/project-layout cmd // 程序入口 internal // 内部库 pkg // 公共库 api // api conf // 配置 hack // 构建部署相关文件 docs // 文档 test // 测试 vendor // Application dependencies Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"develop/gostudy/ch01/":{"url":"develop/gostudy/ch01/","title":"Go基础","keywords":"","body":"Go基础 变量和常量 关键字&保留字 # 25个保留字 break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var # 37个保留字 Constants: true false iota nil Types: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error Functions: make len cap new append copy close delete complex real imag panic recover 注意 1. 函数外的每个语句都必须以关键字开始（var、const、func等） 2. :=不能使用在函数外。 3. _多用于占位，表示忽略值(匿名变量，不占用命名空间，不会分配内存，不需要存在重复申明) 变量 package main import ( \"fmt\" ) // 全局变量m var m = 100 func main() { n := 10 m := 200 // 此处声明局部变量m fmt.Println(m, n) } 常量 常量是恒定不变的值 const pi = 3.1415 const ( // const声明多个常量时，如果省略了值则表示和上面一行的值相同 p1 = 3.1415 p2 // 3.1415 p3 // 3.1415 ) iota iota是go语言的常量计数器，只能在常量的表达式中使用。iota在const关键字出现时将被重置为0。const中每新增一行常量声明将使iota计数一次(iota可理解为const语句块中的行索引) const ( n1 = iota // 0 n2 // 1 n3 // 2 _ // 跳过某些值 n4 // 3 n5 = 10 // 10 n6 // 4 ) const ( _ = iota // 0 KB = 1 基本数据类型 package main import ( \"fmt\" \"math\" ) func main() { var a int = 17 // int 型 (int64) fmt.Printf(\"%d \\n\", a) // 17 十进制 fmt.Printf(\"%b \\n\", a) // 10001 二进制 fmt.Printf(\"%o \\n\", a) // 021 八进制 fmt.Printf(\"%x \\n\", a) // 0x11 16禁止 fmt.Printf(\"%.2f \\n\", math.Pi) // 默认都是浮点型float64 var s = \"666\" fmt.Printf(\"%T \\n\", s) // 类型 string fmt.Printf(\"%v \\n\", s) // 值 666 fmt.Printf(\"%#v \\n\", s) // 值 \"666\" } 注: 布尔类型变量的默认值为false。 Go 语言中不允许将整型强制转换为布尔型. 布尔型无法参与数值运算，也无法与其他类型进行转换 字符串 字符串需要使用\" s1 := \"ysicing\" 多行字符串需要使用``` s1 := `牛 牛 牛 牛 ` 字符串操作 package main import ( \"fmt\" \"strings\" ) func main() { a := \"666\" b := \"777\" c := a + \"-\" + b fmt.Println(len(a)) // len 长度 fmt.Println(fmt.Sprintf(\"%s%s\", a, b)) // 拼接 fmt.Println(a + b) // 拼接 fmt.Print(strings.Split(c, \"-\")[0]) // 分割 res := strings.Split(c, \"-\") // 分割 fmt.Println(res) fmt.Println(strings.HasPrefix(c, \"6\"), strings.HasSuffix(c, \"6\")) //前缀/后缀判断 s := \"China万岁\" for i := 0; i uint8类型, 或者叫 byte型 代表了ASCII码的一个字符. rune类型, 代表一个 UTF-8字符(中文), 实际是一个int32. 因为UTF8编码下一个中文汉字由3~4个字节组成,一个rune字符由一个或多个byte组成. 流程控制 if if 表达式1 { 分支1 } else if 表达式2 { 分支2 } else{ 分支3 } for package main import \"fmt\" func main() { // 基本格式 for i := 1; i %v(%T)-->%c\\n\", i, j, j, j) // rune } // 死循环 //for { // fmt.Println(\"666\") //} } switch case 简化判断 a := 4 switch a { case a goto func main() { for i := 1; i 运算符 算数运算符: + - * / % 关系运算符: == != > >= > (除2的n次方) 赋值运算符: 先其他运算再赋值 5 > 2 // 5 --- 101 右移 2 001 --- 1 注: ++,-- 单独语句，不是运算符 数组 var 数组变量名 [元素数量]T // 一旦定义，长度不能变 var a [3]int 数组初始化 var a1 [3]int //数组会初始化为int类型的零值 var a2 = [3]int{1,2} //使用指定的初始值完成初始化 var a3 = [...]int{1, 2} // 让编译器根据初始值的个数自行推断数组的长度 a4 := [...]int{1: 1, 3: 5} // 指定索引值的方式来初始化数组 [0, 1, 0 ,3] 数组遍历 package main import \"fmt\" func main() { var a = [...]int{1, 2, 3, 4, 5} // for循环遍历 for i := 0; i 数组是值类型，赋值和传参会复制整个数组。因此改变副本的值，不会改变本身的值。 切片slice 切片是引用类型，拥有相同类型元素的可变长度的序列 var 切片名 []切片类型 var name []string # 动态创建 make([]T, size, cap) // T 类型，size 长度， cap容量(从第一个到最后容量数)，cap 可省却 package main import \"fmt\" func main() { //var a []int var c = []string{\"a\", \"b\"} fmt.Println(len(c), cap(c)) d := make([]int, 2, 10) fmt.Println(len(d), cap(d)) e := d[3:6] fmt.Println(len(e), cap(e)) // 3， 3-10 7 s1 := make([]int, 3) // [0,0,0] s2 := s1 // 拷贝前后两个变量共享底层数组，对一个切片的修改会影响另一个切片的内容 s2[0] = 100 fmt.Println(s1) fmt.Println(s2) // 遍历和for一样 for i := 0; i 切片的本质就是对底层数组的封装 append package main import \"fmt\" func main() { var num []int for i := 0; i copy copy(目标切片,源切片) func main() { // copy()复制切片 a := []int{1, 2, 3, 4, 5} c := make([]int, 5, 5) copy(c, a) //使用copy()函数将切片a中的元素复制到切片c fmt.Println(a) //[1 2 3 4 5] fmt.Println(c) //[1 2 3 4 5] c[0] = 1000 fmt.Println(a) //[1 2 3 4 5] fmt.Println(c) //[1000 2 3 4 5] var a = make([]string, 5, 10) // 已经有5个了 for i := 0; i 指针 取地址 &a 根据地址取值 *b make & new 都用来申请内存 make: slice，map， chan申请内存，返回类型本身 new: 基本数据类型申请内存, 返回指针, 且内存对应的值为类型零值 a := new(int) map 散列表(hash)实现, 无序的基于key-value的数据结构,引用类型，必须初始化才能使用. map[KeyType]ValueType make(map[keytype]valuetype, [cap]) code := make(map[string]string, 10) code[\"a\"] = \"a\" code[\"b\"] = \"b\" # 判断 值是否存在 value, ok := map[key] if ok { // 存在 } else { // 不存在 } // 循环 for k, v := range map { // k,v v可省略 } 函数 func 函数名(参数)(返回值){ } 示例 func sum1(x, y int) int { sum := x + y return sum } // 返回值定义了返回值名，return可省却 func sum2(x, y int) (sum int) { sum = x + y return } // 可变参数，可变参数可有可无 func sum3(x int, y ...int) { fmt.Println(x) fmt.Println(y) // 切片 } func main() { r1 := sum1(1, 2) r2 := sum2(1, 2) fmt.Println(r1, r2) sum3(1) // 1 [] sum3(1, 2) 1 [2] sum3(1, 2, 3) 1 [2,3] } defer 延迟处理, 多个defer 按照先进后出处理。 函数return分成两步 返回值赋值 defer return package main import \"fmt\" func deferdemo(name string) { fmt.Println(name) } func f1() int { x := 5 defer func() { x++ // 修改x不是 返回值 }() // 返回值赋值 5 // 修改x值为 6 // retrun 5 return x } func f2() (x int) { defer func() { x++ }() // 返回值赋值 x=5 // 修改x值为 x=6 // return 6 return 5 // 返回值 } func f3() (y int) { x := 5 defer func() { x++ }() // y = x = 5 // x = 6 // return y = 6 return x } func f4() (x int) { defer func(x int) { x++ }(x) // x = 5 // 副本 x = 6 // return x return 5 } func main() { deferdemo(\"666\") defer deferdemo(\"777\") deferdemo(\"888\") defer deferdemo(\"999\") fmt.Println(f1(), f2(), f3(), f4()) // 5,6,5,5 } 匿名函数 // 多次执行 e1 := func(x, y int) { fmt.Println(x) fmt.Println(y) fmt.Println(x + y) } e1(2, 4) // 立即执行，只执行一次 func(x, y int) { fmt.Println(x * y) }(1, 2) 闭包 函数作为返回值 外部变量引用 package main import \"strings\" import \"fmt\" // checkMail func checkMail(domain string) func(string) string { return func(name string) string { if strings.HasSuffix(name, domain) { return name } return name + \"@\" + domain } } func main() { talkFunc := checkMail(\"@ysicing.me\") fmt.Println(talkFunc(\"i\")) fmt.Println(talkFunc(\"root@ysicing.me\")) } panic/recover recover()和defer一起使用 defer在panic语句前 package main import \"fmt\" func funcA() { fmt.Println(\"a\") } func funcB() { defer func() { err := recover() fmt.Println(err) }() panic(\"error\") fmt.Println(\"b\") } func funC() { fmt.Println(\"c\") } func main() { funcA() funcB() funC() } 占位符 m1 := make(map[string]int, 5) m1[\"demo\"] = 1 fmt.Printf(\"%v\\n\", m1) // 值的默认格式 fmt.Printf(\"%+v\\n\", m1) // 类似%v, 结构体会加字段名 fmt.Printf(\"%#v\\n\", m1) // map[string]int{\"demo\":1} 值Go语法 fmt.Printf(\"%T\\n\", m1) // map[string]int 打印类型 结构体 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"gin/guide/installation.html":{"url":"gin/guide/installation.html","title":"安装","keywords":"","body":"安装 要求 安装Go，且Go版本大于1.11 使用 go get -u github.com/gin-gonic/gin Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"gin/guide/customization.html":{"url":"gin/guide/customization.html","title":"自定义","keywords":"","body":"自定义 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"posts/mips64el-loongson-k8s.html":{"url":"posts/mips64el-loongson-k8s.html","title":"龙芯Mips64el平台上部署K8S","keywords":"","body":"龙芯Mips64el平台上部署K8s 不会具体讲怎么部署，原理都类似,将原先amd64换成mips64el 中标麒麟龙芯CPU源 # /etc/yum.repos.d/ns7-mips.repo [ns7-mips64el-os] name=NeoKylin Linux Advanced Server 7 - $basearch - Os baseurl=http://download.cs2c.com.cn/neokylin/server/releases/7.0/ls_64/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-neokylin-release enabled=1 [ns7-mips64el-extras] name=NeoKylin Linux Advanced Server 7 - Addons baseurl=http://download.cs2c.com.cn/neokylin/server/everything/7.0/ls_64/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-neokylin-release enabled=0 [ns7-mips64el-updates] name=NeoKylin Linux Advanced Server 7 - Updates baseurl=http://download.cs2c.com.cn/neokylin/server/updates/7.0/ls_64/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-neokylin-release enabled=1 还有一个就是龙芯自己的仓库比较迷，但是软件包比较新 http://ftp.loongnix.org/os/loongnix/1.0/ 基础镜像 neokylin基础镜像 wget https://raw.githubusercontent.com/docker/docker/master/contrib/mkimage-yum.sh chmod +x ./mkimage-yum.sh ./mkimage-yum.sh -y /etc/yum.conf nk-base wget http://list.isoftos.win/script/create_docker_image.loogson chmod +x create_docker_image.loogson ./create_docker_image.loogson nk-base debian基础镜像 可以通过异构构建镜像方式 可以参考大佬项目 debian-debootstrap k8s编译 宿主机编译 4核16G,性能太差,源码编译安装go新版本差不多两小时 通过异构镜像编译 大概修改印象: # 第一处 mips64*) host_arch=mips64le ;; # 第二处 \"linux/mips64le\") export CGO_ENABLED=1 export CC=mips64el-linux-gnu-gcc ;; # 第三处 linux/mips64le # 第四处 uint64转换一下 具体可以通过make pause镜像 这个得注意一下, 不能用空镜像，可以使用基础镜像 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:41 "},"appendix/":{"url":"appendix/","title":"有趣的开源项目","keywords":"","body":"有趣的开源项目 用于记录发现的有用/有趣的Github项目 20191219 book prometheus 20191208 kubewatch Watch k8s events and trigger Handlers Go 20191121 overlay networking tool slackhq/nebula 20191104 KubeOperator 一键部署和管理生产级别的 Kubernetes 集群 20191010 Status Page statusfy NodeJS 20191009 Status Page statping Go 20191006 iptv 20190927 Collection of Prometheus alerting rules 20190926 listen1 听歌神器 20190925 一个基于Go的Telegram RSS Bot机器人，支持应用内阅读预览 kubernetes app kubeapp 20190924 octant k8s Go 20190920 gopub 发布系统(不维护) PrometheusAlert Go 20190919 felix SSH and RESTful scaffold for Backend and DevOps engineers Go kplcloud基于Kubernetes的应用管理平台 Go Git服务webhook Go 20190916 &#x1F575;️‍♀️ 监视我的手机：数据都去哪儿了？ Python 20190915 &#x1F36D; 集合多家 API 的新一代图床auxpi Go 20190913 TeaWeb-可视化的Web代理服务 Go 20190912 SmartPing 一款开源、高效、便捷的网络质量监控神器！Go 20190911 Mysql web端sql审核平台Yearning Go 简单可信赖的任务管理工具 Go 20190908 Linux透明代理 运维管理平台flask Python 20190906 kubernetes高可用安装工具sealos GO 基于Vue框架构建的github数据可视化平台GitDataV VUE 20190904 nging 基于caddy的网站服务程序，带图形化管理界面 20190903 Aria2-AriaNg-X docker-compose 20190823 trivy 容器的简单而全面的漏洞扫描程序 Go 20190813 anytunnel 开源内网穿透商用平台系统 Go 扩展企业安全测试主动诱导型蜜罐框架系统 Go Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"appendix/macOS-apps.html":{"url":"appendix/macOS-apps.html","title":"macOS常用工具","keywords":"","body":"macOS 常用工具 个人常用工具列表 iTerm2(终端) Sourcetree(git工具) Dash(文档工具) iHosts(hosts编辑器) Microsoft To Do(Todo) pap.er(壁纸) ZeroTier(内网穿透) Xmind Pixiu(记账) Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "},"appendix/about.html":{"url":"appendix/about.html","title":"关于","keywords":"","body":"关于 姑且算我的博客吧，&#x1F436;! 托管 目前博客部署在Ucloud &#x1F1ED;&#x1F1F0; k8s集群上 集群 *.godu.dev 大陆&#x1F1E8;&#x1F1F3;k8s集群 *.ysicing.me 香港&#x1F1ED;&#x1F1F0;k8s集群 *.k7s.xyz 本地k8s集群 Copyright © 2020 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-04-06 16:00:40 "}}