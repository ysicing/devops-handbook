{"./":{"url":"./","title":"序言","keywords":"","body":"DevOps运维指南/云原生实践手册 开始之前 在阅读本书之前希望您掌握以下知识和准备以下环境： Debian 常用命令 Docker 基本操作 Mac/Linux 皆可 系列主题 Kubernetes(1.17+)实践 Go(1.12+)开发实践 运维实践(主要是基于Debian发行版) ? 欢迎交流, 一起努力. 如果有任何疑问或错误，欢迎在 issues 进行提问或给予修正意见 如果喜欢或对你有所帮助，欢迎 Star，对我是一种鼓励和推进 &#x1F600; Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:53 "},"kubernetes/intro/":{"url":"kubernetes/intro/","title":"概念篇","keywords":"","body":"概念篇 Kubernetes 基本概念和使用方法 架构 图片 - k8s架构 图片 - k8s整体架构 etcd保存了整个集群的状态； apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制； controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上； kubelet负责维护容器的生命周期，同时也负责Volume（CSI）和网络（CNI）的管理； Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）； kube-proxy负责为Service提供cluster内部的服务发现和负载均衡； CoreDns 集群dns解析服务 Ingress Controller 应用层负载均衡提供对外访问服务 Kube-state-metrics 集群监控 Prometheus 资源监控 Dashboard Web UI Master(Control Plane)节点 图片 - master节点架构 Master是集群的控制平面: 负载集群的全局决策(如调度等) 探测响应集群事件(如探测应用的实例数是否符合预期) 通常master节点不调度业务服务 Master节点主要包括kube-apiserver、kube-scheduler、kube-controller-manager和etcd四个组件。 kube-apiserver: API服务, Kubernetes控制平面的前端，所有请求入口 kube-scheduler: watch API资源状态，根据相关条件调度到合适的node节点创建相关资源 kube-controller-manager: 生命周期管理和API业务逻辑(如: 节点控制器,副本控制器) etcd: k8s数据存储组件 cloud-controller-manager: 云服务商组件，对接各家云资源(云服务商维护) Worker(Node)节点 图片 - worker节点架构 包括kubelet、kube-proxy和Container Runtime三个组件。 kubelet: 运行在集群每个节点的客户端，需要确保相关容器运行在pod中； 通过PodSpecs标签，描述容器的运行状态； kubelet只管理通过kubernetes创建的容器。 kube-proxy： 是一个运行在集群每个节点的网络代理组件,主要是维护网络规则，保证集群内外与Pod通信。 Container Runtime： 支持运行容器底层环境的软件； 支持： CRI(Container Runtime Interface) 如Docker/Containerd。 核心Addons Addons 使用 Kubernetes 资源（DaemonSet、Deployment等）实现集群的功能特性。由于他们提供集群级别的功能特性，addons使用到的Kubernetes资源都放置在 kube-system名称空间下。 CNI(Calico等) DNS(CoreDNS等) UI(Dashboard) ... 官方Addons Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/intro/deploy.html":{"url":"kubernetes/intro/deploy.html","title":"部署第一个应用","keywords":"","body":"部署第一个应用 # https://ysicing.me/hack/demo/deploy.yaml apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本 kind: Deployment #该配置的类型，我们使用的是 Deployment metadata: #译名为元数据，即 Deployment 的一些基本属性和信息 namespace: default # 命名空间 name: demo-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: demo #为该Deployment设置key为app，value为demo的标签 spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 #strategy: #滚动策略 最多新增一个，最小下线一个 # rollingUpdate: # maxSurge: 1 # maxUnavailable: 1 # type: RollingUpdate selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:demo的资源 app: demo template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:demo的Pod app: demo spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: godemo #container的名称 image: ysicing/godemo #使用镜像godemo创建container，该container默认80端口可访问 部署应用 kubectl apply -f https://ysicing.me/hack/demo/deploy.yaml deployment.apps/demo-deployment created 查看部署结果 # 默认ns就是default可省却 kubectl get deployments -n default NAME READY UP-TO-DATE AVAILABLE AGE demo-deployment 1/1 1 1 2m54s kubectl get pods NAME READY STATUS RESTARTS AGE demo-deployment-59cd96d4d5-cjjwr 1/1 Running 0 3m17s Deployment是pod控制器 常见pod控制器 守护型: 无状态非系统级应用: Deployment (如nginx) 无状态系统级应用: DaemonSet (如日志监控收集端，每个node节点仅且需要跑一个pod) 有状态应用: StatefulSet (数据库类应用如mysql等) 非守护型: 一次性任务: Job 定时任务: CronJob 常用命令 # 获取资料 kubectl explain 类型(如pods) # 获取资源信息 kubectl get 资源类型 kubectl get pods 获取default租户的所有pods资源列表 kubectl get nodes 获取节点资源列表 kubectl get deploy 获取default租户类型为deployment的资源列表 # 显示资源的详细信息 kubectl describe 资源类型 资源名称 kubectl describe deploy demo-deployment 获取default租户deployment类型且名为demo-deployment的详细信息 # 看pod日志，类似docker logs kubectl logs Pod名称 kubectl logs demo-deployment-59cd96d4d5-cjjwr 查看default租户下pod名为demo-deployment-59cd96d4d5-cjjwr的日志 # 进入容器，类型docker exec kubectl exec -it Pod名称 操作命令 kubectl exec -it demo-deployment-59cd96d4d5-cjjwr ash / # 访问部署的应用 那么，部署完第一个应用又该如何访问? Kubernetes 中的 Service（服务） 提供了这样的一个抽象层，它选择具备某些特征的 Pod（容器组）并为它们定义一个访问方式。 在创建Service的时候，通过设置配置文件中的 spec.type 字段的值，可以以不同方式向外部暴露应用程序，这里介绍常用的3种： ClusterIP(默认): 集群内部可访问 NodePort: NAT方式,可以通过访问集群中任意节点+端口号的方式访问服务 :,此时ClusterIP的访问方式仍然可用。 LoadBalancer: 负载均衡(依赖云访问)。此时 ClusterIP 和 NodePort 的访问方式仍然可用。 kubectl explain svc # https://ysicing.me/hack/demo/svc.yaml apiVersion: v1 kind: Service metadata: name: demo-service #Service 的名称 labels: #Service 自己的标签 app: demo #为该 Service 设置 key 为 app，value 为 demo 的标签 spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: demo #选择包含标签 app:demo 的 Pod ports: - name: demo-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 生效 kubectl apply -f https://ysicing.me/hack/demo/svc.yaml service/demo-service created 查看service，通过之前的文档 kubectl get svc -l app=demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-service NodePort 10.96.37.87 80:32600/TCP 55s --- kubectl describe svc/demo-service Name: demo-service Namespace: default Labels: app=demo Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"demo\"},\"name\":\"demo-service\",\"namespace\":\"default\"},\"spe... Selector: app=demo Type: NodePort IP: 10.96.37.87 Port: demo-port 80/TCP TargetPort: 80/TCP NodePort: demo-port 32600/TCP Endpoints: 172.16.219.5:80 Session Affinity: None External Traffic Policy: Cluster Events: 测试服务访问 # 在集群节点 root@k8s1:~# curl 10.96.37.87 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl 172.16.219.5 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl 192.168.100.101:32600 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl 192.168.100.102:32600 {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} 那么问题来了，如果想通过clusterip方式提供对外服务，该怎么做？ # https://ysicing.me/hack/demo/ing.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: labels: app: demo name: demo-ingress # ingress名 namespace: default spec: rules: - host: godemo.slb.k7s.xyz # 域名 http: paths: - backend: serviceName: demo-service # godemo的 service名 servicePort: demo-port # godemo的service定义的port path: / #路径 生效 kubectl apply -f https://ysicing.me/hack/demo/ing.yaml ingress.networking.k8s.io/demo-ingress created kubectl get ing NAME HOSTS ADDRESS PORTS AGE demo-ingress godemo.slb.k7s.xyz 80 91s 验证ingress curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} 这时候流量增加，怎么快速伸缩应用。 伸缩应用 伸缩 的实现可以通过更改 deploy.yaml 文件中部署的 replicas（副本数）来完成 # replicas: 1 ---> replicas: 4 # 改完生效 kubectl apply -f https://ysicing.me/hack/demo/deploy2.yaml deployment.apps/demo-deployment configured # 查看pod kubectl get pods -l app=demo NAME READY STATUS RESTARTS AGE demo-deployment-59cd96d4d5-78v28 1/1 Running 0 16s demo-deployment-59cd96d4d5-cjjwr 1/1 Running 0 15m demo-deployment-59cd96d4d5-mn7r8 1/1 Running 0 16s demo-deployment-59cd96d4d5-mvxk2 1/1 Running 0 16s # 修改了 Deployment 的 replicas 为 4 后，Kubernetes 又为该 Deployment 创建了 3 新的 Pod，这 4 个 Pod 有相同的标签。因此nginx Service 通过标签选择器与新的 Pod建立了对应关系，将访问流量通过负载均衡在 4 个 Pod 之间进行转发 Name: demo-service Namespace: default Labels: app=demo Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"demo\"},\"name\":\"demo-service\",\"namespace\":\"default\"},\"spe... Selector: app=demo Type: NodePort IP: 10.96.37.87 Port: demo-port 80/TCP TargetPort: 80/TCP NodePort: demo-port 32600/TCP Endpoints: 172.16.109.71:80,172.16.109.72:80,172.16.219.5:80 + 1 more... Session Affinity: None External Traffic Policy: Cluster Events: 验证效果,流量是负载到后端不同pod上 root@k8s1:~# curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-mn7r8\",\"ip\":{\"eth0\":\"172.16.109.71/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-cjjwr\",\"ip\":{\"eth0\":\"172.16.219.5/32\",\"lo\":\"127.0.0.1/8\"}} root@k8s1:~# curl godemo.slb.k7s.xyz {\"hostname\":\"demo-deployment-59cd96d4d5-78v28\",\"ip\":{\"eth0\":\"172.16.219.6/32\",\"lo\":\"127.0.0.1/8\"}} 最后 陈述式： kubectl create -f xx.yaml 申明式（建议使用）： kubectl apply -f xx.yaml pod容器如果未发生调度，重启容器ip是不会改变的 另外除了Service这种网络，还有hostPort,hostNetwork hostPort：直接将容器的端口与所调度的节点上的端口路由，这样可以通过宿主机的IP加上来访问Pod了, Ingress就是这样的 hostNetwork：共享宿主机的网络名称空间 这里可以这么测试使用hostPort kubectl apply -f https://ysicing.me/hack/demo/deploy3.yaml kubectl get pods -l app=demo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-deployment-6c5664f4d6-s6w8v 1/1 Running 0 112s 172.16.109.68 k8s2 curl 666.slb.k7s.xyz:28080 {\"hostname\":\"demo-deployment-6c5664f4d6-s6w8v\",\"ip\":{\"eth0\":\"172.16.109.68/32\",\"lo\":\"127.0.0.1/8\"}} Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/intro/sa.html":{"url":"kubernetes/intro/sa.html","title":"资源权限控制","keywords":"","body":"k8s 资源管理的权限控制 在k8s中，由系统自身的接口来创建和管理的账号类型只有一种，叫做ServiceAccount。 可以使用下面的命令来查看目前系统已有的ServiceAccount，简写sa ➜ ~ kubectl get sa NAME SECRETS AGE default 1 2d8h 查看sa具体定义 ➜ ~ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-01-05T06:27:59Z\" name: default namespace: default resourceVersion: \"428\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 9881e9a3-4097-44b4-a9e3-e5a9d4e516e0 secrets: - name: default-token-8xcpl # 引用了名为default-token-8xcpl的secrets ➜ ~ kubectl get secrets default-token-8xcpl -o yaml apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5akNDQWJLZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQ0FYRFRJd01ERXdOVEEyTWpjeE5Gb1lEekl4TVRneE1qRXlNRFl5TnpFMFdqQVZNUk13RVFZRApWUVFERXdwcmRXSmxjbTVsZEdWek1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBCnoyS0ttbzh2emdXWE1WR09hSy9DeWk2cGZ2T0psWUMxd1paRTdWS3IwTENFZDBQWXVKbk10NStQdVhKUkw4OFUKK2t2U0RGRm1RS3J6QUhCN0IwNGVybEwyd0NHeDZwa2FIOFBMMTJqKzdUUW5VS2doa1lHb1ZxUjNKV3NTSS9jcwo0ampGWTJyVmY3Z3pDNG9LbHlBc2VRdXFRaERPV004emtCalFYa3gyZVdnSSthRFNpOHd5SHNSNXZwK0Y4TkNOCnEreHY3bFczOERBcTB2SGlSYnBlZHVCTWpUTksyaHlYYWpyeFpWNTZxTTdnRUJWcVBjRCtUWmQySGs2SGQ2dlgKaGhTeTdUQ3lOY1kvbi9HNjRscTBUaG9JZTBWaFBncG8rU3JzRXVOVTBqWlFmRllDMDRWZlluSU0yZmxuSkY5YQpVWjkraVEza25ZR3RmSTA4cXRScjh3SURBUUFCb3lNd0lUQU9CZ05WSFE4QkFmOEVCQU1DQXFRd0R3WURWUjBUCkFRSC9CQVV3QXdFQi96QU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFOWHdQUkRLd05yMEFzMVpiN1NWWnVQeSsKMVhtbWhWS3dVUmVROXF2QjQwWU5sTVBKMVVqbUNyVmdVRmM4WldmUEhwTmZ6Z2dXTUJUbFFrZDhOK1NhTHB3bAo0bTJtMlpmTTV6Q3R2QVg2MzhHZUVPYTViVkNHcXhudUNVOWQxb2p0M2JZSkYwZGxSMy9VY3FpaDRaeEdUL0syCkJweEZ5QXBRRUZ0elhVZmE0dTBYcFBwUU1aa2txK2hkd01mZjQ3VlBma3NUazUvN2ZaK08vUXk4SElSM00wWFEKUWZyRWpIRXRwL2VSU1hENm95ZHF4R2RKL0pOWU4rKzJYU05lRm51bDFOakh2bG1IT1JpRDI2S2Rqb24vT1Y5cgplYlp0T1oxRmJPVVVFNktJRG9CWTJIK0JCWVppNkwzajJBRkpFTFozWE5tNUdlSTB0NGduWW8zSXU0SnRmZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K namespace: ZGVmYXVsdA== token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkltUnBTa2RCWlZwcVMwOVdOVmxCVDJkemQySTBkRlpPVVVkRlVHOVZYMXB0VDA1Q1dXeG5UVGhhTVdjaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbVJsWm1GMWJIUXRkRzlyWlc0dE9IaGpjR3dpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdWbVlYVnNkQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJams0T0RGbE9XRXpMVFF3T1RjdE5EUmlOQzFoT1dVekxXVTFZVGxrTkdVMU1UWmxNQ0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT21SbFptRjFiSFFpZlEub1NwZ0w4V3liYlJEb0tvYUpxclJ1RkxGOUN6SW1tdlo3VEtWU1RXdVpnWVpCWTRRZ092NjVCbGFLTF9qQU9DSFNyQzd6WW5pSjlXVDVCcVd2N3dfQU9XdEhURVJWSU00Y3I5LXh6LUxMRHZ6bl9qZlNIR0VoSnBPZHZnMGppRFZydi0xanJOb2g1S3VKMGw0TlNULVBnemtUQTU5bWhfNzdodFRtdzJfaDJCWHNDWDBROFg1dm5uMTBMVFJaeHRtNnZTank5dVRIcUZpa0pkU2pTX2c1SjJiS3BXVW1sZnl6OWNKTkliNWhDS0hYVEYyWGlDaG9vMlI3RmdvcHc5X1ltNWxLc2JudGh0bHA5TDUzZ2M2UV9IUkRvb05hSk04RHZndTNybXJheUhPa193RkFTbE1XZ2NkZzF0OGNjYXIwM1V0aWZ5RElDS2s1OTBMM2d2T0hR kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 9881e9a3-4097-44b4-a9e3-e5a9d4e516e0 creationTimestamp: \"2020-01-05T06:27:59Z\" name: default-token-8xcpl namespace: default resourceVersion: \"421\" selfLink: /api/v1/namespaces/default/secrets/default-token-8xcpl uid: 2f07379e-80bb-4c24-a3e3-8a37d1c4c9fb type: kubernetes.io/service-account-token 创建sa ➜ ~ kubectl create sa ysicing serviceaccount/ysicing created ➜ ~ kubectl get sa ysicing -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-01-07T14:40:07Z\" name: ysicing namespace: default resourceVersion: \"134248\" selfLink: /api/v1/namespaces/default/serviceaccounts/ysicing uid: e5f19802-a196-47d6-b194-3684642357ef secrets: - name: ysicing-token-zvgr4 # 获取token,这里token是原始token，未经过base64加密 kubectl describe secrets ysicing-token-zvgr4 测试请求k8s api,发现还是403. ➜ ~ curl -k https://192.168.100.101:6443/healthz/ping -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ' { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"forbidden: User \\\"system:serviceaccount:default:ysicing\\\" cannot get path \\\"/healthz/ping\\\"\", \"reason\": \"Forbidden\", \"details\": { }, \"code\": 403 } k8s对资源操作权限划分的比较详细: 写权限 create update delete patch 读权限 get list watch 配置sa ysicing访问资源,即配置 .kube/config # 配置sa ysicing token鉴权信息 kubectl config set-credentials ysicing --token eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ User \"ysicing\" set. # 配置集群访问信息 # .kube/ca.crt 从192.168.100.101:/etc/kubernetes/pki/ca.crt获取的 ➜ ~ kubectl config set-cluster local --server https://192.168.100.101:6443 --certificate-authority .kube/ca.crt --embed-certs=true Cluster \"local\" set. # 配置context, 将cluster & user 绑定 ➜ ~ kubectl config set-context local-ctx --cluster local --user ysicing Context \"local-ctx\" created. # 切换ctx ➜ ~ kubectl config use-context local-ctx Switched to context \"local-ctx\". 请求接口 ➜ ~ kubectl get pods Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:default:ysicing\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\" 授权配置 创建角色，定义资源操作权限(ClusterRole/Role) 角色绑定，将角色与Sa绑定(ClusterRoleBinding/RoleBinding) # 创建角色 ➜ ~ kubectl create role ysicing-role --resource pod,service,deployment,secret,ingress --verb create,update,delete,patch,get,list,watch role.rbac.authorization.k8s.io/ysicing-role created ➜ ~ kubectl get roles NAME AGE ysicing-role 34s # 角色绑定 ➜ ~ kubectl create rolebinding ysbot.ysicing-binding --role ysicing-role --serviceaccount default:ysicing rolebinding.rbac.authorization.k8s.io/ysbot.ysicing-binding created 再次获取pods信息 ➜ ~ kubectl config use-context local-ctx Switched to context \"local-ctx\". ➜ ~ kubectl get pods NAME READY STATUS RESTARTS AGE demo-deployment-6c5664f4d6-s6w8v 1/1 Running 0 2d4h 再次请求https://192.168.100.101:6443/healthz/ping发现依旧403，那么给ysicing sa通过ClusterRoleBinding 授予一个名为 cluster-admin 的 ClusterRole ➜ ~ kubectl config use-context kubernetes-admin@kubernetes Switched to context \"kubernetes-admin@kubernetes\". ➜ ~ kubectl create clusterrolebinding cluster-ysicing --clusterrole cluster-admin --serviceaccount default:ysicing clusterrolebinding.rbac.authorization.k8s.io/cluster-ysicing created ➜ ~ curl -k https://192.168.100.101:6443/healthz/ping -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImRpSkdBZVpqS09WNVlBT2dzd2I0dFZOUUdFUG9VX1ptT05CWWxnTThaMWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InlzaWNpbmctdG9rZW4tenZncjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoieXNpY2luZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU1ZjE5ODAyLWExOTYtNDdkNi1iMTk0LTM2ODQ2NDIzNTdlZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnlzaWNpbmcifQ.D6L4y4N8e6IzGdiLcRMT_hxtJ2fhLsNTJs4hh7NJlEEulyvklwuA2AFWpdvw5aRXx5DVoHZq__bZ1M2s3-wP_hL57IsoLgW9yFb2GVpnwFXOGe_YipyYcqaVkhpNd7GKjMMxS2IKOdYsX-yBreQsI8x77WPb3ux6TNTFTCv2mmLl1P-0c9TCeeyjjHACYUnV07oHwRk7CJ5ee_S34ZZkEz-ulf2E7xm5cwqWUd738WrN8LwK6p293WwBHeAh3fgK2iAgu5W-PZRFY_5_mV8fAJCVJ-E95WVQUplfuCLQBUx33mDseMuW7keCUJg2JkeSnE3mw_K09EuAt8EXkCDkWQ' ok% # 真香哈哈哈哈 cluster-admin权限特别大,实际使用还需要谨慎操作。 # 可以查看具体定义 kubectl get role ysicing-role Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/install/":{"url":"kubernetes/install/","title":"安装篇","keywords":"","body":"k8s安装 k8s安装方式有很多种，这里我主要安利如下: sealos(完美) minikube Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/install/sealos.html":{"url":"kubernetes/install/sealos.html","title":"sealos","keywords":"","body":"sealos安装k8s 推荐使用sealos, 一键高可用，简单上手 准备工作 虚拟机 环境: 3台机器(debian/buster, 4.19.0-6-amd64, 192.168.100.101~192.168.100.103, 2核4G80G存储) mkdir k8s && cd k8s wget https://ysicing.me/hack/vm/Vagrantfile vagrant up 初始化环境 docker pull ysicing/ansible docker run -it --rm ysicing/ansible bash cp inventory.ini.example inventory.ini # 初始化系统,安装docker ansible-playbook all.yml exit 安装k8s 源码编译安装sealos,使用新特性 git clone https://github.com/fanux/sealos.git --depth 1 make local # 同步sealos到相关节点，如果你用上述步骤初始化系统，默认已经内置了sealos&#x1F602; mv sealos /usr/local/bin/ 下载离线包定制离线包 wget https://sealyun.oss-cn-beijing.aliyuncs.com/413bd3624b2fb9e466601594b4f72072-1.17.0/kube1.17.0.tar.gz tar xf kube1.17.0.tar.gz 定制后的脚本，我移除了镜像和二进制文件,需要自己二次集成，下载路径install.tgz或者kube1.17.0.tar.gz # 安装1master2worker sealos init --passwd vagrant --podcidr 172.16.0.0/16 --repo registry.cn-hangzhou.aliyuncs.com/google_containers --master 192.168.100.101 --node 192.168.100.102 --node 192.168.100.103 --version 1.17.0 --pkg-url /root/kube1.17.0.tar.gz # 清除 sealos clean --passwd vagrant --master 192.168.100.101 --node 192.168.100.102 --node 192.168.100.103 特别说明node节点需要指定路由，否则会安装失败,fanux/sealos#134 20200101 更新 目前已经解决了这个问题, vagrant配置桥接网络并注册默认路由 Vagrantfile 内网网段与calico冲突,故调整calico和vagrant虚拟机网段 桥接网络: 192.168.199.0/24 hostonly: 192.168.100.0/24 podcidr: 172.16.0.0/16 svccidr: 10.96.0.0/12 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/install/minikube.html":{"url":"kubernetes/install/minikube.html","title":"minikube","keywords":"","body":"minikube安装k8s # PROXY可以根据自己需要设置，可不设置 minikube start --memory 4096 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --docker-env HTTP_PROXY=http://192.168.99.1:7890 --docker-env HTTPS_PROXY=http://192.168.99.1:7890 --docker-env NO_PROXY=127.0.0.1/32,192.168.0.0/16,10.0.0.0/8,172.16.0.0/12,localhost Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/install/feature.html":{"url":"kubernetes/install/feature.html","title":"feature启用","keywords":"","body":"开启集群feature ttlSecondsAfterFinished 自动清理完成和失败的Job，目前该特性默认不启用。如何判断未启用,查看job资源，在spec里未发现ttlSecondsAfterFinished定义则表示未启用。 启用 我的集群使用sealos安装的，其配置文件在 /etc/kubernetes/manifests下,分别调整 kube-apiserver.yaml,kube-controller-manager.yaml,kube-scheduler.yaml配置，新增 # 示例 spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true - --feature-gates=TTLAfterFinished=true # 新增配置 image: k8s.gcr.io/kube-scheduler:v1.17.0 调整完成后稍等片刻,相关pod会重建。 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/storage/":{"url":"kubernetes/storage/","title":"存储篇","keywords":"","body":"部署存储 部署nfs # 未安装存储 curl https://ysicing.me/hack/k7s/install/nfs/deploy.sh | bash # 已安装存储 kubectl apply -f https://ysicing.me/hack/k7s/install/nfs/deploy.yaml Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/plugins/":{"url":"kubernetes/plugins/","title":"插件篇","keywords":"","body":"kubectl plugin插件机制初体验之krew 插件机制 这个早就已经GA了，其作用允许开发者以独立的二进制或脚本形式发布自定义的kubectl子命令, 灵活快速操作k8s 插件不限制语言，只需将脚本或二进制可执行文件以kubectl-的前缀放到PATH中即可. 官方示例 官方Go示例辅助库 https://github.com/kubernetes/cli-runtime.git 不太简单的官方示例插件 https://github.com/kubernetes/sample-cli-plugin 示例插件 mkdir /usr/local/k8s export PATH=$PATH:/usr/local/k8s 示例插件如下： root@k8s1:~# cat /usr/local/k8s/kubectl-demo #!/bin/bash echo $1 赋予执行权限 chmod +x /usr/local/k8s/kubectl-demo 运行测试: root@k8s1:~# kubectl demo 666 666 root@k8s1:~# kubectl plugin list The following compatible plugins are available: /usr/local/k8s/kubectl-demo kubectl plugin list 可以查看PATH中查看有哪些插件 krew Package manager for kubectl plugins -- krew 安装krew # Bash and ZSH ( set -x; cd \"$(mktemp -d)\" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/download/v0.3.3/krew.{tar.gz,yaml}\" && tar zxvf krew.tar.gz && KREW=./krew-\"$(uname | tr '[:upper:]' '[:lower:]')_amd64\" && \"$KREW\" install --manifest=krew.yaml --archive=krew.tar.gz && \"$KREW\" update ) # .bashrc/.zshrc export PATH=$PATH:/usr/local/k8s:${KREW_ROOT:-$HOME/.krew}/bin 查看当前可用的kubectl plugin，发现多了一个kubect-krew root@k8s1:~# kubectl plugin list The following compatible plugins are available: /usr/local/k8s/kubectl-demo /root/.krew/bin/kubectl-krew krew使用 kubectl krew krew is the kubectl plugin manager. You can invoke krew through kubectl: \"kubectl krew [command]...\" Usage: krew [command] Available Commands: help Help about any command info Show information about a kubectl plugin install Install kubectl plugins list List installed kubectl plugins search Discover kubectl plugins uninstall Uninstall plugins update Update the local copy of the plugin index upgrade Upgrade installed plugins to newer versions version Show krew version and diagnostics Flags: -h, --help help for krew -v, --v Level number for the log level verbosity Use \"krew [command] --help\" for more information about a command. 初体验 root@k8s1:~# kubectl get pods No resources found in default namespace. root@k8s1:~# kubectl krew install change-ns Installing plugin: change-ns Installed plugin: change-ns root@k8s1:~# kubectl krew list PLUGIN VERSION change-ns v1.0.0 krew v0.3.3 root@k8s1:~# kubectl change-ns nginx-ingress namespace changed to \"nginx-ingress\" root@k8s1:~# kubectl get pod root@k8s1:~# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ingress-rfswh 1/1 Running 0 2m6s nginx-ingress-v6c4l 1/1 Running 0 2m6s Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/dev/":{"url":"kubernetes/dev/","title":"开发篇","keywords":"","body":"参考资料 https://kubernetes.io/docs/concepts/architecture/controller/https://www.kubernetes.org.cn/2693.htmlhttps://github.com/kubernetes/client-gohttps://github.com/goodrain/rainbond/blob/master/gateway/store/store.go Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/helm/install.html":{"url":"kubernetes/helm/install.html","title":"Helm篇","keywords":"","body":"helm 安装 docker pull ysicing/tools docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir # 版本 helm version version.BuildInfo{Version:\"v3.0.2\", GitCommit:\"19e47ee3283ae98139d98460de796c1be1e3975f\", GitTreeState:\"clean\", GoVersion:\"go1.13.5\"} 配置国内helm镜像库 helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/helm/drone.html":{"url":"kubernetes/helm/drone.html","title":"安装Drone","keywords":"","body":"使用Helm安装Drone 一个比较热门的轻量级CI/CD开源工具：Drone 简介 Drone是用Go开发的开源轻量级CI/CD工具 使用简单的 YAML 配置文件来定义和执行 Docker 容器中定义的 Pipeline 构成简单,服务占用资源少 Server端负责身份认证，仓库配置，用户、Secrets 以及 Webhook 相关的配置 Agent端用于接受构建的作业和真正用于运行的 Pipeline 工作流 安装简单，支持主流Git托管服务(github,gitea等) 官方文档也很全 环境 helm v3.0.2 drone 1.6.1 k8s 1.17.0 准备工作 注册Github OAuth应用 图片 - drone github oauth应用 获取到github oauth Client ID,Client Secret留用。 创建持久化存储 # https://ysicing.me/hack/demo/pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: dronepv spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi persistentVolumeReclaimPolicy: Delete storageClassName: nfs-test nfs: server: 192.168.100.101 path: /k8sdata/default-drone-pvc --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: dronepvc namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: nfs-test 示例 # nfs 地址需要按需调整一下 kubectl apply -f https://ysicing.me/hack/demo/pvc.yaml kubectl get pvc 安装 本文使用helm方式安装，因为需要对配置做些调整，需要自定义一些配置 helm repo update # 获取最新的离线drone包 helm pull stable/drone tar xf drone-2.4.0.tgz && cd drone # 编辑values.yaml 示例values.yaml如下 # 有注释的地方说明有修改 images: server: repository: \"docker.io/drone/drone\" tag: 1.6.1 pullPolicy: IfNotPresent agent: repository: \"docker.io/drone/agent\" tag: 1.6.1 pullPolicy: IfNotPresent dind: repository: \"docker.io/library/docker\" tag: 18.06.1-ce-dind pullPolicy: IfNotPresent service: httpPort: 80 type: ClusterIP exposeGRPC: false ingress: enabled: true # 启用 hosts: - drone.godu.dev # 域名 tls: - secretName: godu.dev # 证书，集群内已经证书的secret了，可以不启用，因为我的dev域名必须https访问 hosts: - drone.godu.dev # 域名 sourceControl: provider: github # github secret: github: clientID: xxx # github oauth id xxx clientSecretKey: clientSecret clientSecretValue: xxxx # github oauth secret xxxx server: https://github.com gitlab: clientID: clientSecretKey: clientSecret clientSecretValue: server: gitea: clientID: clientSecretKey: clientSecret clientSecretValue: server: gogs: server: bitbucketCloud: clientID: clientSecretKey: clientSecret clientSecretValue: bitbucketServer: server: consumerKey: consumerKey privateKey: privateKey username: passwordKey: password server: host: \"drone.godu.dev\" protocol: https rpcProtocol: http adminUser: ysicing # github 登录后就具有管理员权限 alwaysAuth: false kubernetes: enabled: true # 运行 Drone 的任务的时候就是直接使用 Kubernetes 的 Job 资源对象来执行，而不是 Drone 的 agent. env: DRONE_LOGS_DEBUG: \"false\" DRONE_DATABASE_DRIVER: \"sqlite3\" DRONE_DATABASE_DATASOURCE: \"/var/lib/drone/drone.sqlite\" annotations: {} resources: {} affinity: {} nodeSelector: {} tolerations: [] extraContainers: | extraVolumes: | agent: env: DRONE_LOGS_DEBUG: \"false\" replicas: 1 annotations: {} resources: {} livenessProbe: {} readinessProbe: {} affinity: {} nodeSelector: {} tolerations: [] dind: enabled: true driver: overlay2 resources: {} metrics: prometheus: enabled: true persistence: enabled: true existingClaim: dronepvc # 刚刚创建的持久化 pvc rbac: create: true apiVersion: v1 serviceAccount: create: true name: 部署drone $ helm install drone -f values.yaml stable/drone $ helm status drone NAME: drone LAST DEPLOYED: Sun Jan 5 20:30:00 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ********************************************************************************* *** PLEASE BE PATIENT: drone may take a few minutes to install *** ********************************************************************************* From outside the cluster, the server URL(s) are: http://drone.godu.dev 最后 root@k8s.cn1:~/drone/drone# kubectl get pods -l app=drone NAME READY STATUS RESTARTS AGE drone-drone-server-5bffbc56df-qzk28 1/1 Running 0 35m root@k8s.cn1:~/drone/drone# kubectl get ing -l app=drone NAME HOSTS ADDRESS PORTS AGE drone-drone drone.godu.dev 80, 443 36m 触发CI构建 # .drone.yml kind: pipeline name: default steps: - name: build image: golang:latest environment: GOPROXY: https://goproxy.cn commands: - CGO_ENABLED=0 go build - name: docker image: plugins/docker settings: repo: ysicing/godemo use_cache: true username: from_secret: dockeruser password: from_secret: docker tags: - latest when: event: push branch: master 代码地址: BeidouCloudPlatform/demo 图片 - drone ci 图片 - drone 详情页 Cli工具 brew install drone-cli export DRONE_SERVER=https://drone.godu.dev export DRONE_TOKEN= drone info # 修复webhook，如果调整了域名，可以通过此命令修复webhook drone repo repair BeidouCloudPlatform/demo Job清理问题 因为使用Job的方式进行pipline操作，如果不启用TTLAfterFinished会导致job不会被自动清理。 开启feature请参考 feature开启 默认drone清理是300s Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/helm/docker-registry.html":{"url":"kubernetes/helm/docker-registry.html","title":"安装Registry","keywords":"","body":"部署docker registry 最近大陆push镜像老是超时，于是想自建一个，使用helm方式, 和drone安装方式类似 部署 helm pull stable/docker-registry tar xf docker-registry-1.9.1.tgz && cd docker-registry/ # 编辑values.yaml,自定义配置 # 注释部分为调整 replicaCount: 1 updateStrategy: podAnnotations: {} podLabels: {} image: repository: registry tag: 2.7.1 pullPolicy: IfNotPresent service: name: registry type: ClusterIP port: 5000 annotations: {} ingress: enabled: true # 启用ingress path: / hosts: - hub.local.godu.dev # 域名 annotations: nginx.ingress.kubernetes.io/proxy-body-size: \"0\" # k8s ingress 413 nginx.org/client-max-body-size: \"0\" # nginxnc ingress 413 labels: {} tls: - secretName: godu.dev # 证书 hosts: - hub.local.godu.dev # 域名 resources: {} persistence: accessMode: 'ReadWriteOnce' enabled: true size: 5Gi existingClaim: registry # 同drone storage: filesystem secrets: haSharedSecret: \"\" htpasswd: \"\" configData: version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 securityContext: enabled: true runAsUser: 1000 fsGroup: 1000 priorityClassName: \"\" podDisruptionBudget: {} nodeSelector: {} tolerations: [] extraVolumeMounts: [] extraVolumes: [] deploy helm install registry -f values.yaml stable/docker-registry Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/addons/":{"url":"kubernetes/addons/","title":"Addons篇","keywords":"","body":"附加组件 扩展了Kubernetes的功能 部署kuboard kubectl apply -f https://ysicing.me/hack/k7s/install/kuboard/deploy.yml # 因为ingress部署关系，所以配置解析域名为ui.slb.k7s.xyz # 管理节点执行，获取token kbtoken 部署metrics-server kubectl apply -f https://ysicing.me/hack/k7s/install/metrics-server/deploy.yaml 部署Dashboard sealos install --pkg-url https://github.com/sealstore/dashboard/releases/download/v2.0.0-bata5/dashboard.tar kdtoken Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/addons/ingress/nginxnc-ingress.html":{"url":"kubernetes/addons/ingress/nginxnc-ingress.html","title":"部署nginxnc","keywords":"","body":"Ingress Controller部署之nginxinc kubectl apply -f https://ysicing.me/hack/k7s/install/nginx-ingress/nginx-ingress.yml 部署文档 nginxinc/kubernetes-ingress v1.6.0 默认情况下 *.k7s.xyz 192.168.100.101 *.slb.k7s.xyz 192.168.100.102 *.vlb.k7s.xyz192.168.100.103 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/ack/":{"url":"kubernetes/ack/","title":"ACK实践","keywords":"","body":"阿里云ACK实践 重点介绍阿里云ACK集群实践 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"kubernetes/ack/ingress.html":{"url":"kubernetes/ack/ingress.html","title":"新增内网slb ingress","keywords":"","body":"ACK 添加内网负载均衡 先已经创建好内网slb,获取slb的ID为lb-xxxx # intranet-slb-ingress.yml apiVersion: v1 kind: Service metadata: # 这里服务取名为nginx-ingress-lb-intranet name: nginx-ingress-lb-intranet namespace: kube-system labels: app: nginx-ingress-lb-intranet annotations: # 指明SLB实例地址类型为私网类型 service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet # 修改为您的私网SLB实例ID service.beta.kubernetes.io/alicloud-loadbalancer-id: lb-xxxx # 是否自动创建SLB端口监听（会覆写已有端口监听），也可手动创建端口监听 service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: 'true' spec: type: LoadBalancer # route traffic to other nodes externalTrafficPolicy: \"Cluster\" ports: - port: 80 name: http targetPort: 80 - port: 443 name: https targetPort: 443 selector: # select app=ingress-nginx pods app: ingress-nginx 生效内网slb kubectl apply -f intranet-slb-ingress.yml 查看slb nginx-ingress-lb LoadBalancer 172.x.x.x 39.x.x.x 80:31110/TCP,443:31574/TCP 20h nginx-ingress-lb-intranet LoadBalancer 172.x.x.x 10.x.x.x 80:30740/TCP,443:30852/TCP 73m Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"linux/":{"url":"linux/","title":"Linux","keywords":"","body":"Linux姿势 linux management note Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"linux/linux.html":{"url":"linux/linux.html","title":"Debian使用技巧","keywords":"","body":"Linux 快速启动一个Debian环境 Debian 9.11 vagrant init ysicing/debian --box-version 9.11.0 vagrant up Debian 10.2 vagrant init ysicing/debian vagrant up 源码 构建box源码 debian-vagrant Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"linux/init.html":{"url":"linux/init.html","title":"初始化","keywords":"","body":"初始化Debian 基础环境配置和安装docker配置 操作 git clone https://github.com/ysicing/play-ansible.git cd play-ansible # 安装ansible,可跳过 ./install.sh # 配置初始化机器 cp inventory.ini.example inventory.ini # 执行初始化 ansible-playbook init.yml 或者参考 k8s安装部分 容器化方式初始化 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"linux/reinstall.html":{"url":"linux/reinstall.html","title":"在线重装","keywords":"","body":"在线重装Debian 背景 一条命令快速重装干净的Linux环境 目前仅支持Debian(不会不打算支持其他系统) 基于萌咖大佬的二次魔改 默认做了 默认root密码 vagrant(安装完成建议修改，禁止密码登录) 默认配置源为mirrors.tuna.tsinghua.edu.cn,默认添加了security,backports 默认时区为Asia/Shanghai 默认安装了curl wget openssh-server sudo sed apt-transport-https net-tools等常用工具 同时默认支持自定义密码 安装 curl -sSL https://ysicing.me/hack/reinstall/install.sh | bash # 指定参数 bash 自定义硬盘 存在多个硬盘时，需要下载 https://ysicing.me/hack/reinstall/installhk.sh文件，编辑如下部分即可 d-i partman-auto/disk string /dev/sdb 参考附录 [ Linux VPS ] Debian/Ubuntu/CentOS 网络安装/重装系统/纯净安装 一键脚本 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"linux/aliyun_debian_upgrade_kernel.html":{"url":"linux/aliyun_debian_upgrade_kernel.html","title":"轻量云升级内核","keywords":"","body":"阿里云轻量应用服务器升级内核 升级有风险请慎重哦 配置说明 阿里云HK 1核2G2TB30Mbps ¥34/m Debian 8.9 更新源 需要移除默认源，使用如下源 # /etc/apt/sources.list deb http://mirror.xtom.com.hk/debian/ stretch main contrib non-free # deb-src http://mirror.xtom.com.hk/debian/ stretch main contrib non-free deb http://mirror.xtom.com.hk/debian/ stretch-updates main contrib non-free # deb-src http://mirror.xtom.com.hk/debian/ stretch-updates main contrib non-free deb http://mirror.xtom.com.hk/debian/ stretch-backports main contrib non-free # deb-src http://mirror.xtom.com.hk/debian/ stretch-backports main contrib non-free deb http://mirror.xtom.com.hk/debian-security/ stretch/updates main contrib non-free #deb-src http://mirror.xtom.com.hk/debian-security/ stretch/updates main contrib non-free 更新升级 apt-get update apt-get dist-upgrade 升级到最新内核 apt-get install -t stretch-backports linux-image-amd64 update-grub reboot 升级完成后docker无法启动 prior storage driver aufs failed: driver not supported 删除/var/lib/docker，docker存储由aufs变成overlay2 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"linux/timers.html":{"url":"linux/timers.html","title":"定时器","keywords":"","body":"定时任务，如每65分钟执行一次 crontab 定时任务 65 分钟执行一次，怎么写？ 这个时候，用系统自带的crontab就不好实现了,这时候就是systemd该上传了 参考Systemd 定时器教程 cat /etc/systemd/system/example.timer [Unit] Description=example timer [Timer] OnUnitActiveSec=1h Unit=example.service [Install] WantedBy=multi-user.target cat /etc/systemd/system/example.service [Unit] Description=example [Service] ExecStart= [Install] WantedBy=multi-user.target 定制定时器 OnActiveSec：定时器生效后，多少时间开始执行任务 OnBootSec：系统启动后，多少时间开始执行任务 OnStartupSec：Systemd 进程启动后，多少时间开始执行任务 OnUnitActiveSec：该单元上次执行后，等多少时间再次执行 OnUnitInactiveSec： 定时器上次关闭后多少时间，再次执行 OnCalendar：基于绝对时间，而不是相对时间执行 AccuracySec：如果因为各种原因，任务必须推迟执行，推迟的最大秒数，默认是60秒 Unit：真正要执行的任务，默认是同名的带有.service后缀的单元 Persistent：如果设置了该字段，即使定时器到时没有启动，也会自动执行相应的单元 WakeSystem：如果系统休眠，是否自动唤醒系统 Timer和Service大体用法一致 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"linux/faq.html":{"url":"linux/faq.html","title":"FAQ","keywords":"","body":"FAQ docker npm安装问题 npm config set unsafe-perm true git操作 # 改崩了 git fetch --all git reset --hard origin/master # 放弃本地全部/单个 git checkout . git checkout -- filename Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"storage/storage.html":{"url":"storage/storage.html","title":"存储","keywords":"","body":"存储 主要介绍常用存储安装使用 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"storage/nfs.html":{"url":"storage/nfs.html","title":"NFS","keywords":"","body":"NFS存储 Debian # 安装 apt update apt install -y nfs-kernel-server # 配置 mkdir /k8sdata echo \"/k8sdata/ *(insecure,rw,sync,no_root_squash,no_subtree_check)\" > /etc/exports # 启动nfs systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r # 测试 showmount -e 127.0.0.1 CentOS # 安装nfs yum install -y nfs-utils # 配置共享目录 mkdir /k8sdata echo \"/k8sdata *(insecure,rw,sync,no_root_squash)\" > /etc/exports # 启动nfs systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r # 测试 showmount -e 127.0.0.1 k8s集群部署nfs # 未安装存储 curl https://ysicing.me/hack/k7s/install/nfs/deploy.sh | bash # 已安装存储 kubectl apply -f https://ysicing.me/hack/k7s/install/nfs/deploy.yaml Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"docker/":{"url":"docker/","title":"Docker","keywords":"","body":"docker使用 不会特别强调docker具体使用 docker快速安装 curl -fsSL https://ysicing.me/hack/docker/install.sh | bash 做了如下设置 { \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\",\"https://reg-mirror.qiniu.com\"], \"bip\": \"172.30.42.1/16\", \"max-concurrent-downloads\": 10, \"log-driver\": \"json-file\", \"log-level\": \"warn\", \"log-opts\": { \"max-size\": \"20m\", \"max-file\": \"2\" }, \"storage-driver\": \"overlay2\" } docker常用工具安装 docker run --rm -v /usr/local/bin:/sysdir ysicing/tools tar zxf /pkg.tgz -C /sysdir 内置了 etcdctl helm(v3) docker-compose calicoctl ctop cclear (清理退出容器) din (进入容器 din cid> ash/bash(默认)/sh) dps (容器状态) etcdcli (特别支持k8s集群的etcd) kbtoken (查看kuboard admin用户token) kdtoken (查看dashboard-admin 用户token) Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:53 "},"soft/":{"url":"soft/","title":"开源软件","keywords":"","body":"开源软件部署 记录常用开源软件使用方式 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"soft/dns/adguard.html":{"url":"soft/dns/adguard.html","title":"DNS-Adguard","keywords":"","body":"AdGuard使用姿势 AdGuard Home使用Golang开发，因此安装非常简单，这里以容器的方式部署为例，其它方式可参考官方帮助文档。 主要功能 拦截AD 号称隐私保护 家庭保护模式 自定义过滤(劫持) (&#x1F602;我用的最多的是这个) 部署 二话不说直接上 # docker-compose.yaml version: '2.1' services: caddy: image: spanda/caddy container_name: caddy volumes: - ./ssl:/root/.caddy - /var/log/caddy:/var/log/caddy - ./Caddyfile:/etc/Caddyfile network_mode: host restart: always dns: image: adguard/adguardhome container_name: dns volumes: - ./ad/conf:/opt/adguardhome/conf - ./ad/work:/opt/adguardhome/work network_mode: host restart: always # Caddyfile dns.ysicing.net { gzip tls root@ysicing.net log / /var/log/caddy/dns.ysicing.net.log \"{remote} {when} {method} {uri} {proto} {status} {size} {>User-Agent} {latency}\" { rotate_size 50 rotate_age 90 rotate_keep 20 rotate_compress } header / { Strict-Transport-Security \"max-age=31536000;includeSubDomains;preload\" Access-Control-Allow-Origin * Access-Control-Allow-Methods \"GET, POST, OPTIONS\" -Server } proxy / 127.0.0.1:7070 { transparent websocket } } 访问公网IP:3000,按着无脑一顿猛操作。修改默认web监听端口为127.0.0.1:7070,53端口默认监听全部。 使用 Windows 打开网络和Internet设置 打开网络和共享中心 打开以太网 打开属性 编辑TCP/IPV4 使用下面的DNS服务器 首 59.110.220.53 备 8.8.8.8 Mac 跳过很简单 安卓 自己折腾吧，私有dns搞不定，官方APP可以 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"soft/ldap/openldap_install.html":{"url":"soft/ldap/openldap_install.html","title":"OpenLDAP","keywords":"","body":"OpenLDAP安装初体验 简介这里不在描述，google即可 docke快速部署 version: '2' services: openldap: image: osixia/openldap:1.3.0 container_name: openldap environment: LDAP_LOG_LEVEL: \"256\" LDAP_ORGANISATION: \"Godu Inc.\" LDAP_DOMAIN: \"ysicing.me\" LDAP_BASE_DN: \"\" LDAP_ADMIN_PASSWORD: \"meadmin\" LDAP_CONFIG_PASSWORD: \"meconfig\" LDAP_READONLY_USER: \"true\" LDAP_READONLY_USER_USERNAME: \"readonly\" LDAP_READONLY_USER_PASSWORD: \"readonly\" LDAP_RFC2307BIS_SCHEMA: \"false\" LDAP_BACKEND: \"mdb\" LDAP_TLS: \"false\" LDAP_REPLICATION: \"false\" KEEP_EXISTING_CONFIG: \"false\" LDAP_REMOVE_CONFIG_AFTER_SETUP: \"true\" LDAP_SSL_HELPER_PREFIX: \"ldap\" tty: true stdin_open: true ports: - \"389:389\" - \"636:636\" domainname: \"ldap.ysicing.me\" # important: same as hostname # hostname: \"ldap.ysicing.me\" command: [\"--copy-service\", \"--loglevel\", \"debug\"] phpldapadmin: image: osixia/phpldapadmin:latest container_name: phpldapadmin environment: PHPLDAPADMIN_LDAP_HOSTS: \"openldap\" PHPLDAPADMIN_HTTPS: \"false\" ports: - \"8080:80\" depends_on: - openldap 验证 din openldap ldapsearch -x -H ldap://localhost -b dc=ysicing,dc=me -D 'cn=admin,dc=ysicing,dc=me' -w meadmin 或者访问127.0.0.1:8080 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"shell/":{"url":"shell/","title":"Shell","keywords":"","body":"shell相关 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"shell/awk-func.html":{"url":"shell/awk-func.html","title":"awk 中关于多个$的用法","keywords":"","body":"awk 中关于多个$的用法 上周有遇到过这个问题 awk '{print $$1}' 这个$$是什么用法呢 间接字段寻址,其类似 awk '{print $$1}' ===> awk '{print $($1)}' ===> awk '{Nr=$1; print $Nr}' 示例: $ echo -e \"1 b c d\\n2 b c d\\n3 b c d\\n4 b c d\" 1 b c d 2 b c d 3 b c d 4 b c d $ echo -e \"1 b c d\\n2 b c d\\n3 b c d\\n4 b c d\" | awk '{print $$1}' 1 # since filed #1=1 result print first field b # since filed #1=2 result print secondfield c # since filed #1=3 result print third field d # since filed #1=4 result print fourth field 类似$可以根据需要添加更多 awk '{print $$$1}' ===> awk '{print $$($1)}' ===> awk '{print $($($1))}' Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"ide/vim_config.html":{"url":"ide/vim_config.html","title":"IDE工具-vim配置","keywords":"","body":"vim配置 安装vim plug curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 写配置 向~/.vimrc写入 \" autocmd BufWritePost $MYVIMRC source $MYVIMRC \" 关闭兼容模式 set nocompatible \" 设置行号 set nu \"突出显示当前行,列 set cursorline set cursorcolumn \"显示括号匹配 set showmatch \" tab 缩进 set tabstop=4 \" 设置Tab长度为4空格 set shiftwidth=4 \" 设置自动缩进长度为4空格 set autoindent \" 继承前一行的缩进方式，适用于多行注释 \" 定义快捷键的前缀，即 let mapleader=\";\" \" ==== 系统剪切板复制粘贴 ==== \" v 模式下复制内容到系统剪切板 vmap c \"+yy \" n 模式下复制一行到系统剪切板 nmap c \"+yy \" n 模式下粘贴系统剪切板的内容 nmap v \"+p \" 开启实时搜索 set incsearch \" 搜索时大小写不敏感 set ignorecase syntax enable syntax on \" 开启文件类型侦测 filetype plugin indent on \" 启用自动补全 \" 退出插入模式指定类型的文件自动保存 au InsertLeave *.go,*.sh,*.php write \" 插件 call plug#begin('~/.vim/plugged') \" 可以快速对齐的插件 Plug 'junegunn/vim-easy-align' \" 用来提供一个导航目录的侧边栏 Plug 'scrooloose/nerdtree' \" 可以在导航目录中看到 git 版本信息 Plug 'Xuyuanp/nerdtree-git-plugin' \" 自动补全括号的插件，包括小括号，中括号，以及花括号 Plug 'jiangmiao/auto-pairs' \" 可以在 vim 中使用 tab 补全 Plug 'vim-scripts/SuperTab' Plug 'majutsushi/tagbar' \" go 主要插件 Plug 'fatih/vim-go', { 'tag': '*' } \" go 中的代码追踪，输入 gd 就可以自动跳转 Plug 'dgryski/vim-godef' call plug#end() 安装插件 vim ~/.vimrc # 安装插件 :PlugInstall # 更新插件 :PlugUpdate Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:54 "},"appendix/":{"url":"appendix/","title":"附录说明","keywords":"","body":"附录说明 用于记录发现的有用/有趣的Github项目 20191219 book prometheus 20191208 kubewatch Watch k8s events and trigger Handlers Go 20191121 overlay networking tool slackhq/nebula 20191104 KubeOperator 一键部署和管理生产级别的 Kubernetes 集群 20191010 Status Page statusfy NodeJS 20191009 Status Page statping Go 20191006 iptv 20190927 Collection of Prometheus alerting rules 20190926 listen1 听歌神器 20190925 一个基于Go的Telegram RSS Bot机器人，支持应用内阅读预览 kubernetes app kubeapp 20190924 octant k8s Go 20190920 gopub 发布系统(不维护) PrometheusAlert Go 20190919 felix SSH and RESTful scaffold for Backend and DevOps engineers Go kplcloud基于Kubernetes的应用管理平台 Go Git服务webhook Go 20190916 &#x1F575;️‍♀️ 监视我的手机：数据都去哪儿了？ Python 20190915 &#x1F36D; 集合多家 API 的新一代图床auxpi Go 20190913 TeaWeb-可视化的Web代理服务 Go 20190912 SmartPing 一款开源、高效、便捷的网络质量监控神器！Go 20190911 Mysql web端sql审核平台Yearning Go 简单可信赖的任务管理工具 Go 20190908 Linux透明代理 运维管理平台flask Python 20190906 kubernetes高可用安装工具sealos GO 基于Vue框架构建的github数据可视化平台GitDataV VUE 20190904 nging 基于caddy的网站服务程序，带图形化管理界面 20190903 Aria2-AriaNg-X docker-compose 20190823 trivy 容器的简单而全面的漏洞扫描程序 Go 20190813 anytunnel 开源内网穿透商用平台系统 Go 扩展企业安全测试主动诱导型蜜罐框架系统 Go Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:53 "},"appendix/macOS-apps.html":{"url":"appendix/macOS-apps.html","title":"macOS常用工具","keywords":"","body":"macOS 常用工具 个人常用工具列表 iTerm2(终端) Sourcetree(git工具) Dash(文档工具) iHosts(hosts编辑器) Microsoft To Do(Todo) pap.er(壁纸) ZeroTier(内网穿透) Xmind Pixiu(记账) Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:53 "},"appendix/about.html":{"url":"appendix/about.html","title":"关于","keywords":"","body":"关于 姑且算我的博客吧，&#x1F436;! 托管 目前博客部署在Ucloud &#x1F1ED;&#x1F1F0; k8s集群上 集群 *.godu.dev 大陆&#x1F1E8;&#x1F1F3;k8s集群 *.ysicing.me 香港&#x1F1ED;&#x1F1F0;k8s集群 *.k7s.xyz 本地k8s集群 Copyright © 2019 | 仅供学习参考,不可用于其他途径. all right reserved，powered by Gitbook Updated at 2020-01-08 08:00:53 "}}